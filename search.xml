<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>代码随想录--二叉树</title>
    <url>/2025/02/10day/</url>
    <content><![CDATA[二叉树层序遍历
二叉树层序遍历的模板

第一种方法看了一遍代码+写 花了16分钟，挺简单的，递归法就先不看了。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        vector&lt;vector&lt;int&gt;&gt; res;        while(!q.empty())&#123;            int size=q.size();            vector&lt;int&gt; re;            for(int i=0;i&lt;size;i++)            &#123;                TreeNode* node = q.front();                q.pop();                re.push_back(node-&gt;val);                if(node-&gt;left) q.push(node-&gt;left);                if(node-&gt;right) q.push(node-&gt;right);            &#125;            res.push_back(re);        &#125;        return res;    &#125;&#125;;
# 递归法class Solution &#123;public:    void order(TreeNode* cur, vector&lt;vector&lt;int&gt;&gt;&amp; result, int depth)    &#123;        if (cur == nullptr) return;        if (result.size() == depth) result.push_back(vector&lt;int&gt;());        result[depth].push_back(cur-&gt;val);        order(cur-&gt;left, result, depth + 1);        order(cur-&gt;right, result, depth + 1);    &#125;    vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123;        vector&lt;vector&lt;int&gt;&gt; result;        int depth = 0;        order(root, result, depth);        return result;    &#125;&#125;;
二叉树的层序遍历 II
题目链接：107. 二叉树的层序遍历 II
五分钟结束战斗，就是上一个代码翻一下。但是性能有点慢，又交了一次时间也是超100%了。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; levelOrderBottom(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        vector&lt;vector&lt;int&gt;&gt; res;        while(!q.empty())&#123;            int size=q.size();            vector&lt;int&gt; re;            for(int i=0;i&lt;size;i++)&#123;                TreeNode* node=q.front();                q.pop();                re.push_back(node-&gt;val);                if(node-&gt;left) q.push(node-&gt;left);                if(node-&gt;right) q.push(node-&gt;right);            &#125;            res.push_back(re);        &#125;        reverse(res.begin(),res.end());        return res;    &#125;&#125;;
二叉树的右视图
题目链接：199. 二叉树的右视图
四分钟解决战斗，思考了不到一分钟。
class Solution &#123;public:    vector&lt;int&gt; rightSideView(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        vector&lt;int&gt; res;        while(!q.empty())&#123;            int size=q.size();            vector&lt;int&gt; re;            for(int i=0;i&lt;size;i++)            &#123;                TreeNode* node=q.front();                q.pop();                re.push_back(node-&gt;val);                 //代码随想录 if (i == (size - 1)) result.push_back(node-&gt;val);                 if(node-&gt;left) q.push(node-&gt;left);                if(node-&gt;right) q.push(node-&gt;right);            &#125;            res.push_back(re[size-1]);        &#125;        return res;    &#125;&#125;;
二叉树的层平均值
题目链接：637. 二叉树的层平均值
出了一点小插曲，花了五分钟，前边写的q后边写成p了，double第一次用的int忘了改。
class Solution &#123;public:    vector&lt;double&gt; averageOfLevels(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        vector&lt;double&gt; res;        while(!q.empty())&#123;            int s=q.size();            double sum=0;            for(int i=0;i&lt;s;i++)&#123;                TreeNode* node=q.front();                q.pop();                sum+=node-&gt;val;                if(node-&gt;left) q.push(node-&gt;left);                if(node-&gt;right) q.push(node-&gt;right);            &#125;            res.push_back(sum/s);        &#125;        return res;    &#125;&#125;;
N 叉树的层序遍历
题目链接：429. N 叉树的层序遍历
乍一看以为是第一题，看了一下不太明白孩子是啥意思，就直接看的题解，其实也没多大区别。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; levelOrder(Node* root) &#123;        queue&lt;Node*&gt; que;        if (root != NULL) que.push(root);        vector&lt;vector&lt;int&gt;&gt; result;        while (!que.empty()) &#123;            int size = que.size();            vector&lt;int&gt; vec;            for (int i = 0; i &lt; size; i++) &#123;                Node* node = que.front();                que.pop();                vec.push_back(node-&gt;val);                for (int i = 0; i &lt; node-&gt;children.size(); i++) &#123; // 将节点孩子加入队列                    if (node-&gt;children[i]) que.push(node-&gt;children[i]);                &#125;            &#125;            result.push_back(vec);        &#125;        return result;    &#125;&#125;;
在每个树行中找最大值
题目链接：515. 在每个树行中找最大值
花了8分钟，因为INT_MIN不知道如何操作，本来用的int，然后改成long long一点一点试的。
class Solution &#123;public:    vector&lt;int&gt; largestValues(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        vector&lt;int&gt; res;        while(!q.empty())&#123;            int s=q.size();            long long m=-1e14;   //int maxValue = INT_MIN; // 取每一层的最大值            for(int i=0;i&lt;s;i++)            &#123;            TreeNode* node=q.front();            q.pop();            if(node-&gt;val&gt;m) m=node-&gt;val;            if(node-&gt;left) q.push(node-&gt;left);            if(node-&gt;right) q.push(node-&gt;right);            &#125;            res.push_back(m);        &#125;        return res;    &#125;&#125;;
填充每个节点的下一个右侧节点指针
题目链接：116. 填充每个节点的下一个右侧节点指针
这题没想明白怎么处理，我是应该新建一个什么类型的vector，用int,char,Node类型都不行，然后就直接看题解了。
class Solution &#123;public:    Node* connect(Node* root) &#123;        if (root == NULL) return NULL;        queue&lt;Node*&gt; q;        q.push(root);        while (!q.empty()) &#123;            int size = q.size();            Node* prev = NULL;  // 用来保存当前层级的前一个节点            for (int i = 0; i &lt; size; ++i) &#123;                Node* current = q.front();                q.pop();                // 连接前一个节点的next指针指向当前节点                if (prev != NULL) prev-&gt;next = current;                // 更新prev为当前节点                prev = current;                // 将左子树和右子树加入队列                if (current-&gt;left) q.push(current-&gt;left);                if (current-&gt;right) q.push(current-&gt;right);            &#125;            // 将当前层级最后一个节点的next指针设置为NULL            prev-&gt;next = NULL;        &#125;        return root;    &#125;&#125;;
利用next指针逐层处理（优化）
class Solution &#123;public:    Node* connect(Node* root) &#123;        if (!root) return nullptr;        Node* leftmost = root; // 当前层的最左节点        while (leftmost-&gt;left) &#123; // 直到叶子层            Node* curr = leftmost;            while (curr) &#123;                // 连接左子节点和右子节点                curr-&gt;left-&gt;next = curr-&gt;right;                // 连接右子节点和相邻节点的左子节点                if (curr-&gt;next) &#123;                    curr-&gt;right-&gt;next = curr-&gt;next-&gt;left;                &#125;                curr = curr-&gt;next; // 移动到同层下一节点            &#125;            leftmost = leftmost-&gt;left; // 移到下一层最左        &#125;        return root;    &#125;&#125;;
填充每个节点的下一个右侧节点指针 II
题目链接：117. 填充每个节点的下一个右侧节点指针 II
class Solution &#123;public:    Node* connect(Node* root) &#123;        if (root == NULL) return NULL;        queue&lt;Node*&gt; q;        q.push(root);        while (!q.empty()) &#123;            int size = q.size();            Node* prev = NULL;  // 用来保存当前层级的前一个节点            for (int i = 0; i &lt; size; ++i) &#123;                Node* current = q.front();                q.pop();                // 连接前一个节点的next指针指向当前节点                if (prev != NULL) prev-&gt;next = current;                // 更新prev为当前节点                prev = current;                // 将左子树和右子树加入队列                if (current-&gt;left) q.push(current-&gt;left);                if (current-&gt;right) q.push(current-&gt;right);            &#125;            // 将当前层级最后一个节点的next指针设置为NULL            prev-&gt;next = NULL;        &#125;        return root;    &#125;&#125;;
二叉树的最大深度
题目链接：104. 二叉树的最大深度
虽然简单，但是用这个方法有种高射炮打蚊子的感觉，感觉应该有更简单的方法。
class Solution &#123;public:    int maxDepth(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        int sum=0;        while(!q.empty())&#123;            int s=q.size();            sum=sum+1;            for(int i=0;i&lt;s;i++)&#123;                TreeNode* node=q.front();                q.pop();                if(node-&gt;right) q.push(node-&gt;right);                if(node-&gt;left) q.push(node-&gt;left);            &#125;                &#125;        return sum;    &#125;&#125;;
递归方法：后序遍历（DFS）
class Solution &#123;public:    int maxDepth(TreeNode* root) &#123;        if (root == nullptr) return 0;        return max(maxDepth(root-&gt;left), maxDepth(root-&gt;right)) + 1;    &#125;&#125;;
二叉树的最小深度
题目链接：111. 二叉树的最小深度
找第一个没有左右子树的节点的深度。
class Solution &#123;public:    int minDepth(TreeNode* root) &#123;        if (root == NULL) return 0;        int depth = 0;        queue&lt;TreeNode*&gt; que;        que.push(root);        while(!que.empty()) &#123;            int size = que.size();            depth++; // 记录最小深度            for (int i = 0; i &lt; size; i++) &#123;                TreeNode* node = que.front();                que.pop();                if (node-&gt;left) que.push(node-&gt;left);                if (node-&gt;right) que.push(node-&gt;right);                if (!node-&gt;left &amp;&amp; !node-&gt;right) &#123; // 当左右孩子都为空的时候，说明是最低点的一层了，退出                    return depth;                &#125;            &#125;        &#125;        return depth;    &#125;&#125;;
递归
如果 node 是空节点，由于没有节点，返回 0。
如果 node 没有右儿子，那么深度就是左子树的深度加一，即 dfs(node)=dfs(node.left)+1。
如果 node 没有左儿子，那么深度就是右子树的深度加一，即 dfs(node)=dfs(node.right)+1。
如果 node 左右儿子都有，那么分别递归计算左子树的深度，以及右子树的深度，二者取最小值再加一，即 dfs(node)=min(dfs(node.left),dfs(node.right))+1

class Solution &#123;public:    int minDepth(TreeNode *root) &#123;        if (root == nullptr) return 0;        if (root-&gt;right == nullptr) return minDepth(root-&gt;left) + 1;        if (root-&gt;left == nullptr) return minDepth(root-&gt;right) + 1;        return min(minDepth(root-&gt;left), minDepth(root-&gt;right)) + 1;    &#125;&#125;;
翻转二叉树
题目链接：226. 翻转二叉树
迭代法：深度优先遍历
class Solution &#123;public:    TreeNode* invertTree(TreeNode* root) &#123;        if (root == nullptr) return nullptr;        queue&lt;TreeNode*&gt; q;        q.push(root);        while (!q.empty()) &#123;            TreeNode* node = q.front();            q.pop();            // 使用临时变量交换左右子节点            TreeNode* temp = node-&gt;left;            node-&gt;left = node-&gt;right;            node-&gt;right = temp;            // 如果左右子节点不为空，则加入队列            if (node-&gt;left) q.push(node-&gt;left);            if (node-&gt;right) q.push(node-&gt;right);        &#125;        return root;    &#125;&#125;;
递归法：
class Solution &#123;public:    TreeNode* invertTree(TreeNode* root) &#123;        if (root == NULL) return root;        swap(root-&gt;left, root-&gt;right);  // 中        invertTree(root-&gt;left);         // 左        invertTree(root-&gt;right);        // 右        return root;    &#125;&#125;;
对称二叉树
题目链接：101. 对称二叉树

迭代队列法：
class Solution &#123;public:    bool isSymmetric(TreeNode* root) &#123;        if (root == NULL) return true;        queue&lt;TreeNode*&gt; que;        que.push(root-&gt;left);   // 将左子树头结点加入队列        que.push(root-&gt;right);  // 将右子树头结点加入队列                while (!que.empty()) &#123;  // 接下来就要判断这两个树是否相互翻转            TreeNode* leftNode = que.front(); que.pop();            TreeNode* rightNode = que.front(); que.pop();            if (!leftNode &amp;&amp; !rightNode) &#123;  // 左节点为空、右节点为空，此时说明是对称的                continue;            &#125;            // 左右一个节点不为空，或者都不为空但数值不相同，返回false            if ((!leftNode || !rightNode || (leftNode-&gt;val != rightNode-&gt;val))) &#123;                return false;            &#125;            que.push(leftNode-&gt;left);   // 加入左节点左孩子            que.push(rightNode-&gt;right); // 加入右节点右孩子            que.push(leftNode-&gt;right);  // 加入左节点右孩子            que.push(rightNode-&gt;left);  // 加入右节点左孩子        &#125;        return true;    &#125;&#125;;
迭代栈法：
class Solution &#123;public:    bool isSymmetric(TreeNode* root) &#123;        if (root == NULL) return true;        stack&lt;TreeNode*&gt; st; // 这里改成了栈        st.push(root-&gt;left);        st.push(root-&gt;right);        while (!st.empty()) &#123;            TreeNode* rightNode = st.top(); st.pop();            TreeNode* leftNode = st.top(); st.pop();            if (!leftNode &amp;&amp; !rightNode) &#123;                continue;            &#125;            if ((!leftNode || !rightNode || (leftNode-&gt;val != rightNode-&gt;val))) &#123;                return false;            &#125;            st.push(leftNode-&gt;left);            st.push(rightNode-&gt;right);            st.push(leftNode-&gt;right);            st.push(rightNode-&gt;left);        &#125;        return true;    &#125;&#125;;
递归法：
class Solution &#123;public:    bool compare(TreeNode* left, TreeNode* right) &#123;        // 首先排除空节点的情况        if (left == NULL &amp;&amp; right != NULL) return false;        else if (left != NULL &amp;&amp; right == NULL) return false;        else if (left == NULL &amp;&amp; right == NULL) return true;        // 排除了空节点，再排除数值不相同的情况        else if (left-&gt;val != right-&gt;val) return false;        // 此时就是：左右节点都不为空，且数值相同的情况        // 此时才做递归，做下一层的判断        bool outside = compare(left-&gt;left, right-&gt;right);   // 左子树：左、 右子树：右        bool inside = compare(left-&gt;right, right-&gt;left);    // 左子树：右、 右子树：左        bool isSame = outside &amp;&amp; inside;                    // 左子树：中、 右子树：中 （逻辑处理）        return isSame;    &#125;    bool isSymmetric(TreeNode* root) &#123;        if (root == NULL) return true;        return compare(root-&gt;left, root-&gt;right);    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>二叉树</tag>
        <tag>二叉树遍历</tag>
        <tag>递归</tag>
        <tag>迭代</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--二叉树</title>
    <url>/2025/02/11day/</url>
    <content><![CDATA[二叉树
完全二叉树的节点个数
题目链接：222. 完全二叉树的节点个数
迭代法：
class Solution &#123;public:    int countNodes(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        if(root!=NULL) q.push(root);        int sum=0;        while(!q.empty())&#123;            int s=q.size();            for(int i=0;i&lt;s;i++)&#123;                TreeNode* node=q.front();                q.pop();                sum+=1;                if(node-&gt;right) q.push(node-&gt;right);                if(node-&gt;left) q.push(node-&gt;left);            &#125;        &#125;        return sum;    &#125;&#125;;
平衡二叉树
题目链接：110. 平衡二叉树
class Solution &#123;    int get_height(TreeNode* node)&#123;        if(node==NULL) return 0;        int left=get_height(node-&gt;left);        if(left==-1) return -1;        int right=get_height(node-&gt;right);        if(right==-1 || abs(left-right)&gt;1) return -1;        return max(left,right)+1;    &#125;public:     bool isBalanced(TreeNode* root) &#123;        return get_height(root) != -1;    &#125;&#125;;
二叉树的所有路径
题目链接：257. 二叉树的所有路径
回溯和递归是一一对应的，有一个递归，就要有一个回溯
class Solution &#123;private:    void traversal(TreeNode* cur, string path, vector&lt;string&gt;&amp; result) &#123;        path += to_string(cur-&gt;val); // 中        if (cur-&gt;left == NULL &amp;&amp; cur-&gt;right == NULL) &#123;            result.push_back(path);            return;        &#125;        if (cur-&gt;left) traversal(cur-&gt;left, path + &quot;-&gt;&quot;, result); // 左        if (cur-&gt;right) traversal(cur-&gt;right, path + &quot;-&gt;&quot;, result); // 右    &#125;public:    vector&lt;string&gt; binaryTreePaths(TreeNode* root) &#123;        vector&lt;string&gt; result;        string path;        if (root == NULL) return result;        traversal(root, path, result);        return result;    &#125;&#125;;
class Solution &#123;public:    vector&lt;string&gt; binaryTreePaths(TreeNode* root) &#123;        stack&lt;TreeNode*&gt; treeSt;         stack&lt;string&gt; path;        vector&lt;string&gt; ans;        if(root!=NULL) treeSt.push(root);        path.push(to_string(root-&gt;val));        while(!treeSt.empty())&#123;            TreeNode* node=treeSt.top();            treeSt.pop();            string p=path.top();            path.pop();            if(node-&gt;right==NULL &amp;&amp; node-&gt;left==NULL) ans.push_back(p);            if(node-&gt;left)&#123;                treeSt.push(node-&gt;left);                path.push(p+&quot;-&gt;&quot;+to_string(node-&gt;left-&gt;val));            &#125;            if(node-&gt;right)&#123;                treeSt.push(node-&gt;right);                path.push(p+&quot;-&gt;&quot;+to_string(node-&gt;right-&gt;val));            &#125;        &#125;        return ans;    &#125;&#125;;
左叶子之和
题目链接：404. 左叶子之和
感觉还是迭代法简单好想
class Solution &#123;public:    int sumOfLeftLeaves(TreeNode* root) &#123;        stack&lt;TreeNode*&gt; s;        if(root!=NULL) s.push(root);        int sum=0;        while(!s.empty())&#123;            TreeNode* node=s.top();            s.pop();            if(node-&gt;left &amp;&amp; node-&gt;left-&gt;left==NULL &amp;&amp; node-&gt;left-&gt;right==NULL) sum+=node-&gt;left-&gt;val;            if(node-&gt;left) s.push(node-&gt;left);            if(node-&gt;right) s.push(node-&gt;right);        &#125;        return sum;    &#125;&#125;;
递归法：
class Solution &#123;public:    int sumOfLeftLeaves(TreeNode* root) &#123;        if (root == NULL) return 0;        if (root-&gt;left == NULL &amp;&amp; root-&gt;right== NULL) return 0;        int leftValue = sumOfLeftLeaves(root-&gt;left);    // 左        if (root-&gt;left &amp;&amp; !root-&gt;left-&gt;left &amp;&amp; !root-&gt;left-&gt;right) &#123; // 左子树就是一个左叶子的情况            leftValue = root-&gt;left-&gt;val;        &#125;        int rightValue = sumOfLeftLeaves(root-&gt;right);  // 右        int sum = leftValue + rightValue;               // 中        return sum;    &#125;&#125;;
找树左下角的值
题目链接：513. 找树左下角的值
迭代法
class Solution &#123;public:    int findBottomLeftValue(TreeNode* root) &#123;        queue&lt;TreeNode*&gt; q;        int ans=0;        if(root!=NULL) q.push(root);        while(!q.empty())&#123;            int s=q.size();            for(int i=0;i&lt;s;i++)            &#123;                TreeNode* node = q.front();                q.pop();                if(i==0) ans=node-&gt;val;                if(node-&gt;left) q.push(node-&gt;left);                if(node-&gt;right) q.push(node-&gt;right);            &#125;        &#125;        return ans;    &#125;&#125;;
路径总和
题目链接：112. 路径总和
迭代法
这题和二叉树的所有路径很像，就是把存储路径改成存储路径和。
版本一
class Solution &#123;public:    bool hasPathSum(TreeNode* root, int targetSum) &#123;        stack&lt;TreeNode*&gt; s;        stack&lt;int&gt; sums;  // 用来存储每个节点的路径和        if (root != NULL) &#123;            s.push(root);  // 将根节点压入栈            sums.push(root-&gt;val);  // 将根节点的值作为初始路径和压入路径和栈        &#125;          while (!s.empty()) &#123;            TreeNode* node = s.top();            int sum = sums.top();  // 获取当前节点的路径和            s.pop();            sums.pop();                        // 判断是否是叶子节点并且路径和等于目标值            if (node-&gt;left == NULL &amp;&amp; node-&gt;right == NULL &amp;&amp; sum == targetSum) &#123;                return true;            &#125;                        // 继续将左右子节点和更新后的路径和压入栈中            if (node-&gt;right) &#123;                s.push(node-&gt;right);                sums.push(sum + node-&gt;right-&gt;val);            &#125;            if (node-&gt;left) &#123;                s.push(node-&gt;left);                sums.push(sum + node-&gt;left-&gt;val);            &#125;        &#125;                return false;    &#125;&#125;;
版本二
class Solution &#123;public:    bool hasPathSum(TreeNode* root, int targetSum) &#123;        if (root == NULL) return false;               stack&lt;pair&lt;TreeNode*, int&gt;&gt; s; // 使用栈存储节点和当前的路径和        s.push(&#123;root, root-&gt;val&#125;); // 初始化，根节点和它的值              while (!s.empty()) &#123;            TreeNode* node = s.top().first;            int currentSum = s.top().second;            s.pop();               // 判断是否是叶子节点，并且路径和是否等于目标值            if (node-&gt;left == NULL &amp;&amp; node-&gt;right == NULL &amp;&amp; currentSum == targetSum) &#123;                return true;            &#125;                   // 将子节点和更新后的路径和压入栈中            if (node-&gt;right) s.push(&#123;node-&gt;right, currentSum + node-&gt;right-&gt;val&#125;);            if (node-&gt;left) s.push(&#123;node-&gt;left, currentSum + node-&gt;left-&gt;val&#125;);        &#125;             return false;    &#125;&#125;;
递归法
路径总和 II
题目链接：113. 路径总和 II
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>二叉树</tag>
        <tag>二叉树遍历</tag>
        <tag>递归</tag>
        <tag>迭代</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--回溯算法</title>
    <url>/2025/03/12day/</url>
    <content><![CDATA[回溯
组合
题目链接：77. 组合

class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; result; // 存放符合条件结果的集合    vector&lt;int&gt; path; // 用来存放符合条件结果    void backtracking(int n, int k, int startIndex) &#123;        if (path.size() == k) &#123;            result.push_back(path);            return;        &#125;        for (int i = startIndex; i &lt;= n; i++) &#123;            path.push_back(i); // 处理节点            backtracking(n, k, i + 1); // 递归            path.pop_back(); // 回溯，撤销处理的节点        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; combine(int n, int k) &#123;        result.clear(); // 可以不写        path.clear();   // 可以不写        backtracking(n, k, 1);        return result;    &#125;&#125;;
回溯剪枝
剪枝的地方就在递归中每一层的for循环所选择的起始位置。
如果for循环选择的起始位置之后的元素个数 已经不足 我们需要的元素个数了，那么就没有必要搜索了。

class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; result;    vector&lt;int&gt; path;    void backtracking(int n, int k, int startIndex) &#123;        if (path.size() == k) &#123;            result.push_back(path);            return;        &#125;        for (int i = startIndex; i &lt;= n - (k - path.size()) + 1; i++) &#123; // 优化的地方            path.push_back(i); // 处理节点            backtracking(n, k, i + 1);            path.pop_back(); // 回溯，撤销处理的节点        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; combine(int n, int k) &#123;        backtracking(n, k, 1);        return result;    &#125;&#125;;
组合总和 III
题目链接：216. 组合总和 III
class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; ans;    vector&lt;int&gt; an;    int sum=0;    void track(int n,int k,int s)&#123;        if(an.size()==k &amp;&amp; sum==n)&#123;            ans.push_back(an);            return;        &#125;        if(sum&gt;n) return;        for(int i=s;i&lt;=9;i++)&#123;            an.push_back(i);            sum+=i;            track(n,k,i+1);            sum-=i;            an.pop_back();        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123;        track(n,k,1);        return ans;    &#125;&#125;;
电话号码的字母组合
题目链接：17. 电话号码的字母组合
class Solution &#123;private:    vector&lt;string&gt; ans; // 存储最终结果，字符串组合    string combination;   // 当前正在构建的字符串组合    string phoneMap[10] = &#123;&quot;&quot;, &quot;&quot;, &quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;, &quot;jkl&quot;, &quot;mno&quot;, &quot;pqrs&quot;, &quot;tuv&quot;, &quot;wxyz&quot;&#125;; // 电话号码数字到字母的映射    void backtracking(string digits, int index) &#123;        if (index == digits.size()) &#123; // 递归终止条件：当处理完所有数字时            ans.push_back(combination); // 将当前组合添加到结果集            return;        &#125;        char digitChar = digits[index]; // 获取当前处理的数字字符        int digitIndex = digitChar - &#x27;0&#x27;; // 将数字字符转换为整数索引 (0-9)        string letters = phoneMap[digitIndex]; // 获取数字对应的字母字符串        for (char letter : letters) &#123; // 遍历当前数字对应的每个字母            combination.push_back(letter); // 将当前字母添加到组合中            backtracking(digits, index + 1); // 递归处理下一个数字，索引 + 1            combination.pop_back(); // 回溯：移除最后一个添加的字母，尝试下一个字母        &#125;    &#125;public:    vector&lt;string&gt; letterCombinations(string digits) &#123;        ans.clear(); // 清空结果集        combination.clear(); // 清空当前组合        if (digits.empty()) &#123; // 如果输入 digits 为空，直接返回空结果集            return ans;        &#125;        backtracking(digits, 0); // 从 digits 的第一个数字开始回溯，索引从 0 开始        return ans;    &#125;&#125;;
组合总和
题目链接：39. 组合总和
class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; ans;    vector&lt;int&gt; an;    void track(vector&lt;int&gt;&amp; candidates, int target, int startIndex)&#123;        if(target == 0)&#123;            ans.push_back(an);            return;        &#125;        if(target &lt; 0) return;        for(int i = startIndex; i &lt; candidates.size(); i++)&#123;            an.push_back(candidates[i]);            track(candidates, target - candidates[i], i); // 注意这里是 i，允许重复使用同一元素            an.pop_back();        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; combinationSum(vector&lt;int&gt;&amp; candidates, int target) &#123;        ans.clear();        an.clear();        track(candidates, target, 0);        return ans;    &#125;&#125;;
组合总和 II
题目链接：40. 组合总和 II
class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; ans;    vector&lt;int&gt; an;    void track(vector&lt;int&gt;&amp; candidates, int target, int inx) &#123;        if (target == 0) &#123;            ans.push_back(an);            return;        &#125;        if (target &lt; 0) return;        for (int i = inx; i &lt; candidates.size(); i++) &#123;            if (i &gt; inx &amp;&amp; candidates[i] == candidates[i - 1]) continue; // 如果该元素与左边元素相等，说明该搜索分支重复，直接跳过            an.push_back(candidates[i]);            track(candidates, target - candidates[i], i + 1);             an.pop_back();        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; combinationSum2(vector&lt;int&gt;&amp; candidates, int target) &#123;        sort(candidates.begin(), candidates.end());        track(candidates, target, 0);        return ans;    &#125;&#125;;
子集
题目链接：78. 子集
class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; ans;    vector&lt;int&gt; an;    void track(vector&lt;int&gt;&amp; nums,int idx)&#123;        int n=nums.size();        ans.push_back(an);        for(int i=idx;i&lt;n;i++)&#123;            an.push_back(nums[i]);            track(nums,i+1);            an.pop_back();        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123;        track(nums,0);        return ans;    &#125;&#125;;
子集 II
题目链接：90. 子集 II
class Solution &#123;private:    vector&lt;vector&lt;int&gt;&gt; ans;    vector&lt;int&gt; an;    void track(vector&lt;int&gt;&amp; nums,int idx)&#123;        int n=nums.size();        ans.push_back(an);        for(int i=idx;i&lt;n;i++)&#123;            if(i&gt;idx &amp;&amp; nums[i]==nums[i-1]) continue;            an.push_back(nums[i]);            track(nums,i+1);            an.pop_back();        &#125;    &#125;public:    vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt;&amp; nums) &#123;        sort(nums.begin(),nums.end());        track(nums,0);        return ans;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>递归</tag>
        <tag>回溯</tag>
        <tag>剪枝</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--图论</title>
    <url>/2025/03/13day/</url>
    <content><![CDATA[图论
图的存储方式
邻接矩阵
邻接矩阵 使用 二维数组来表示图结构。 邻接矩阵是从节点的角度来表示图，有多少节点就申请多大的二维数组。
有n 个节点，因为节点标号是从1开始的，为了节点标号和下标对齐，申请 n + 1 * n + 1  这么大的二维数组。
vector&lt;vector&lt;int&gt;&gt; graph(n + 1, vector&lt;int&gt;(n + 1, 0));while (m--) &#123;    cin &gt;&gt; s &gt;&gt; t    // 使用邻接矩阵 ，1 表示 节点s 指向 节点t    graph[s][t] = 1;&#125;
邻接表
邻接表 使用 数组 + 链表的方式来表示。 邻接表是从边的数量来表示图，有多少边 才会申请对应大小的链表。
邻接表的构造相对邻接矩阵难理解一些。

// 节点编号从1到n，所以申请 n+1 这么大的数组vector&lt;list&lt;int&gt;&gt; graph(n + 1); // 邻接表，list为C++里的链表while (m--) &#123;    cin &gt;&gt; s &gt;&gt; t;    // 使用邻接表 ，表示 s -&gt; t 是相连的    graph[s].push_back(t);&#125;
深度优先搜索三部曲

确认递归函数，参数

vector&lt;vector&lt;int&gt;&gt; result; // 收集符合条件的路径vector&lt;int&gt; path; // 0节点到终点的路径// x：目前遍历的节点// graph：存当前的图// n：终点void dfs (const vector&lt;vector&lt;int&gt;&gt;&amp; graph, int x, int n) &#123;

确认终止条件

// 当前遍历的节点x 到达节点n if (x == n) &#123; // 找到符合条件的一条路径    result.push_back(path);    return;&#125;

处理目前搜索节点出发的路径

for (int i = 1; i &lt;= n; i++) &#123; // 遍历节点x链接的所有节点    if (graph[x][i] == 1) &#123; // 找到 x链接的节点        path.push_back(i); // 遍历到的节点加入到路径中来        dfs(graph, i, n); // 进入下一层递归        path.pop_back(); // 回溯，撤销本节点    &#125;&#125;

打印结果

// 输出结果if (result.size() == 0) cout &lt;&lt; -1 &lt;&lt; endl;for (const vector&lt;int&gt; &amp;pa : result) &#123;    for (int i = 0; i &lt; pa.size() - 1; i++) &#123; // 这里指打印到倒数第二个        cout &lt;&lt; pa[i] &lt;&lt; &quot; &quot;;    &#125;    cout &lt;&lt; pa[pa.size() - 1]  &lt;&lt; endl; // 这里再打印倒数第一个，控制最后一个元素后面没有空格&#125;
例题：所有可能的路径
题目链接：797. 所有可能的路径
邻接矩阵写法
#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; result; // 收集符合条件的路径vector&lt;int&gt; path; // 1节点到终点的路径void dfs (const vector&lt;vector&lt;int&gt;&gt;&amp; graph, int x, int n) &#123;    // 当前遍历的节点x 到达节点n     if (x == n) &#123; // 找到符合条件的一条路径        result.push_back(path);        return;    &#125;    for (int i = 1; i &lt;= n; i++) &#123; // 遍历节点x链接的所有节点        if (graph[x][i] == 1) &#123; // 找到 x链接的节点            path.push_back(i); // 遍历到的节点加入到路径中来            dfs(graph, i, n); // 进入下一层递归            path.pop_back(); // 回溯，撤销本节点        &#125;    &#125;&#125;int main() &#123;    int n, m, s, t;    cin &gt;&gt; n &gt;&gt; m;    // 节点编号从1到n，所以申请 n+1 这么大的数组    vector&lt;vector&lt;int&gt;&gt; graph(n + 1, vector&lt;int&gt;(n + 1, 0));    while (m--) &#123;        cin &gt;&gt; s &gt;&gt; t;        // 使用邻接矩阵 表示无线图，1 表示 s 与 t 是相连的        graph[s][t] = 1;    &#125;    path.push_back(1); // 无论什么路径已经是从0节点出发    dfs(graph, 1, n); // 开始遍历    // 输出结果    if (result.size() == 0) cout &lt;&lt; -1 &lt;&lt; endl;    for (const vector&lt;int&gt; &amp;pa : result) &#123;        for (int i = 0; i &lt; pa.size() - 1; i++) &#123;            cout &lt;&lt; pa[i] &lt;&lt; &quot; &quot;;        &#125;        cout &lt;&lt; pa[pa.size() - 1]  &lt;&lt; endl;    &#125;&#125;
邻接表写法
#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;list&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; result; // 收集符合条件的路径vector&lt;int&gt; path; // 1节点到终点的路径void dfs (const vector&lt;list&lt;int&gt;&gt;&amp; graph, int x, int n) &#123;    if (x == n) &#123; // 找到符合条件的一条路径        result.push_back(path);        return;    &#125;    for (int i : graph[x]) &#123; // 找到 x指向的节点        path.push_back(i); // 遍历到的节点加入到路径中来        dfs(graph, i, n); // 进入下一层递归        path.pop_back(); // 回溯，撤销本节点    &#125;&#125;int main() &#123;    int n, m, s, t;    cin &gt;&gt; n &gt;&gt; m;    // 节点编号从1到n，所以申请 n+1 这么大的数组    vector&lt;list&lt;int&gt;&gt; graph(n + 1); // 邻接表    while (m--) &#123;        cin &gt;&gt; s &gt;&gt; t;        // 使用邻接表 ，表示 s -&gt; t 是相连的        graph[s].push_back(t);    &#125;    path.push_back(1); // 无论什么路径已经是从0节点出发    dfs(graph, 1, n); // 开始遍历    // 输出结果    if (result.size() == 0) cout &lt;&lt; -1 &lt;&lt; endl;    for (const vector&lt;int&gt; &amp;pa : result) &#123;        for (int i = 0; i &lt; pa.size() - 1; i++) &#123;            cout &lt;&lt; pa[i] &lt;&lt; &quot; &quot;;        &#125;        cout &lt;&lt; pa[pa.size() - 1]  &lt;&lt; endl;    &#125;&#125;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--动态规划</title>
    <url>/2025/03/14day/</url>
    <content><![CDATA[动态规划
动态规划，英文：Dynamic Programming，简称DP，如果某一问题有很多重叠子问题，使用动态规划是最有效的。所以动态规划中每一个状态一定是由上一个状态推导出来的，这一点就区分于贪心，贪心没有状态推导，而是从局部直接选最优的。
斐波那契数
题目链接：509. 斐波那契数
前几天做过一个斐波那契用递归超时，动态规划感觉就是递归加剪枝。
class Solution &#123;public:    int fib(int N) &#123;        if (N &lt;= 1) return N;        int dp[2];        dp[0] = 0;        dp[1] = 1;        for (int i = 2; i &lt;= N; i++) &#123;            int sum = dp[0] + dp[1];            dp[0] = dp[1];            dp[1] = sum;        &#125;        return dp[1];    &#125;&#125;;
爬楼梯
题目链接：70. 爬楼梯
按照题意，要上楼梯的话，第一步只有两种上法，要么走一级台阶要么走两级台阶，走了一级台阶之后，等于是还要上一个n-1级台阶的楼梯，同理，走了两级台阶的话就等于是还要上一个n-2级台阶的楼梯，以此类推这样动态规划的思想就体现出来了。
class Solution &#123;public:    int climbStairs(int n) &#123;        if (n &lt;= 1) return n;        int dp[3];        dp[1] = 1;        dp[2] = 2;        for (int i = 3; i &lt;= n; i++) &#123;            int sum = dp[1] + dp[2];            dp[1] = dp[2];            dp[2] = sum;        &#125;        return dp[2];    &#125;&#125;;
使用最小花费爬楼梯
题目链接：746. 使用最小花费爬楼梯
class Solution &#123;public:    int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123;        vector&lt;int&gt; dp(cost.size()+1);        for(int i=2;i&lt;=cost.size();i++)        &#123;            dp[i]=min(dp[i-1]+cost[i-1],dp[i-2]+cost[i-2]);        &#125;        return dp[cost.size()];    &#125;&#125;;
不同路径
题目链接：62. 不同路径
class Solution &#123;public:    int uniquePaths(int m, int n) &#123;        vector f(m + 1, vector&lt;int&gt;(n + 1));        f[0][1] = 1;        for (int i = 0; i &lt; m; i++) &#123;            for (int j = 0; j &lt; n; j++) &#123;                f[i + 1][j + 1] = f[i][j + 1] + f[i + 1][j];            &#125;        &#125;        return f[m][n];    &#125;&#125;;
比特位计数
题目链接：LCR 003. 比特位计数
除以2
如果 i 是偶数，那么 i 的二进制表示中 1 的个数与 i / 2 的二进制表示中 1 的个数相同。
如果 i 是奇数，那么 i 的二进制表示中 1 的个数比 i / 2 的二进制表示中 1 的个数多 1。
class Solution &#123;public:    vector&lt;int&gt; countBits(int n) &#123;        vector&lt;int&gt; ans(n + 1);        ans[0] = 0;        for (int i = 1; i &lt;= n; ++i) &#123;            if (i % 2 == 0) &#123;                ans[i] = ans[i / 2];            &#125; else &#123;                ans[i] = ans[i / 2] + 1;            &#125;        &#125;        return ans;    &#125;&#125;;
位运算
进行按位与运算 i &amp; (i - 1) 后，从最右边的 1 开始的所有位都会变成 0，而其左边的位保持不变。因此，i &amp; (i - 1) 的结果就是将 i 最右边的 1 去掉后得到的数字。
class Solution &#123;public:    vector&lt;int&gt; countBits(int n) &#123;        vector&lt;int&gt; ans(n + 1);        ans[0] = 0;        for (int i = 1; i &lt;= n; ++i) &#123;            ans[i] = ans[i &amp; (i - 1)] + 1;        &#125;        return ans;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--数组</title>
    <url>/2025/01/1day/</url>
    <content><![CDATA[二分查找
每次去除当前区间一半的元素，时间复杂度O(logn)，注意处理好区间。
例题
题目链接：704. 二分查找
题目描述：在严格递增的序列中找到给定的数，并返回其下标。
左闭右闭 [left,right]
class Solution &#123;public:    int search(vector&lt;int&gt;&amp; nums, int target) &#123;        int l=0,r=nums.size()-1;        while(l&lt;=r)        &#123;            int mid=(l+r)/2;            if(nums[mid]&gt;target) r=mid-1;            else if(nums[mid]&lt;target) l=mid+1;            else return mid;        &#125;        return -1;    &#125;&#125;;
左闭右开 [left,right)
class Solution &#123;public:    int search(vector&lt;int&gt;&amp; nums, int target) &#123;        int l=0,r=nums.size()-1;        while(l&lt;r) //当l=r时没有意义，所以改为l&lt;r.        &#123;            int mid=(l+r)/2;            if(nums[mid]&gt;target) r=mid;            else if(nums[mid]&lt;target) l=mid+1;            else return mid;        &#125;        return -1;    &#125;&#125;;
相关题目
35. 搜索插入位置
比较简单，处理一下返回值就可以。
34. 在排序数组中查找元素的第一个和最后一个位置
需要进行两次操作，找到左右边界，较复杂，还需要多做做。
69. x 的平方根 
我的思路是当(l-r)&lt;=1,找到的l便是整数部分。
367. 有效的完全平方数
题目还未做
双指针
一种重要的编程思想，非常高效。
例题
题目链接：https://leetcode.cn/problems/remove-element/
题目描述：给定一个数组 nums 和一个值 val，原地移除所有数值等于 val 的元素，并返回移除后数组的新长度。
左右指针法
思路：从两头向中间移动指针，当左边==val，右边！=val时，交换两个元素，边界问题不太好处理，较麻烦。
class Solution &#123;public:    int removeElement(vector&lt;int&gt;&amp; nums, int val) &#123;        int l=0,r=nums.size()-1;        while(l&lt;=r)        &#123;            while(l&lt;=r &amp;&amp; nums[l]!=val) l++;            while(l&lt;=r &amp;&amp; nums[r]==val) r--;            if(l&lt;r)&#123; swap(nums[l],nums[r]);            l++;            r--;&#125;        &#125;        return r+1;    &#125;&#125;;
快慢指针法
思路：定义一个快指针，用于一直向前循环，定义一个慢指针，当快指针指到的元素！=val时，将这个元素加入到慢指针指向的位置。快指针不会慢于慢指针，所以慢指针元素的更改就是它最后输出的数组。
class Solution &#123;public:    int removeElement(vector&lt;int&gt;&amp; nums, int val) &#123;        int fast=0,slow=0,n=nums.size();        while(fast&lt;n)        &#123;            if(nums[fast]==val) fast++;            else nums[slow++]=nums[fast++];        &#125;        return slow;    &#125;&#125;;
相关题目
26.删除有序数组中的重复项：https://leetcode.cn/problems/remove-duplicates-from-sorted-array/description/
思路：快慢指针，题解中i为快指针，n为慢指针，当快慢指针指向的元素不相等时，更新慢指针，思路和例题中的差不多。
class Solution &#123;public:    int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123;        int n=0;        for(int i=0;i&lt;nums.size();i++) if(nums[i]!=nums[n]) nums[++n]=nums[i];        return n+1;    &#125;&#125;;
283.移动零：https://leetcode.cn/problems/move-zeroes/solutions/2821184/san-chong-jie-fa-duo-yu-yan-you-pei-tu-b-1d3s/
思路：这题不太好想，前几天做过，这次看还是没思路。
class Solution &#123;public:    void moveZeroes(vector&lt;int&gt;&amp; nums) &#123;        int n=0,tem;        for(int i=0;i&lt;nums.size();i++)        &#123;            if(nums[i]!=0)            &#123;                tem=nums[i];                nums[i]=0;                nums[n++]=tem;                 &#125;        &#125;    &#125;&#125;;
977.有序数组的平方：https://leetcode.cn/problems/squares-of-a-sorted-array/
这题印象比较深刻，前几天做的时候一直想不明白我的做法的问题，后来问了学长，才知道，我因为没有定义一个新数组，导致在原数组上操作导致的问题。直接定义int数组也不行，得用vector定义一个动态数组，没接触过vector也是不太会用，一直想着看看呢，也总是不想看。当时一直以为是超出int范围了。
思路：左右指针法，两边的数的平方向中间是递减的，所以比较两端，大的那一个就是剩下元素中最大的那个。
class Solution &#123;public:    vector&lt;int&gt; sortedSquares(vector&lt;int&gt;&amp; nums) &#123;        int l = 0, r = nums.size() - 1, pos = nums.size() - 1;        vector&lt;int&gt; result(nums.size());  // 创建 int 类型的结果数组        while (l &lt;= r) &#123;            int ll = nums[l] * nums[l];              int rr = nums[r] * nums[r];              if (ll &gt;= rr) &#123;                result[pos] = ll;                l++;            &#125; else &#123;                result[pos] = rr;                r--;            &#125;            pos--;        &#125;        return result;  // 返回结果数组    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>二分查找</tag>
        <tag>双指针</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--数组</title>
    <url>/2025/01/2day/</url>
    <content><![CDATA[滑动窗口
​	刚开始不太好想到滑动窗口这种方法，即使想到了也并没有办法证明滑动窗口方法的正确性，现在我也不会证明。只是会用，前几天做过一次，这是第二次做，记住了这个方法之后也是没有什么障碍。
例题
题目链接：209. 长度最小的子数组
题目描述：找出数组中满足其总和大于等于 target 的长度最小的子数组，并返回其长度**。**如果不存在符合条件的子数组，返回 0 。
class Solution &#123;public:    int minSubArrayLen(int target, vector&lt;int&gt;&amp; nums) &#123;      int n=nums.size();      int sum=0,m=100010,j=0;      for(int i=0;i&lt;n;i++)      &#123;        sum=sum+nums[i];        while(sum&gt;=target)&#123;            m=min(m,i-j+1);            sum-=nums[j++];        &#125;      &#125;      if(m==100010) return 0;      return m;    &#125;&#125;;
相关题目
904. 水果成篮
题目描述：找一个最长连续子数组，满足子数组中至多有两种数字。返回子数组的长度。
这题题目描述很抽象，没看懂啥意思，去评论区的解释。
76. 最小覆盖子串
不会做，没思路，目前水平不够，直接跳。
矩阵规律
题目链接：59. 螺旋矩阵 II
题目描述：给定一个正整数 n ，生成一个包含 1 到 n2 所有元素，且元素按顺时针顺序螺旋排列的 n x n 正方形矩阵 matrix 。
例题
贪吃蛇输出
题目思路：直接按顺序输出，定义上下左右四个边界，从左到右输出上边界加一，从上到下输出右边界减一，从右向左输出下边界减一，从下到上输出左边界加一，依次输出就可以，因为上边界加一，从上到下就少输出一个，右边界减一，从右到左输出就少输出一个。从下向上输出时，下边界减一，上边界加一，正好按顺序输出，完美！
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; generateMatrix(int n) &#123;        vector&lt;vector&lt;int&gt;&gt; res(n, vector&lt;int&gt;(n, 0));        int l=0,r=n-1,t=0,b=n-1,count=1;        while(count&lt;=(n*n))        &#123;            for(int i=l;i&lt;=r;i++)            &#123;                res[t][i]=count;                count++;            &#125;            t++;            for(int j=t;j&lt;=b;j++)            &#123;                res[j][r]=count;                count++;            &#125;            r--;            for(int j=r;j&gt;=l;j--)            &#123;                res[b][j]=count;                count++;            &#125;            b--;            for(int j=b;j&gt;=t;j--)            &#123;                res[j][l]=count;                count++;            &#125;            l++;        &#125;        return res;    &#125;&#125;;
循环不变量原则
思路：每条边按左闭右开的输出，每圈输出四次，每次输出边长减一个，这里直接贴一下代码随想录的代码。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; generateMatrix(int n) &#123;        vector&lt;vector&lt;int&gt;&gt; res(n, vector&lt;int&gt;(n, 0)); // 使用vector定义一个二维数组        int startx = 0, starty = 0; // 定义每循环一个圈的起始位置        int loop = n / 2;         // 每个圈循环几次，例如n为奇数3，那么loop = 1 只是循环一圈，矩阵中间的值需要单独处理        int mid = n / 2;         // 矩阵中间的位置，例如：n为3， 中间的位置就是(1，1)，n为5，中间位置为(2, 2)        int count = 1; // 用来给矩阵中每一个空格赋值        int offset = 1; // 需要控制每一条边遍历的长度，每次循环右边界收缩一位        int i,j;        while (loop --) &#123;            i = startx;            j = starty;        // 下面开始的四个for就是模拟转了一圈        // 模拟填充上行从左到右(左闭右开)            for (j; j &lt; n - offset; j++) res[i][j] = count++;        // 模拟填充右列从上到下(左闭右开)            for (i; i &lt; n - offset; i++) res[i][j] = count++;        // 模拟填充下行从右到左(左闭右开)            for (; j &gt; starty; j--) res[i][j] = count++;        // 模拟填充左列从下到上(左闭右开)            for (; i &gt; startx; i--) res[i][j] = count++;        // 第二圈开始的时候，起始位置要各自加1， 例如：第一圈起始位置是(0, 0)，第二圈起始位置是(1, 1)            startx++;            starty++;        // offset 控制每一圈里每一条边遍历的长度            offset += 1;        &#125;        // 如果n为奇数的话，需要单独给矩阵最中间的位置赋值        if (n % 2) res[mid][mid] = count;        return res;    &#125;&#125;;
相关题目
54. 螺旋矩阵
进阶了一下，今天没时间了改天再做，这两天光建blog了。
LCR 146. 螺旋遍历二维数组
这题虽然是简单题，但是做起来比例题难，这题边界问题比较棘手。
class Solution &#123;public:    vector&lt;int&gt; spiralArray(vector&lt;vector&lt;int&gt;&gt;&amp; array) &#123;        if (array.empty() || array[0].empty()) &#123;            return &#123;&#125;; // 返回空 vector        &#125;        int t=0,l=0,r=array[0].size() -1,b=array.size()-1,n=array[0].size()*array.size();        vector&lt;int&gt; ans(n);        int j=0;        while(t &lt;= b &amp;&amp; l &lt;= r)&#123;            for(int i=l;i&lt;=r;i++) ans[j++]=array[t][i];            t++;            for(int i=t;i&lt;=b;i++) ans[j++]=array[i][r];            r--;            if (t &lt;= b) &#123; // 确保没有重复遍历                for (int i = r; i &gt;= l; i--) &#123;                    ans[j++] = array[b][i];                &#125;                b--;            &#125;            if (l &lt;= r) &#123; // 确保没有重复遍历                for (int i = b; i &gt;= t; i--) &#123;                    ans[j++] = array[i][l];                &#125;                l++;            &#125;        &#125;        return ans;    &#125;&#125;;
前缀和
前缀和 在涉及计算区间和的问题时非常有用！
刚开始使用的暴力解法，一直不行，知道看了题解才知道，故意卡暴力解。然后自己也想不出什么好方法，就问的chatgpt，这种方法刚开始我还以为时间复杂度差不多呢。
例题
题目链接：区间和
题目描述：给定一个整数数组 Array，请计算该数组在每个指定区间内元素的总和。
#include &lt;cstdio&gt;int main() &#123;    int n;    scanf(&quot;%d&quot;, &amp;n);    int a[n];    // 读取数组元素    for (int i = 0; i &lt; n; i++) &#123;        scanf(&quot;%d&quot;, &amp;a[i]);    &#125;    // 计算前缀和数组    int prefixSum[n + 1];  // prefixSum[0] = 0    prefixSum[0] = 0;    for (int i = 1; i &lt;= n; i++) &#123;        prefixSum[i] = prefixSum[i - 1] + a[i - 1];    &#125;    // 处理区间查询    int l, r;    while (scanf(&quot;%d %d&quot;, &amp;l, &amp;r) != EOF) &#123;        // 使用前缀和快速计算区间 [l, r] 的和        printf(&quot;%d\n&quot;, prefixSum[r + 1] - prefixSum[l]);    &#125;    return 0;&#125;
相关题目
开发商购买土地
看起来挺难的，没看懂题目意思，明天再研究。
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>数组</tag>
        <tag>滑动窗口</tag>
        <tag>矩阵规律</tag>
        <tag>前缀和</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--链表</title>
    <url>/2025/01/3day/</url>
    <content><![CDATA[链表基础
​	链表是一种通过指针串联在一起的线性结构，每一个节点由两部分组成，一个是数据域一个是指针域，最后一个节点的指针域指向null（空指针的意思）。
移除链表元素
题目链接：203. 移除链表元素
题目描述：一个链表的头节点 head 和一个整数 val ，删除链表中所有满足 Node.val == val 的节点，并返回新的头节点 。
虚拟头节点
时间复杂度：O(n)     空间复杂度：O(1)
题目思路：如果直接处理链表的话，需要考虑头节点，但是加入虚拟头节点就可以按照统一的方式去处理。
class Solution &#123;public:    ListNode* removeElements(ListNode* head, int val) &#123;        ListNode* dummy=new ListNode(0);        ListNode* pre=dummy;        pre-&gt;next=head;        while(head)        &#123;            if(head-&gt;val==val)            &#123;                pre-&gt;next=head-&gt;next;                ListNode* tem=head;                head=head-&gt;next;                delete tem;            &#125;            else            &#123;                pre=head;                head=head-&gt;next;            &#125;        &#125;        return dummy-&gt;next;    &#125;&#125;;
直接操作
时间复杂度：O(n)     空间复杂度：O(1)
class Solution &#123;public:    ListNode* removeElements(ListNode* head, int val) &#123;        // 删除头结点        while (head != NULL &amp;&amp; head-&gt;val == val) &#123; // 注意这里不是if            ListNode* tmp = head;            head = head-&gt;next;            delete tmp;        &#125;        // 删除非头结点        ListNode* cur = head;        while (cur != NULL &amp;&amp; cur-&gt;next!= NULL) &#123;            if (cur-&gt;next-&gt;val == val) &#123;                ListNode* tmp = cur-&gt;next;                cur-&gt;next = cur-&gt;next-&gt;next;                delete tmp;            &#125; else &#123;                cur = cur-&gt;next;            &#125;        &#125;        return head;    &#125;&#125;;
递归解法
时间复杂度：O(n)     空间复杂度：O(n)
题目思路：首先检查头节点的值是否为 val，如果是则移除头节点，答案即为在头节点的后续节点上递归的结果；如果头节点的值不为 val，则答案为头节点与在头节点的后续节点上递归得到的新链表拼接的结果。
class Solution &#123;public:    ListNode* removeElements(ListNode* head, int val) &#123;        // 基础情况：空链表        if (head == nullptr) &#123;            return nullptr;        &#125;        // 递归处理        if (head-&gt;val == val) &#123;            ListNode* newHead = removeElements(head-&gt;next, val);            delete head;            return newHead;        &#125; else &#123;            head-&gt;next = removeElements(head-&gt;next, val);            return head;        &#125;    &#125;&#125;;
反转链表
题目链接：206. 反转链表
题目描述：反转单链表，并返回反转后的链表。
双指针法
时间复杂度：O(n)     空间复杂度：O(1)
题目思路：定义cur和pre指针，pre初始化为NULL，cur指向head，然后反转链表，cur-&gt;next指向pre，按逻辑移动。
class Solution &#123;public:    ListNode* reverseList(ListNode* head) &#123;        ListNode* tem;        ListNode* cur=head;        ListNode* pre=NULL;        while(cur)        &#123;            tem=cur-&gt;next;            cur-&gt;next=pre;            pre=cur;            cur=tem;        &#125;        return pre;    &#125;&#125;;
递归法
思路和双指针差不多，明天研究一下。
时间复杂度：O(n)     空间复杂度：O(n)
class Solution &#123;public:    ListNode* reverse(ListNode* pre,ListNode* cur)&#123;        if(cur == NULL) return pre;        ListNode* temp = cur-&gt;next;        cur-&gt;next = pre;        // 可以和双指针法的代码进行对比，如下递归的写法，其实就是做了这两步        // pre = cur;        // cur = temp;        return reverse(cur,temp);    &#125;    ListNode* reverseList(ListNode* head) &#123;        // 和双指针法初始化是一样的逻辑        // ListNode* cur = head;        // ListNode* pre = NULL;        return reverse(NULL, head);    &#125;&#125;;
设计链表
题目链接：707. 设计链表
题目描述：获取第index个节点的值，添加头节点，添加尾节点，在第 index 个节点之前添加值为 val 的节点，删除链表中的第 index 个节点。
虚拟头节点
class MyLinkedList &#123;private:    struct LinkedNode &#123;          // 先定义 LinkedNode 结构体        int val;        LinkedNode* next;        LinkedNode(int val) : val(val), next(nullptr) &#123;&#125;    &#125;;    LinkedNode* dummyhead;       // 现在可以使用 LinkedNode 类型    int size;    public:    MyLinkedList() &#123;        dummyhead=new LinkedNode(0);        size=0;    &#125;       int get(int index) &#123;        if(index&gt;(size-1) || index&lt;0)&#123;            return -1;        &#125;        LinkedNode* cur=dummyhead-&gt;next;        while(index--)&#123;            cur=cur-&gt;next;        &#125;        return cur-&gt;val;    &#125;      void addAtHead(int val) &#123;        LinkedNode* newNode = new LinkedNode(val);        newNode-&gt;next=dummyhead-&gt;next;        dummyhead-&gt;next=newNode;        size++;    &#125;     void addAtTail(int val) &#123;        LinkedNode* newNode=new LinkedNode(val);        LinkedNode* cur=dummyhead;        while(cur-&gt;next != NULL)        &#123;            cur=cur-&gt;next;        &#125;        cur-&gt;next=newNode;        size++;    &#125;       void addAtIndex(int index, int val) &#123;        if(index&gt;size) return;        if(index&lt;0) index=0;        LinkedNode* newNode=new LinkedNode(val);        LinkedNode* cur=dummyhead;        while(index--)&#123;            cur=cur-&gt;next;        &#125;        newNode-&gt;next=cur-&gt;next;        cur-&gt;next=newNode;        size++;    &#125;    void deleteAtIndex(int index) &#123;        if(index&gt;(size-1) || index&lt;0)&#123;            return;        &#125;        LinkedNode* cur=dummyhead;        while(index--)        &#123;            cur=cur-&gt;next;        &#125;        LinkedNode* tem=cur-&gt;next;        cur-&gt;next=cur-&gt;next-&gt;next;        delete tem;        size--;    &#125;&#125;;
虚拟头节点（双链表）
还未看，直接copy的
//采用循环虚拟结点的双链表实现class MyLinkedList &#123;public:    // 定义双向链表节点结构体    struct DList &#123;        int elem; // 节点存储的元素        DList *next; // 指向下一个节点的指针        DList *prev; // 指向上一个节点的指针        // 构造函数，创建一个值为elem的新节点        DList(int elem) : elem(elem), next(nullptr), prev(nullptr) &#123;&#125;;    &#125;;    // 构造函数，初始化链表    MyLinkedList() &#123;        sentinelNode = new DList(0); // 创建哨兵节点，不存储有效数据        sentinelNode-&gt;next = sentinelNode; // 哨兵节点的下一个节点指向自身，形成循环        sentinelNode-&gt;prev = sentinelNode; // 哨兵节点的上一个节点指向自身，形成循环        size = 0; // 初始化链表大小为0    &#125;    // 获取链表中第index个节点的值    int get(int index) &#123;        if (index &gt; (size - 1) || index &lt; 0) &#123; // 检查索引是否超出范围            return -1; // 如果超出范围，返回-1        &#125;        int num;        int mid = size &gt;&gt; 1; // 计算链表中部位置        DList *curNode = sentinelNode; // 从哨兵节点开始        if (index &lt; mid) &#123; // 如果索引小于中部位置，从前往后遍历            for (int i = 0; i &lt; index + 1; i++) &#123;                curNode = curNode-&gt;next; // 移动到目标节点            &#125;        &#125; else &#123; // 如果索引大于等于中部位置，从后往前遍历            for (int i = 0; i &lt; size - index; i++) &#123;                curNode = curNode-&gt;prev; // 移动到目标节点            &#125;        &#125;        num = curNode-&gt;elem; // 获取目标节点的值        return num; // 返回节点的值    &#125;    // 在链表头部添加节点    void addAtHead(int val) &#123;        DList *newNode = new DList(val); // 创建新节点        DList *next = sentinelNode-&gt;next; // 获取当前头节点的下一个节点        newNode-&gt;prev = sentinelNode; // 新节点的上一个节点指向哨兵节点        newNode-&gt;next = next; // 新节点的下一个节点指向原来的头节点        size++; // 链表大小加1        sentinelNode-&gt;next = newNode; // 哨兵节点的下一个节点指向新节点        next-&gt;prev = newNode; // 原来的头节点的上一个节点指向新节点    &#125;    // 在链表尾部添加节点    void addAtTail(int val) &#123;        DList *newNode = new DList(val); // 创建新节点        DList *prev = sentinelNode-&gt;prev; // 获取当前尾节点的上一个节点        newNode-&gt;next = sentinelNode; // 新节点的下一个节点指向哨兵节点        newNode-&gt;prev = prev; // 新节点的上一个节点指向原来的尾节点        size++; // 链表大小加1        sentinelNode-&gt;prev = newNode; // 哨兵节点的上一个节点指向新节点        prev-&gt;next = newNode; // 原来的尾节点的下一个节点指向新节点    &#125;    // 在链表中的第index个节点之前添加值为val的节点    void addAtIndex(int index, int val) &#123;        if (index &gt; size) &#123; // 检查索引是否超出范围            return; // 如果超出范围，直接返回        &#125;        if (index &lt;= 0) &#123; // 如果索引为0或负数，在头部添加节点            addAtHead(val);            return;        &#125;        int num;        int mid = size &gt;&gt; 1; // 计算链表中部位置        DList *curNode = sentinelNode; // 从哨兵节点开始        if (index &lt; mid) &#123; // 如果索引小于中部位置，从前往后遍历            for (int i = 0; i &lt; index; i++) &#123;                curNode = curNode-&gt;next; // 移动到目标位置的前一个节点            &#125;            DList *temp = curNode-&gt;next; // 获取目标位置的节点            DList *newNode = new DList(val); // 创建新节点            curNode-&gt;next = newNode; // 在目标位置前添加新节点            temp-&gt;prev = newNode; // 目标位置的节点的前一个节点指向新节点            newNode-&gt;next = temp; // 新节点的下一个节点指向目标位置的结点            newNode-&gt;prev = curNode; // 新节点的上一个节点指向当前节点        &#125; else &#123; // 如果索引大于等于中部位置，从后往前遍历            for (int i = 0; i &lt; size - index; i++) &#123;                curNode = curNode-&gt;prev; // 移动到目标位置的后一个节点            &#125;            DList *temp = curNode-&gt;prev; // 获取目标位置的节点            DList *newNode = new DList(val); // 创建新节点            curNode-&gt;prev = newNode; // 在目标位置后添加新节点            temp-&gt;next = newNode; // 目标位置的节点的下一个节点指向新节点            newNode-&gt;prev = temp; // 新节点的上一个节点指向目标位置的节点            newNode-&gt;next = curNode; // 新节点的下一个节点指向当前节点        &#125;        size++; // 链表大小加1    &#125;    // 删除链表中的第index个节点    void deleteAtIndex(int index) &#123;        if (index &gt; (size - 1) || index &lt; 0) &#123; // 检查索引是否超出范围            return; // 如果超出范围，直接返回        &#125;        int num;        int mid = size &gt;&gt; 1; // 计算链表中部位置        DList *curNode = sentinelNode; // 从哨兵节点开始        if (index &lt; mid) &#123; // 如果索引小于中部位置，从前往后遍历            for (int i = 0; i &lt; index; i++) &#123;                curNode = curNode-&gt;next; // 移动到目标位置的前一个节点            &#125;            DList *next = curNode-&gt;next-&gt;next; // 获取目标位置的下一个节点            curNode-&gt;next = next; // 删除目标位置的节点            next-&gt;prev = curNode; // 目标位置的下一个节点的前一个节点指向当前节点        &#125; else &#123; // 如果索引大于等于中部位置，从后往前遍历            for (int i = 0; i &lt; size - index - 1; i++) &#123;                curNode = curNode-&gt;prev; // 移动到目标位置的后一个节点            &#125;            DList *prev = curNode-&gt;prev-&gt;prev; // 获取目标位置的下一个节点            curNode-&gt;prev = prev; // 删除目标位置的节点            prev-&gt;next = curNode; // 目标位置的下一个节点的下一个节点指向当前节点        &#125;        size--; // 链表大小减1    &#125;private:    int size; // 链表的大小    DList *sentinelNode; // 哨兵节点的指针&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>递归</tag>
        <tag>双指针</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--链表</title>
    <url>/2025/01/4day/</url>
    <content><![CDATA[链表操作
​	链表是一种通过指针串联在一起的线性结构，每一个节点由两部分组成，一个是数据域一个是指针域，最后一个节点的指针域指向null（空指针的意思）。
两两交换链表中的节点
题目链接：24. 两两交换链表中的节点
题目描述：两两交换链表中相邻的节点，并返回交换后链表的头节点
虚拟头节点
时间复杂度：O(n)     空间复杂度：O(1)
题目思路：直接对指针进行处理就行了。刚才随手写了一下，想着交一下看看哪里有bug呢，结果直接过了，也是挺顺的。
class Solution &#123;public:    ListNode* swapPairs(ListNode* head) &#123;        ListNode* dummy=new ListNode(0);        ListNode* pre=dummy;        dummy-&gt;next=head;        while(head &amp;&amp; head-&gt;next)        &#123;            ListNode* tmp=pre-&gt;next;            ListNode* tmp1=pre-&gt;next-&gt;next-&gt;next;            pre-&gt;next=head-&gt;next;            pre-&gt;next-&gt;next=tmp;            pre-&gt;next-&gt;next-&gt;next=tmp1;            pre=tmp;            head=tmp1;        &#125;        return dummy-&gt;next;    &#125;&#125;;
删除链表的倒数第 N 个结点
题目链接：9. 删除链表的倒数第 N 个结点
题目描述：删除链表的倒数第 n 个结点，并且返回链表的头结点。
快慢双指针法
时间复杂度：O(n)     空间复杂度：O(1)
题目思路：又给秒了，定义一个快指针，一个慢指针，快指针先移动到第n个位置，然后开始快慢指针开始同时移动，当快指针指向尾节点的时候，慢指针正好指向导数第n个位置的前一个结点，略过倒数第n个结点，就结束了。
class Solution &#123;public:    ListNode* removeNthFromEnd(ListNode* head, int n) &#123;        ListNode* dummy=new ListNode(0);        ListNode* fast=dummy;        ListNode* slow=dummy;        dummy-&gt;next=head;        for(int i=0;i&lt;=n;i++) fast=fast-&gt;next;        while(fast)        &#123;            fast=fast-&gt;next;            slow=slow-&gt;next;        &#125;        ListNode* tmp=slow-&gt;next;        slow-&gt;next=slow-&gt;next-&gt;next;        delete tmp;        return dummy-&gt;next;    &#125;&#125;;
链表相交
题目链接：面试题 02.07. 链表相交
题目描述：给定单链表的头节点 headA 和 headB ，找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。
思路1
题目思路：求两个链表交点节点的指针。题目我都看不懂！看题解秒了。
求出两个链表的长度，并求出两个链表长度的差值，然后让curA移动到，和curB 末尾对齐的位置。比较curA和curB是否相同，如果不相同，同时向后移动curA和curB，如果遇到curA == curB，则找到交点。否则循环退出返回空指针。
class Solution &#123;public:    ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) &#123;        ListNode* curA = headA;        ListNode* curB = headB;        int lenA = 0, lenB = 0;        while (curA != NULL) &#123; // 求链表A的长度            lenA++;            curA = curA-&gt;next;        &#125;        while (curB != NULL) &#123; // 求链表B的长度            lenB++;            curB = curB-&gt;next;        &#125;        curA = headA;        curB = headB;        // 让curA为最长链表的头，lenA为其长度        if (lenB &gt; lenA) &#123;            swap (lenA, lenB);            swap (curA, curB);        &#125;        // 求长度差        int gap = lenA - lenB;        // 让curA和curB在同一起点上（末尾位置对齐）        while (gap--) &#123;            curA = curA-&gt;next;        &#125;        // 遍历curA 和 curB，遇到相同则直接返回        while (curA != NULL) &#123;            if (curA == curB) &#123;                return curA;            &#125;            curA = curA-&gt;next;            curB = curB-&gt;next;        &#125;        return NULL;    &#125;&#125;;
思路二
再贴个牛逼代码。
class Solution &#123;public:    ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) &#123;        ListNode *A = headA, *B = headB;        while (A != B) &#123;            A = A != nullptr ? A-&gt;next : headB;            B = B != nullptr ? B-&gt;next : headA;        &#125;        return A;    &#125;&#125;;
环形链表
题目链接：142. 环形链表 II
题目描述：给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。
题目思路：很有生活的题，不太好想，看题解才明白怎么才能找到环的入口，也是挺有思维量的。想明白了就好了，记下来就行了。
class Solution &#123;public:    ListNode *detectCycle(ListNode *head) &#123;        ListNode* fast=head;        ListNode* slow=head;        while(fast &amp;&amp; fast-&gt;next)&#123;            fast=fast-&gt;next-&gt;next;            slow=slow-&gt;next;            if(fast==slow)&#123;                ListNode* f=fast;                ListNode* s=head;                while(f!=s)&#123;                    f=f-&gt;next;                    s=s-&gt;next;                &#125;                return s;            &#125;        &#125;        return 0;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>递归</tag>
        <tag>双指针</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--哈希表</title>
    <url>/2025/02/5day/</url>
    <content><![CDATA[哈希表
​	查询一个元素是否出现过，或者一个元素是否在集合里的时候，就要第一时间想到哈希法。一种以关键码的值**「key-value」而直接进行访问的数据结构**。
有效的字母异位词
题目链接：242. 有效的字母异位词
题目描述：给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的 字母异位词。
题目思路：创建一个哈希表，表长为26，哈希函数采用直接定址法，都不用处理冲突，还是比较简单的。直接排序，看是否两个数组相等应该也可以吧。
class Solution &#123;public:    bool isAnagram(string s, string t) &#123;        int n=s.size(),m=t.size(),q[26]=&#123;0&#125;;        if(m!=n) return false;        for(int i=0;i&lt;n;i++) q[s[i]-&#x27;a&#x27;]++;        for(int i=0;i&lt;m;i++) q[t[i]-&#x27;a&#x27;]--;        for(int i=0;i&lt;26;i++) if(q[i]!=0) return false;        return true;    &#125;&#125;;
两个数组的交集
题目链接：349. 两个数组的交集
题目描述：给定两个数组 nums1 和 nums2 ，返回它们的交集。输出结果中的每个元素一定是 唯一 的。
我的思路：感觉自己写的这个代码和屎一样，又臭又长，一点也不优雅。首先循环nums1数组，将出现的元素在s[]的位置变为1，然后再遍历第二个数组，如果出现和第一个数组相同的元素，则t[]对应位置变为1，再将这些位置存到一个新数组中，记录数组长度，然后构建一个正好长度的新数组，返回这个数组。
class Solution &#123;public:    vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;        int n=nums1.size(),m=nums2.size();        int s[1001]=&#123;0&#125;,t[1001]=&#123;-1&#125;,num=0,u=1001;        for(int i=0;i&lt;n;i++) s[nums1[i]]=1;        for(int i=0;i&lt;m;i++) if(s[nums2[i]]==1) t[nums2[i]]=1;        vector&lt;int&gt; r(u);        for(int i=0;i&lt;1001;i++)        &#123;            if(t[i]&gt;0)&#123;r[num]=i;            num++;            &#125;        &#125;        vector&lt;int&gt; v(num);        for(int i=0;i&lt;num;i++) v[i]=r[i];        return v;    &#125;&#125;;
代码随想录题解：太高级了，看都看不懂，特意搜了一下这几个函数的用法。
​	使用 unordered_set&lt;int&gt; result 来存储交集，这样可以自动去重，只保留一个 nums2 中与 nums1 相同的元素。
set1.find(num)：

find 是 unordered_set 提供的一个成员函数，用于查找一个元素。如果元素存在，它会返回指向该元素的迭代器；如果元素不存在，它会返回指向 set1.end() 的迭代器。

set1.end()：

set1.end() 返回的是一个指向 set1 容器最后一个元素之后的位置的迭代器。它并不是容器中的一个有效元素，表示容器的末尾。

set1.find(num) != set1.end()：

如果 find(num) 返回的迭代器与 set1.end() 不同，说明 num 存在于 set1 中。
如果 find(num) 返回的迭代器等于 set1.end()，说明 num 不存在于 set1 中。

class Solution &#123;public:    vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;        unordered_set&lt;int&gt; result_set;         unordered_set&lt;int&gt; nums_set(nums1.begin(), nums1.end());        for (int num : nums2) &#123;            if (nums_set.find(num) != nums_set.end()) &#123;                result_set.insert(num);            &#125;        &#125;        return vector&lt;int&gt;(result_set.begin(), result_set.end());    &#125;&#125;;
class Solution &#123;public:    vector&lt;int&gt; intersection(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;        unordered_set&lt;int&gt; result_set;         int hash[1005] = &#123;0&#125;;         for (int num : nums1) &#123;             hash[num] = 1;        &#125;        for (int num : nums2) &#123;             if (hash[num] == 1) &#123;                result_set.insert(num);            &#125;        &#125;        return vector&lt;int&gt;(result_set.begin(), result_set.end());    &#125;&#125;;
快乐数
题目链接：202. 快乐数
我的思路：我就随便试一下，就直接过了，因为不知道停止条件，所以直接设了个100次，性能还很高。
class Solution &#123;public:    bool isHappy(int n) &#123;        for(int j=0;j&lt;100;j++)        &#123;            int num=0,t;            while(n&gt;0)            &#123;t=n%10;            n=n/10;            num=num+t*t;            &#125;            n=num;            if(n==1) return true;        &#125;        return false;    &#125;&#125;;
代码随想录：题目中说了会 无限循环，那么也就是说求和的过程中，sum会重复出现，这对解题很重要！
class Solution &#123;public:    // 取数值各个位上的单数之和    int getSum(int n) &#123;        int sum = 0;        while (n) &#123;            sum += (n % 10) * (n % 10);            n /= 10;        &#125;        return sum;    &#125;    bool isHappy(int n) &#123;        unordered_set&lt;int&gt; set;        while(1) &#123;            int sum = getSum(n);            if (sum == 1) &#123;                return true;            &#125;            // 如果这个sum曾经出现过，说明已经陷入了无限循环了，立刻return false            if (set.find(sum) != set.end()) &#123;                return false;            &#125; else &#123;                set.insert(sum);            &#125;            n = sum;        &#125;    &#125;&#125;;
两数之和
题目链接：1. 两数之和
暴力解法：我直接先用暴力解写了一下,比较简单。
class Solution &#123;public:    vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123;         int n=nums.size();        for(int i=0;i&lt;n;i++)        &#123;            for(int j=i+1;j&lt;n;j++)            &#123;                if((nums[i]+nums[j])==target)                 &#123;                    return &#123;i,j&#125;;                &#125;            &#125;        &#125;        return &#123;&#125;;    &#125;&#125;;
哈希解法：思路还是挺好想的，但是代码不太会实现，不太熟练，自己写了一半，发现很多操作不会表达。直接贴一份代码随想录的代码。

数组的大小是受限制的，而且如果元素很少，而哈希值太大会造成内存空间的浪费。
set是一个集合，里面放的元素只能是一个key，而两数之和这道题目，不仅要判断y是否存在而且还要记录y的下标位置，因为要返回x 和 y的下标。所以set 也不能用。

​	此时就要选择另一种数据结构：map ，map是一种key value的存储结构，可以用key保存数值，用value再保存数值所在的下标。
map中的存储结构为 {key：数据元素，value：数组元素对应的下标}。
auto 是 C++11 引入的 类型自动推导（Type Inference） 关键字，编译器会根据变量的 初始化值 自动推导出其数据类型，而无需手动声明类型。
class Solution &#123;public:    vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123;        std::unordered_map &lt;int,int&gt; map;        for(int i = 0; i &lt; nums.size(); i++) &#123;            // 遍历当前元素，并在map中寻找是否有匹配的key            auto iter = map.find(target - nums[i]);             if(iter != map.end()) &#123;                return &#123;iter-&gt;second, i&#125;;            &#125;            // 如果没找到匹配对，就把访问过的元素和下标加入到map中            map.insert(pair&lt;int, int&gt;(nums[i], i));         &#125;        return &#123;&#125;;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>哈希表</tag>
        <tag>查找</tag>
        <tag>set</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--哈希表</title>
    <url>/2025/02/6day/</url>
    <content><![CDATA[哈希表
​	查询一个元素是否出现过，或者一个元素是否在集合里的时候，就要第一时间想到哈希法。一种以关键码的值**「key-value」而直接进行访问的数据结构**。总结
四数相加Ⅱ
题目链接：454. 四数相加 II
题目思路：我刚开始的思路是for四次，但是算了一下，可能有点超时，然后就直接看题解了。将四个数组两两分成一组进行处理，时间复杂度就是O(n*n)。

首先定义 一个unordered_map，key放a和b两数之和，value 放a和b两数之和出现的次数。
遍历大A和大B数组，统计两个数组元素之和，和出现的次数，放到map中。
定义int变量count，用来统计 a+b+c+d = 0 出现的次数。
再遍历大C和大D数组，找到如果 0-(c+d) 在map中出现过的话，就用count把map中key对应的value也就是出现次数统计出来。
最后返回统计值 count 就可以了

class Solution &#123;public:    int fourSumCount(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2, vector&lt;int&gt;&amp; nums3, vector&lt;int&gt;&amp; nums4) &#123;        unordered_map&lt;int,int&gt; umap;        for(int a : nums1) for(int b:nums2) umap[a+b]++;        int count=0;        for(int c:nums3) for(int d:nums4) if(umap.find(0-(c+d))!=umap.end()) count+=umap[0-(c+d)];        return count;    &#125;&#125;;
救赎金
题目链接：383. 赎金信
题目思路：秒了，这和 242. 有效的字母异位词 几乎差不多。
class Solution &#123;public:    bool canConstruct(string ransomNote, string magazine) &#123;        int n=magazine.size(),m=ransomNote.size();        int j[26]=&#123;0&#125;;        for(int i=0;i&lt;n;i++) j[magazine[i]-&#x27;a&#x27;]++;        for(int i=0;i&lt;m;i++) j[ransomNote[i]-&#x27;a&#x27;]--;        for(int i=0;i&lt;26;i++) if(j[i]&lt;0) return false;         return true;    &#125;&#125;;
三数之和
题目链接：15. 三数之和
题目思路1：感觉这题最不好处理的地方就是去重。三次循环，意料之中的超时了，当锻炼一下代码熟练度了。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123;        vector&lt;vector&lt;int&gt;&gt; result;        sort(nums.begin(),nums.end());        for(int i=0;i&lt;nums.size();i++)        &#123;            for(int j=i+1;j&lt;nums.size();j++)            &#123;                for(int k=j+1;k&lt;nums.size();k++)                &#123;                    if((nums[i]+nums[j]+nums[k])==0)                     result.push_back(&#123;nums[i], nums[j], nums[k]&#125;);                &#125;            &#125;        &#125;        set&lt;vector&lt;int&gt;&gt; unique_nums(result.begin(), result.end());        vector&lt;vector&lt;int&gt;&gt; result_vector(unique_nums.begin(), unique_nums.end());        return result_vector;    &#125;&#125;;
题目思路2：改了一下上一种方法，用哈希表进行处理。虽然可以通过，但是还是很慢。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123;        vector&lt;vector&lt;int&gt;&gt; result;        sort(nums.begin(),nums.end());        for(int i=0;i&lt;nums.size();i++)        &#123;            if (nums[i] &gt; 0) break;            if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1]) continue;            unordered_set&lt;int&gt; set;            for(int j=i+1;j&lt;nums.size();j++)            &#123;                int target = 0 - (nums[i] + nums[j]);                if (set.find(target) != set.end()) &#123;                    result.push_back(&#123;nums[i], target, nums[j]&#125;);                       set.erase(target);                &#125;                else &#123;                    set.insert(nums[j]);                &#125;            &#125;        &#125;        set&lt;vector&lt;int&gt;&gt; unique_nums(result.begin(), result.end());        vector&lt;vector&lt;int&gt;&gt; result_vector(unique_nums.begin(), unique_nums.end());        return result_vector;    &#125;&#125;;
双指针：代码随想录的那个代码，感觉很多地方可以优化，所以就去找了一个优化完的代码。首先先将数组排序，我们只需要输出加和等于0的元素就可以，不用管次序。排序之后就可以从两端开始操作了，先创造一个大循环，用来固定住第一个数，然后再用双指针取操作另外两个数。
优化一：当最小的三个数的和大于0时，就可以直接退出循环了。
优化二：当最大的两个数加最小的那个数，还是小于0，就可以向前移动到再大的数了。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123;        ranges::sort(nums);        vector&lt;vector&lt;int&gt;&gt; ans;        int n = nums.size();        for (int i = 0; i &lt; n - 2; i++) &#123;            int x = nums[i];            if (i &amp;&amp; x == nums[i - 1]) continue; // 跳过重复数字            if (x + nums[i + 1] + nums[i + 2] &gt; 0) break; // 优化一            if (x + nums[n - 2] + nums[n - 1] &lt; 0) continue; // 优化二            int j = i + 1, k = n - 1;            while (j &lt; k) &#123;                int s = x + nums[j] + nums[k];                if (s &gt; 0) &#123;                    k--;                &#125; else if (s &lt; 0) &#123;                    j++;                &#125; else &#123; // 三数之和为 0                    ans.push_back(&#123;x, nums[j], nums[k]&#125;);                    for (j++; j &lt; k &amp;&amp; nums[j] == nums[j - 1]; j++); // 跳过重复数字                    for (k--; k &gt; j &amp;&amp; nums[k] == nums[k + 1]; k--); // 跳过重复数字                &#125;            &#125;        &#125;        return ans;    &#125;&#125;;
四数之和
题目链接：18. 四数之和
我的思路：做了半个小时，好几个小问题，思路和上一题一样，用同样的方法，只不过多了一层循环，需要多判断一下重复条件，那几个相加超范围不太会如何处理，chatgpt让它给的方案。看了一下代码随想录的思路，差不多，它加了两行剪枝代码，但是它的代码性能有点慢，加了也不如我的这个性能高，我也在代码中加上了那两行剪枝代码。
class Solution &#123;public:    vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt;&amp; nums, int target) &#123;        int n=nums.size();        sort(nums.begin(),nums.end());        vector&lt;vector&lt;int&gt;&gt; ans;        for(int i=0;i&lt;n-3;i++)            if(i&gt;0 &amp;&amp; nums[i]==nums[i-1]) continue;            if (nums[i] &gt; target &amp;&amp; nums[i] &gt;= 0) break; //             for(int j=i+1;j&lt;n-2;j++)            &#123;                if(j&gt;i+1 &amp;&amp; nums[j]==nums[j-1]) continue;                if(nums[i]+nums[j] &gt; target &amp;&amp; nums[i]+nums[j] &gt;= 0) break; //                int c=j+1,d=n-1;                if((long long)nums[i]+nums[j]+nums[j+1]+nums[j+2]&gt;target) break;                if((long long)nums[i]+nums[j]+nums[n-1]+nums[n-2]&lt;target) continue;                while(c&lt;d)&#123;                    long long sum=(long long)nums[i]+nums[j]+nums[c]+nums[d];                    if(sum&gt;target) d--;                    else if(sum&lt;target) c++;                    else&#123;                        ans.push_back(vector&lt;int&gt; &#123;nums[i],nums[j],nums[c],nums[d]&#125;);                        while(c&lt;d &amp;&amp; nums[c]==nums[c+1]) c++;                        while(c&lt;d &amp;&amp; nums[d]==nums[d-1]) d--;                        c++;                        d--;                    &#125;                &#125;            &#125;        &#125;        return ans;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>双指针</tag>
        <tag>哈希表</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--字符串</title>
    <url>/2025/02/7day/</url>
    <content><![CDATA[字符串
​	字符串，就是由字符连接而成的序列。常见的字符串问题包括字符串匹配问题、子串相关问题、前缀/后缀相关问题、回文串相关问题、子序列相关问题等。
反转字符串
题目链接：344. 反转字符串
class Solution &#123;public:    void reverseString(vector&lt;char&gt;&amp; s) &#123;        for(int i=0,j=s.size()-1;i&lt;j;i++,j--)        &#123;            char a=s[i];            s[i]=s[j];            s[j]=a;        &#125;    &#125;&#125;;
反转字符串
题目链接：541. 反转字符串 II
注意一下reverse函数的用法。
class Solution &#123;public:    string reverseStr(string s, int k) &#123;        for (int i = 0; i &lt; s.size(); i += (2 * k)) &#123;            if (i + k &lt;= s.size()) &#123;                reverse(s.begin()  i, s.begin() + i + k );            &#125; else &#123;                reverse(s.begin() + i, s.end());            &#125;        &#125;        return s;    &#125;&#125;;
替换数字
题目链接：替换数字（第八期模拟笔试）
题目思路：看似简单，实则不好操作，gpt给的这个思路很好啊，重新弄一个新字符串然后拼接。代码随想录的那个太麻烦了，直接pass掉了。
#include &lt;iostream&gt;#include &lt;cctype&gt;  // 需要 isdigit()using namespace std;string replaceDigitsWithNumber(const string&amp; s) &#123;    string result;    for (char c : s) &#123;        if (isdigit(c)) &#123;            result += &quot;number&quot;;  // 仅替换数字        &#125; else &#123;            result += c;  // 直接添加非数字字符        &#125;    &#125;    return result;&#125;int main() &#123;    string s;    cin &gt;&gt; s;  // 读取输入    cout &lt;&lt; replaceDigitsWithNumber(s) &lt;&lt; endl;      return 0;&#125;
主要难点在于如何去掉空格，使用快慢指针。
class Solution &#123;public:    void reverse(string&amp; s, int start, int end)&#123; //翻转，区间写法：左闭右闭 []        for (int i = start, j = end; i &lt; j; i++, j--) &#123;            swap(s[i], s[j]);        &#125;    &#125;    void removeExtraSpaces(string&amp; s) &#123;//去除所有空格并在相邻单词之间添加空格, 快慢指针。        int slow = 0;   //整体思想参考https://programmercarl.com/0027.移除元素.html        for (int i = 0; i &lt; s.size(); ++i) &#123; //            if (s[i] != &#x27; &#x27;) &#123; //遇到非空格就处理，即删除所有空格。                if (slow != 0) s[slow++] = &#x27; &#x27;; //手动控制空格，给单词之间添加空格。slow != 0说明不是第一个单词，需要在单词前添加空格。                while (i &lt; s.size() &amp;&amp; s[i] != &#x27; &#x27;) &#123; //补上该单词，遇到空格说明单词结束。                    s[slow++] = s[i++];                &#125;            &#125;        &#125;        s.resize(slow); //slow的大小即为去除多余空格后的大小。    &#125;    string reverseWords(string s) &#123;        removeExtraSpaces(s); //去除多余空格，保证单词之间之只有一个空格，且字符串首尾没空格。        reverse(s, 0, s.size() - 1);        int start = 0; //removeExtraSpaces后保证第一个单词的开始下标一定是0。        for (int i = 0; i &lt;= s.size(); ++i) &#123;            if (i == s.size() || s[i] == &#x27; &#x27;) &#123; //到达空格或者串尾，说明一个单词结束。进行翻转。                reverse(s, start, i - 1); //翻转，注意是左闭右闭 []的翻转。                start = i + 1; //更新下一个单词的开始下标start            &#125;        &#125;        return s;    &#125;&#125;;
右旋字符串
题目链接：右旋字符串
题目思路：和前边有一题很类似，都是申请了一个额外的空间往上加，这题是先把后n个加上，再把前m-n个加上。
#include&lt;iostream&gt;using namespace std;int main()&#123;    int n;    string s,ans;    cin&gt;&gt;n&gt;&gt;s;    int m=s.size();    for(int i=(m-n);i&lt;m;i++) ans+=s[i];    for(int i=0;i&lt;(m-n);i++) ans+=s[i];    cout&lt;&lt;ans;    return 0;&#125;
代码随想录：它要求不申请额外的空间，这个就直接reverse两次，和上一题差不多，先全局再局部。
#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;int main() &#123;    int n;    string s;    cin &gt;&gt; n;    cin &gt;&gt; s;    int len = s.size(); //获取长度    reverse(s.begin(), s.end()); // 整体反转    reverse(s.begin(), s.begin() + n); // 先反转前一段，长度n    reverse(s.begin() + n, s.end()); // 再反转后一段    cout &lt;&lt; s &lt;&lt; endl;&#125; 
找出字符串中第一个匹配项的下标
题目链接：28. 找出字符串中第一个匹配项的下标
暴力解法：思路正确，但是我写的代码写的太麻烦了，下边贴一份优雅的代码。时间复杂度O(m*n)。
class Solution &#123;public:    int strStr(string haystack, string needle) &#123;        int s[26]=&#123;0&#125;;        if(needle.size()&gt;haystack.size()) return -1;        for(int i:needle) s[i-&#x27;a&#x27;]++;        for(int i=0;i&lt;haystack.size();i++)        &#123;            int t;            if(s[haystack[i]-&#x27;a&#x27;]==0) continue;            else&#123;                t=i;                int a=t;                for(int j=0;j&lt;needle.size();j++) if(haystack.size()-a&gt;=needle.size() &amp;&amp; haystack[a+j]==needle[j]) t++;            &#125;            if((t-i)==needle.size()) return i;        &#125;        return -1;    &#125;&#125;;
class Solution &#123;public:    int strStr(string s, string p) &#123;        int n = s.size(), m = p.size();        for(int i = 0; i &lt;= n - m; i++)&#123;            int j = i, k = 0;             while(k &lt; m and s[j] == p[k])&#123;                j++;                k++;            &#125;            if(k == m) return i;        &#125;        return -1;    &#125;&#125;;
**KMP解法：**当出现字符串不匹配时，可以记录一部分之前已经匹配的文本内容，利用这些信息避免从头再去做匹配。
前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。代码随想录写的这个就很通俗易懂。
Nex数组构造：

初始化
处理前后缀不相同的情况
处理前后缀相同的情况


class Solution &#123;public:    void getNext(int* next, const string&amp; s) &#123;        int j = -1;        next[0] = j;        for(int i = 1; i &lt; s.size(); i++) &#123; // 注意i从1开始            while (j &gt;= 0 &amp;&amp; s[i] != s[j + 1]) &#123; // 前后缀不相同了                j = next[j]; // 向前回退            &#125;            if (s[i] == s[j + 1]) &#123; // 找到相同的前后缀                j++;            &#125;            next[i] = j; // 将j（前缀的长度）赋给next[i]        &#125;    &#125;    int strStr(string haystack, string needle) &#123;        if (needle.size() == 0) &#123;            return 0;        &#125;		vector&lt;int&gt; next(needle.size());		getNext(&amp;next[0], needle);        int j = -1; // // 因为next数组里记录的起始位置为-1        for (int i = 0; i &lt; haystack.size(); i++) &#123; // 注意i就从0开始            while(j &gt;= 0 &amp;&amp; haystack[i] != needle[j + 1]) &#123; // 不匹配                j = next[j]; // j 寻找之前匹配的位置            &#125;            if (haystack[i] == needle[j + 1]) &#123; // 匹配，j和i同时向后移动                j++; // i的增加在for循环里            &#125;            if (j == (needle.size() - 1) ) &#123; // 文本串s里出现了模式串t                return (i - needle.size() + 1);            &#125;        &#125;        return -1;    &#125;&#125;;
重复的子字符串
题目链接：459. 重复的子字符串
暴力解法：想用暴力法写一下，结果写了两个小时，还是看了题解写出来的，钻牛角尖了。我写的代码性能不如官方代码高，贴一下官方代码。
class Solution &#123;public:    bool repeatedSubstringPattern(string s) &#123;        int n = s.size();        for (int i = 1; i * 2 &lt;= n; ++i) &#123;            if (n % i == 0) &#123;                bool match = true;                for (int j = i; j &lt; n; ++j) &#123;                    if (s[j] != s[j - i]) &#123;                        match = false;                        break;                    &#125;                &#125;                if (match) &#123;                    return true;                &#125;            &#125;        &#125;        return false;    &#125;&#125;;
移动匹配：将两个 s 连在一起，并移除第一个和最后一个字符。如果 s 是该字符串的子串，那么 s 就满足题目要求。
class Solution &#123;public:    bool repeatedSubstringPattern(string s) &#123;        string t = s + s;        t.erase(t.begin()); t.erase(t.end() - 1); // 掐头去尾        if (t.find(s) != std::string::npos) return true; // r        return false;    &#125;&#125;;
KMP实现：
class Solution &#123;public:    void getNext (int* next, const string&amp; s)&#123;        next[0] = -1;        int j = -1;        for(int i = 1;i &lt; s.size(); i++)&#123;            while(j &gt;= 0 &amp;&amp; s[i] != s[j + 1]) &#123;                j = next[j];            &#125;            if(s[i] == s[j + 1]) &#123;                j++;            &#125;            next[i] = j;        &#125;    &#125;    bool repeatedSubstringPattern (string s) &#123;        if (s.size() == 0) &#123;            return false;        &#125;        int next[s.size()];        getNext(next, s);        int len = s.size();        if (next[len - 1] != -1 &amp;&amp; len % (len - (next[len - 1] + 1)) == 0) &#123;            return true;        &#125;        return false;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>双指针</tag>
        <tag>字符串</tag>
        <tag>KMP</tag>
        <tag>reverse</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--栈·队列</title>
    <url>/2025/02/8day/</url>
    <content><![CDATA[栈·队列
​                                                                                    
stack
STL 中的 stack 容器提供了一众成员函数以供调用，其中较为常用的有：

元素访问

st.top() 返回栈顶


修改

st.push() 插入传入的参数到栈顶
st.pop() 弹出栈顶


容量

st.empty() 返回是否为空
st.size() 返回元素数量



queue
STL 中的 queue 容器提供了一众成员函数以供调用。其中较为常用的有：

元素访问

q.front() 返回队首元素
q.back() 返回队尾元素


修改

q.push() 在队尾插入元素
q.pop() 弹出队首元素


容量

q.empty() 队列是否为空
q.size() 返回队列中元素的数量



deque
STL 中的 deque 容器提供了一众成员函数以供调用。其中较为常用的有：

元素访问

q.front() 返回队首元素
q.back() 返回队尾元素


修改

q.push_back() 在队尾插入元素
q.pop_back() 弹出队尾元素
q.push_front() 在队首插入元素
q.pop_front() 弹出队首元素
q.insert() 在指定位置前插入元素（传入迭代器和元素）
q.erase() 删除指定位置的元素（传入迭代器）


容量

q.empty() 队列是否为空
q.size() 返回队列中元素的数量



此外，deque 还提供了一些运算符。其中较为常用的有：


使用赋值运算符 = 为 deque 赋值，类似 queue。


使用 [] 访问元素，类似 vector。


用栈实现队列
题目链接：232. 用栈实现队列
class MyQueue &#123;private:    // 如果 B 栈为空，将 A 栈中的元素倒入 B 栈    void moveToB() &#123;        if (B.empty()) &#123;            while (!A.empty()) &#123;                B.push(A.top());                A.pop();            &#125;        &#125;    &#125;public:    stack&lt;int&gt; A, B;    MyQueue() &#123;    &#125;    void push(int x) &#123;        A.push(x);    &#125;    int pop() &#123;        moveToB(); // 移动元素到 B 栈        int val = B.top();        B.pop();        return val;    &#125;    int peek() &#123;        moveToB(); // 移动元素到 B 栈        return B.top();    &#125;    bool empty() &#123;        return A.empty() &amp;&amp; B.empty();    &#125;&#125;;
用队列实现栈
题目链接：225. 用队列实现栈
方法一：使用 一个队列

push(x)：

首先，将元素 x 入队到队列末尾。
然后，将队列中前面已有的元素依次出队，再重新入队到队列末尾。
这样，队列的前端就始终是最新压入的元素，相当于实现了栈的“后进先出（LIFO）”顺序。


pop()：

直接将队首元素出队即可。因为在上一步的处理里，队首元素就是栈顶。


top()：

返回队首元素（不弹出）。


empty()：

判断队列是否为空。



class MyStack &#123;public:    queue&lt;int&gt; q;    MyStack() &#123;    &#125;        // 每次 push 后，把前面的 (size - 1) 个元素依次出队并重新入队，确保新元素在队首    void push(int x) &#123;        // 1. 新元素先入队        q.push(x);        // 2. 将前面的元素依次移到末尾        int n = q.size();        // 只需要将前面的 (n - 1) 个元素移动到末尾        for(int i = 0; i &lt; n - 1; i++) &#123;            q.push(q.front());            q.pop();        &#125;    &#125;        int pop() &#123;        int val = q.front();  // 由于 push 中的旋转操作，队首就是栈顶        q.pop();        return val;    &#125;        int top() &#123;        return q.front();     // 队首即为栈顶    &#125;        bool empty() &#123;        return q.empty();    &#125;&#125;;
方法二：使用 两个队列

队列 A 和 队列 B 协同工作，保证其中一个队列始终存放当前所有元素，另一个队列用来做辅助操作。
push(x)：

把新元素直接入队到空队列中（或者你可以总是把它放到 A，然后把 A 里旧的元素转移到 B，具体实现略有差异）。


pop()：

在 pop() 之前，我们需要把除最后一个元素外的所有元素，从一个队列（有数据的队列）依次出队并入队到另一个空队列，让最后一个元素留在原队列，然后弹出这个元素，这相当于栈顶元素。


top()：

类似 pop()，只是查看最后一个元素而不把它真正弹出。


empty()：

两个队列都为空即为真。



#include &lt;queue&gt;using namespace std;class MyStack &#123;public:    queue&lt;int&gt; q1, q2;  // 两个队列    MyStack() &#123;    &#125;        void push(int x) &#123;        // 1. 新元素先入空的队列 q2        q2.push(x);        // 2. 把 q1 中所有元素依次移到 q2 中        while (!q1.empty()) &#123;            q2.push(q1.front());            q1.pop();        &#125;        // 3. 交换 q1 和 q2，保证所有元素又回到 q1，q2 变空        swap(q1, q2);    &#125;        int pop() &#123;        // 队首就是栈顶        int val = q1.front();        q1.pop();        return val;    &#125;        int top() &#123;        return q1.front();    &#125;        bool empty() &#123;        return q1.empty() &amp;&amp; q2.empty();    &#125;&#125;;
有效的括号
题目链接：20. 有效的括号
题目思路：先判断一下s的长度是否为偶数，如果不是就返回false，再将全部的左符号压入栈中，然后开始判断剩余的符号是否可以匹配，如果不能就返回false，将s循环结束，如果全部匹配成功，最后应该栈是空的，如果不是返回false。
class Solution &#123;public:    bool isValid(string s) &#123;        int n=s.size();        if(n%2!=0) return false;        stack&lt;char&gt; t;        for(int i=0;i&lt;n;i++)        &#123;            if(s[i]==&#x27;(&#x27;) t.push(&#x27;)&#x27;);            else if(s[i]==&#x27;&#123;&#x27;) t.push(&#x27;&#125;&#x27;);            else if(s[i]==&#x27;[&#x27;) t.push(&#x27;]&#x27;);            else if(t.empty() || t.top()!=s[i]) return false;            else t.pop();        &#125;        return t.empty();    &#125;&#125;;
删除字符串中的所有相邻重复项
题目链接：1047. 删除字符串中的所有相邻重复项
建栈的写法，性能太差了。
class Solution &#123;public:    string removeDuplicates(string s) &#123;        stack&lt;char&gt; t;        int n=s.size();        for(int i=0;i&lt;n;i++)        &#123;            if(!t.empty() &amp;&amp; s[i]==t.top()) t.pop();            else t.push(s[i]);         &#125;         string result;        while(!t.empty())        &#123;            result=t.top()+result;            t.pop();        &#125;        return result;    &#125;&#125;;
string自己就可以做栈，string有pop和push接口，性能很高。
class Solution &#123;public:    string removeDuplicates(string s) &#123;        string stk;        for (char ch : s) &#123;            if (!stk.empty() &amp;&amp; stk.back() == ch) &#123;                stk.pop_back();            &#125; else &#123;                stk.push_back(ch);            &#125;        &#125;        return stk;    &#125;&#125;;
逆波兰表达式求值
题目链接：150. 逆波兰表达式求值
题目挺简单的，就是一个点需要注意：
stoi()：

stoi 是一个将 std::string 类型转换为 int 的函数。如果 tokens[i] 是一个有效的整数表示，stoi(tokens[i]) 将返回对应的整数值。

class Solution &#123;public:    int evalRPN(vector&lt;string&gt;&amp; tokens) &#123;        stack&lt;int&gt; t;        int n=tokens.size(),re;        for(int i=0;i&lt;n;i++)        &#123;            if(tokens[i]==&quot;+&quot;)             &#123;                int a=t.top();t.pop();                int b=t.top();t.pop();                re=a+b;                t.push(re);            &#125;            else if(tokens[i]==&quot;-&quot;)            &#123;                int a=t.top();t.pop();                int b=t.top();t.pop();                re=b-a;                t.push(re);            &#125;            else if(tokens[i]==&quot;*&quot;)            &#123;                int a=t.top();t.pop();                int b=t.top();t.pop();                re=b*a;                t.push(re);            &#125;            else if(tokens[i]==&quot;/&quot;)            &#123;                int a=t.top();t.pop();                int b=t.top();t.pop();                re=b/a;                t.push(re);            &#125;            else t.push(stoi(tokens[i]));        &#125;        return t.top();    &#125;&#125;;
滑动窗口最大值
题目链接：239. 滑动窗口最大值
双端队列：队列中元素个数大于k，就删除队首元素，若入队元素大于队列中的元素，则把队列中的元素弹出。
class Solution &#123;public:    vector&lt;int&gt; maxSlidingWindow(vector&lt;int&gt;&amp; nums, int k) &#123;        vector&lt;int&gt; ans;        deque&lt;int&gt; q; // 双端队列        for (int i = 0; i &lt; nums.size(); i++) &#123;            // 1. 入            while (!q.empty() &amp;&amp; nums[q.back()] &lt;= nums[i]) &#123;                q.pop_back(); // 维护 q 的单调性            &#125;            q.push_back(i); // 入队            // 2. 出            if (i - q.front() &gt;= k) &#123; // 队首已经离开窗口了                q.pop_front();            &#125;            // 3. 记录答案            if (i &gt;= k - 1) &#123;                // 由于队首到队尾单调递减，所以窗口最大值就是队首                ans.push_back(nums[q.front()]);            &#125;        &#125;        return ans;    &#125;&#125;;
前 K 个高频元素
题目链接：347. 前 K 个高频元素
堆是一棵完全二叉树，树中每个结点的值都不小于（或不大于）其左右孩子的值。 如果父亲结点是大于等于左右孩子就是大顶堆，小于等于左右孩子就是小顶堆。小顶堆每次将最小的元素弹出，最后小顶堆里积累的才是前k个最大元素。

class Solution &#123;public:    // 小顶堆    class mycomparison &#123;    public:        bool operator()(const pair&lt;int, int&gt;&amp; lhs, const pair&lt;int, int&gt;&amp; rhs) &#123;            return lhs.second &gt; rhs.second;        &#125;    &#125;;    vector&lt;int&gt; topKFrequent(vector&lt;int&gt;&amp; nums, int k) &#123;        // 要统计元素出现频率        unordered_map&lt;int, int&gt; map; // map&lt;nums[i],对应出现的次数&gt;        for (int i = 0; i &lt; nums.size(); i++) &#123;            map[nums[i]]++;        &#125;        // 对频率排序        // 定义一个小顶堆，大小为k        priority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, mycomparison&gt; pri_que;        // 用固定大小为k的小顶堆，扫面所有频率的数值        for (unordered_map&lt;int, int&gt;::iterator it = map.begin(); it != map.end(); it++) &#123;            pri_que.push(*it);            if (pri_que.size() &gt; k) &#123; // 如果堆的大小大于了K，则队列弹出，保证堆的大小一直为k                pri_que.pop();            &#125;        &#125;        // 找出前K个高频元素，因为小顶堆先弹出的是最小的，所以倒序来输出到数组        vector&lt;int&gt; result(k);        for (int i = k - 1; i &gt;= 0; i--) &#123;            result[i] = pri_que.top().first;            pri_que.pop();        &#125;        return result;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>栈</tag>
        <tag>队列</tag>
        <tag>双端队列</tag>
        <tag>堆</tag>
        <tag>queue</tag>
        <tag>stack</tag>
        <tag>deque</tag>
      </tags>
  </entry>
  <entry>
    <title>代码随想录--二叉树</title>
    <url>/2025/02/9day/</url>
    <content><![CDATA[二叉树前中后序遍历
二叉树定义
struct TreeNode &#123;    int val;    TreeNode *left;    TreeNode *right;    TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;&#125;;
二叉树的前序遍历
题目链接：144. 二叉树的前序遍历
递归遍历
class Solution &#123;public:    void traversal(TreeNode* cur, vector&lt;int&gt;&amp; vec) &#123;        if (cur == NULL) return;        vec.push_back(cur-&gt;val);    // 中        traversal(cur-&gt;left, vec);  // 左        traversal(cur-&gt;right, vec); // 右    &#125;    vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; result;        traversal(root, result);        return result;    &#125;&#125;;
迭代遍历

class Solution &#123;public:    vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; res;        stack&lt;TreeNode*&gt; s;        if(root==NULL) return res;        s.push(root);        while(!s.empty())&#123;            TreeNode* node= s.top();            s.pop();            res.push_back(node-&gt;val);            if(node-&gt;right) s.push(node-&gt;right);            if(node-&gt;left) s.push(node-&gt;left);        &#125;        return res;    &#125;&#125;;
统一迭代法
每次加入一个右中左，接着将中全部输出，最后再按顺序输出栈里的内容，思路很直观，另外两种遍历也是如此。
class Solution &#123;public:    vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; result;        stack&lt;TreeNode*&gt; st;        if (root != NULL) st.push(root);        while (!st.empty()) &#123;            TreeNode* node = st.top();            if (node != NULL) &#123;                st.pop();                if (node-&gt;right) st.push(node-&gt;right);  // 右                if (node-&gt;left) st.push(node-&gt;left);    // 左                st.push(node);                          // 中                st.push(NULL);            &#125; else &#123;                st.pop();                node = st.top();                st.pop();                result.push_back(node-&gt;val);            &#125;        &#125;        return result;    &#125;&#125;;
二叉树的中序遍历
题目链接：94. 二叉树的中序遍历
递归遍历
class Solution &#123;public:    void traversal(TreeNode* cur, vector&lt;int&gt;&amp; vec) &#123;        if (cur == NULL) return;        traversal(cur-&gt;left, vec);  // 左        vec.push_back(cur-&gt;val);    // 中        traversal(cur-&gt;right, vec); // 右    &#125;    vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; result;        traversal(root, result);        return result;    &#125;&#125;;
迭代遍历

class Solution &#123;public:    vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; res;        stack&lt;TreeNode*&gt; s;        TreeNode* cur = root;                while (cur != nullptr || !s.empty()) &#123;            // 将当前节点及其左子树压入栈            while (cur != nullptr) &#123;                s.push(cur);                cur = cur-&gt;left;            &#125;            cur = s.top();            s.pop();            res.push_back(cur-&gt;val);            cur = cur-&gt;right;        &#125;         return res;    &#125;&#125;;
统一迭代法

class Solution &#123;public:    vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; result;        stack&lt;TreeNode*&gt; st;        if (root != NULL) st.push (root);        while (!st.empty()) &#123;            TreeNode* node = st.top();            if (node != NULL) &#123;                st.pop(); // 将该节点弹出，避免重复操作，下面再将右中左节点添加到栈中                if (node-&gt;right) st.push(node-&gt;right);  // 添加右节点（空节点不入栈）                st.push(node);                          // 添加中节点                st.push(NULL); // 中节点访问过，但是还没有处理，加入空节点做为标记。                if (node-&gt;left) st.push(node-&gt;left);    // 添加左节点（空节点不入栈）            &#125; else &#123; // 只有遇到空节点的时候，才将下一个节点放进结果集                st.pop();           // 将空节点弹出                node = st.top();    // 重新取出栈中元素                st.pop();                result.push_back(node-&gt;val); // 加入到结果集            &#125;        &#125;        return result;    &#125;&#125;;
二叉树的后序遍历
题目链接：145. 二叉树的后序遍历
递归遍历
class Solution &#123;public:    void traversal(TreeNode *root,vector&lt;int&gt;&amp; res)&#123;        if(root==NULL) return;        traversal(root-&gt;left,res);        traversal(root-&gt;right,res);        res.push_back(root-&gt;val);    &#125;    vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; re;        traversal(root,re);        return re;    &#125;&#125;;
迭代遍历
class Solution &#123;public:    vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123;        vector&lt;int&gt; res;        stack&lt;TreeNode*&gt; s;        if(root==NULL) return res;        s.push(root);        while(!s.empty())&#123;            TreeNode* node=s.top();            s.pop();            res.push_back(node-&gt;val);            if(node-&gt;left) s.push(node-&gt;left);            if(node-&gt;right) s.push(node-&gt;right);        &#125;        reverse(res.begin(),res.end());        return res;    &#125;&#125;;
统一迭代法
class Solution &#123;public:    vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123;        stack&lt;TreeNode*&gt; s;        vector&lt;int&gt; res;        if(root==NULL) return res;        s.push(root);        while(!s.empty())        &#123;            TreeNode* node =s.top();            if(node!=NULL)            &#123;                s.pop();                s.push(node);                s.push(NULL);                if(node-&gt;right) s.push(node-&gt;right);                if(node-&gt;left) s.push(node-&gt;left);            &#125;            else&#123;                s.pop();                node =s.top();                s.pop();                res.push_back(node-&gt;val);            &#125;        &#125;        return res;    &#125;&#125;;
]]></content>
      <categories>
        <category>算法刷题</category>
        <category>代码随想录</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>二叉树</tag>
        <tag>二叉树遍历</tag>
        <tag>递归</tag>
        <tag>迭代</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：DeepSeekMath-v2 自我验证（Verifier &amp; Meta-Verifier）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9ADeepSeekMath-v2%20%E8%87%AA%E6%88%91%E9%AA%8C%E8%AF%81%EF%BC%88Verifier%20&amp;%20Meta-Verifier%EF%BC%89/</url>
    <content><![CDATA[这一篇对应视频 06：“DeepSeekMath-v2, 自我验证, verifier &amp; meta verifier，验证&amp;微调迭代”（BV1AaSTBEEeS）。
我会把它按“论文精度”的方式拆开讲清楚：为什么 verifier 必须升级成 verifier + meta-verifier，以及如何把“外部验证能力”逐步迁移到 generator（自我验证 / self-verification）上，从而降低 RL4LLM 中最典型的 reward hacking。
系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）



0. 资料对齐（视频 + GitHub）

视频 06：BV1AaSTBEEeS

https://www.bilibili.com/video/BV1AaSTBEEeS/


配套 notebook（视频简介里直接给了路径）：

agentic_rl/reasoning/math/deepseekmath-v2.ipynb
你本地路径：/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reasoning/math/deepseekmath-v2.ipynb



本文会以该 notebook 的符号与公式为主，补上视频里强调的工程直觉：rollout case 分析 -&gt; 发现 hacking -&gt; 改 reward/rubric -&gt; 再训练。

1. 先把问题说清楚：reward hacking 不是“模型坏”，是 reward 不够 faithful
RL4LLM/Agentic RL 里最危险的失败模式不是“优化不收敛”，而是：

policy 学会了“骗判卷人”（reward model / verifier），reward 曲线很好看，但答案是错的。

从视频的表述来看，核心原则只有一句：

external reward signal 必须足够 faithful：正确要高分，错误要低分。

但现实里你很难一步到位写出完全 faithful 的 reward。最常见的工作流是：

identify：看你的 rollouts（模型输出样例）里出现了哪些“投机取巧”的模式。
solve：在 reward function / rubrics 里加约束项，专门打击这些 hack。
重新训练，继续循环。

DeepSeekMath-v2 的 verifier + meta-verifier 本质上就是这个循环的“系统化版本”：它试图让判卷人自己也更难被 hack。

2. Generator + Verifier：为什么这是一个稳健的 agentic workflow
把它抽象成最简单的两角色系统：

generator：产出解题过程/证明（proof）
verifier：检查 proof，指出问题并打分

这个“生成 -&gt; 验证 -&gt; 改写/精炼”的闭环，本质上就是 agentic loop，只不过工具不是检索/代码执行，而是“数学验证”。
重要直觉（视频强调）：iterative verification &amp; refinement 本身就是提升 generation 质量的过程。

3. Initial Verifier：输入/输出长什么样，怎么训练
3.1 Verifier 的输入与输出
用 notebook 的符号：

高层 rubrics：$I_v$（文本 prompt 驱动）
verifier policy：$\pi_\varphi(\cdot\mid X,Y,I_v)$
输出包含两部分：

$V$：先总结识别到的问题（analysis）
$s’$：再给一个分数（论文里用三档离散分：$s’\in{0,0.5,1}$）



这里的关键不是“有打分”，而是 verifier 必须把“为什么这么判”说出来，这给后续的 refinement 提供了可操作的 feedback。
3.2 Verifier 的训练数据
数据集形式：

$\mathcal D_v={(X_i,Y_i,s_i)}$
$Y_i$：generator 产生的 proof
$s_i$：数学专家给的标注分数

3.3 为什么 verifier 也用 RL（而不是只做 SFT）
notebook 给的训练目标是一个 RL 结构（它把“格式正确”和“打分正确”拆成两个 reward）：
$$\max_{\pi_\varphi}\ \mathbb{E}_{(X_i,Y_i,s_i)\sim\mathcal D_v,\ (V’_i,s’_i)\sim\pi_\varphi(\cdot\mid X_i,Y_i,I_v)}[R_{\text{format}}(V’_i)\cdot R_{\text{score}}(s’_i,s_i)]$$
直觉：

$R_{format}$：保证输出结构可解析（例如必须先列问题再给分数）
$R_{score}$：保证分数和专家标注一致

这点对“工程可控”非常重要：你的 verifier 如果输出格式漂了，整个 pipeline 会直接崩。

4. 为什么必须引入 Meta-Verifier：verifier 也会“判错”
视频里给了一个非常关键的 failure case：

verifier 指出了某些问题，但这些“问题本身是错的”。

也就是说，verifier 可能在“自信地胡说”。如果你把它当 reward model 去训 generator，generator 会被强行推向错误偏好。
因此 DeepSeekMath-v2 再加一层：

meta-verifier：专门判“verifier 的分析是否靠谱”。


5. Meta-Verifier：它判什么，怎么训练
5.1 Meta-Verifier 的输入与输出
符号对齐 notebook：

rubrics：$I_{mv}$（文本 prompt 驱动）
数据集：$\mathcal D_{mv}={(X_i,Y_i,V_i,ms_i)}$

$ms_i$：数学专家对 verifier 分析 $V_i$ 的打分标注（meta score）


meta-verifier policy：$\pi_\eta(\cdot\mid X_i,Y_i,V_i,I_{mv})$
输出包含：

对 verifier 分析“本身的问题”的总结
一个质量分（衡量 verifier 的分析是否准确、是否有充分依据）



5.2 训练目标
notebook 里写得很直接：meta-verifier 的 RL objective 结构和 verifier 训练相同，也是 format + score 的组合（只不过 score 的对齐对象变成了 $ms_i$）。

6. Final Verifier：把三件事乘在一起，才叫“更难被 hack 的 reward”
notebook 给了 final verifier 的组合方式：
$$R_v = R_{\text{format}}\cdot R_{\text{score}}\cdot R_{\text{meta}}$$
你可以把它理解成“三道闸门”：

format 过关：输出结构必须可用
score 过关：判分必须接近专家标注
meta 过关：判卷人的分析必须也被“更强的判卷人”认可

这一步的核心作用是：把 reward hacking 的空间压缩。generator 想“骗分”，得同时骗过 verifier 和 meta-verifier，这会显著提高作弊成本。
6.1 乘法组合不是银弹：它会让 reward 变稀疏，训练更依赖 curriculum
把多个 reward “相乘”相当于设置了多个硬闸门：任何一个环节失败，整体 reward 就会被压到接近 0。它确实能显著减少“只骗过其中一项”的漏洞，但也带来一个很现实的工程副作用：

早期训练很容易全是 0 分：generator 还不够强时，format/score/meta 任意一项失败都会把信号切断，导致学习信号稀疏。
你会更依赖 curriculum：先把 format 训稳（让输出可解析），再逐步提高 score/meta 的要求；否则你可能会误以为“RL 不 work”，其实只是 reward 太硬。
错误归因更难：reward 变成 0 之后，你需要额外的日志把三项分别打出来，否则很难知道到底是格式挂了、判分挂了，还是 meta-verifier 在否决分析。

如果你要把这套思想迁移到 deep research agent，建议你一开始就把 reward 拆开记录（哪怕最终训练用乘法），让 case analysis 可定位。

7. Proof Generator 的训练：把 verifier 当成 reward model
generator policy：$\pi_\theta(\cdot\mid X)$，目标是最大化 verifier 给出的 reward：
$$\max_{\pi_\theta}\ \mathbb{E}_{X_i\sim\mathcal D_p,\ Y_i\sim\pi_\theta(\cdot\mid X_i)}[R_Y]$$
其中 $R_Y$ 来自 verifier（对齐 notebook：$R_Y=\pi_\phi(\cdot\mid X_i,Y_i,I_v)$，可理解为“final verifier 对 proof 的打分/奖励”）。
这里你应该马上联想到 RLHF：只是 RM 从“偏好模型”换成了“数学 verifier 系统”。

8. Toward Self-Verification：把验证能力迁移回 generator
视频里用“学生 vs 外部检查员”的直觉解释得很好：

generator（学生）往往“有能力改正错误”（能根据外部反馈 refine）
但它缺乏自己发现自己错误的能力，尤其当它既当作者又当检查员时，很难像外部 verifier 一样严厉客观

所以论文做的是：让 generator 不只输出 proof $Y$，还输出一段自我分析/自我验证 $Z$。
notebook 给出的组合 reward（我把 align 改成单行，避免 Hexo 渲染问题）：
$$R = R_{\text{format}}(Y,Z)\cdot(\alpha\cdot R_Y + \beta\cdot R_Z)$$
其中：
$$R_Z = R_{\text{score}}(s’,s)\cdot R_{\text{meta}}(Z)$$
解释：

$R_Y$：对 proof 本身的奖励（外部 verifier）
$R_Z$：对自我分析/自我验证的奖励（用 meta-verifier 判“分析质量”）
$\alpha,\beta$：权衡“答案质量”和“自我验证质量”

这一步非常关键：它让 generator 学到的不只是“把题做对”，还包括“把自己为什么做对/哪里可能错讲清楚”，从而逐步拥有 genuine verification capabilities。

9. 迭代验证 + 迭代微调：你应该从这篇论文学到的工程方法
我把视频里强调的做法总结成一个可复用模板（不只适用于数学）：

先把 task 变成“可验证”或“伪可验证”

数学题天然可验证；开放领域任务需要你设计 proxy verifier（引用一致性、可执行性、事实核查、结构约束）。


先别急着训 generator

先把 verifier 做到“基本 faithful”，否则 RL 一定会把 generator 训歪。


verifier 一定要有结构化输出

否则你没法把它接入 refinement loop，更没法做 case analysis。


用 meta-verifier 约束 verifier

判卷人也会被 hack；对判卷人做“二次判卷”是降风险的关键。


不断循环 case 分析

rollout -&gt; 找 hack -&gt; 改 rubrics/加惩罚 -&gt; 继续训练。



9.1 我会如何做评测与 ablation（防“判卷系统”自嗨）
把 verifier/meta-verifier 引入训练闭环之后，一个常见风险是：系统内部的分数变得越来越高，但它只是学会了“更好地说服判卷人”，而不是更正确。为了避免这种自嗨，我会强制做几类评测与消融：

相关性与校准（calibration）

在 holdout 数据上测 verifier 分数和真实 correctness 的相关性（AUC/accuracy/分桶统计）。
重点看“高分但错”的比例：这是 reward hacking 的温床。


对抗样本

专门构造“格式完美但逻辑错误”的 proof，测试 format reward 是否掩盖了 correctness。
对 open-domain 任务，加 prompt injection / citation hallucination / tool-output 篡改，测试 judge 是否会被带偏。


组件消融

去掉 meta-verifier、去掉 format gate、把乘法改加法、只训 score 不训 analysis，分别观察：

generator 的正确率、分布多样性、训练稳定性（是否更容易崩/更容易 hack）。




训练外泛化

用完全不同分布的题目/任务做评测（例如题型变化、难度变化、模板变化），看 verifier 是否只是记住了训练分布的“判卷套路”。



把这些评测固定成周期性报告，你才能放心把 verifier 当成 reward 去做 RL，而不是把 pipeline 变成“自我说服机器”。
如果你最终要做 deep research agentic RL，这篇的价值不在“数学”，而在“评测闭环”：

你需要 verifier（引用/事实/覆盖/结构/成本）
你也需要 meta-verifier（判卷质量，防止 judge 被 prompt injection 或投机输出骗过）
然后才能谈得上用 RL 去做稳定迭代

]]></content>
      <categories>
        <category>论文笔记</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Reward Model</tag>
        <tag>Verifier</tag>
        <tag>Self-Verification</tag>
        <tag>DeepSeekMath</tag>
        <tag>Reasoning</tag>
        <tag>Agentic RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：PG Loss 组件详解（PPO-clip / Dual-Clip / Entropy / KL / 聚合）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9APG%20Loss%20%E7%BB%84%E4%BB%B6%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[这篇文章把「Agentic RL / LLM-RL 训练里常见的 Policy Gradient (PG) loss 到底由哪些组件拼起来」讲清楚，重点解释：

PPO-clip：为什么要 clip、clip 住了哪些情况、什么时候梯度为 0
Dual-clip：它在 PPO-clip 基础上到底“多 clip 了什么”，解决什么不稳定
Entropy / KL：为什么要加正则、权重怎么理解
聚合（aggregate）：token/sequence/group 维度的 sum/mean 会如何改变梯度尺度
推导补全（视频 02）：从 REINFORCE / PG 到 TRPO，再到 PPO-clip，解释 log pi、ratio、以及“为什么 PPO 公式里看起来没有 log”

本文偏“工程视角”：你看完应该能把这些项在代码里正确实现出来，并能解释训练曲线为什么会那样。
补充说明：下面不少“实现细节/监控指标/聚合模式”的表述，我会刻意对齐 verl 生态里常见写法。
我不会把任何视频/博客里的“经验区间”当成硬规则。对 LLM-RL 来说，更可靠的做法是：把每个超参都映射回它影响的对象（采样分布 / 数据复用 / 更新幅度 / 探索强度），然后用日志把因果关系闭环验证。
你后面读到 clip fraction / KL / entropy / loss agg 这些东西时，如果你能回答下面 3 个问题，基本就不会被“配方”带偏：

你在约束哪一个分布的偏离：π_new vs π_old 还是 π vs π_ref？
你看到的统计量是 token-level 还是 seq-level 聚合？（这会直接改变数值尺度）
你这次更新到底复用了多少 rollout 数据（ppo_epochs / 多轮优化）？复用越强，越容易变成轻度 off-policy。

关联阅读（更偏“现象与边界”而不是公式）：


RLVR 的边界：Base vs RL、pass@k、PPL 与 vLLM 评测细节


系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


如果你想先把 PG -&gt; TRPO -&gt; PPO-clip 的推导主线补齐，再回来看这些“工程组件”，建议配套阅读：

Agentic RL：从 PG 到 TRPO 到 PPO-Clip（推导与代码对齐）


配套资料（视频 + GitHub）
这篇文章的写作素材来自视频 + 你本地已经下载的配套仓库。我不会照搬视频口述，而是把“loss 组件”按工程实现需要的维度拆开，同时给你能直接对照的 notebook/代码入口。

视频 01（PG loss 组件）：BV1KFpczQEkA

https://www.bilibili.com/video/BV1KFpczQEkA/


配套仓库（你本地已下载）：wdkns/modern_genai_bilibili

GitHub：https://github.com/wdkns/modern_genai_bilibili/tree/main/agentic_rl



建议你优先对照这些笔记（按“从抽象到实现”的顺序）：

PG/PPO 代理目标的“组成零件”与符号约定

agentic_rl/deep_RL/objective_代理loss.ipynb


从 PG 推导到 PPO-clip 的主线（包含 ratio/clip/稳定性直觉）

agentic_rl/deep_RL/pg/from_pg_to_ppo-clip.ipynb


PPO 从零实现（更偏经典 RL，不含 LLM 特殊项，但能补齐最基本的实现细节）

agentic_rl/deep_RL/ppo/ppo_from_scratch.ipynb


loss 聚合（token/seq/group）为什么会改变数值尺度（对齐你后面看 verl 的日志）

agentic_rl/verl/objectives/agg_loss.ipynb


KL/entropy 的数值内涵与训练诊断（别背“经验区间”，用指标闭环）

agentic_rl/training_practices/review_kl.ipynb
agentic_rl/training_practices/entropy.ipynb



如果你在 RL4LLM/Agentic RL 里遇到 “fKL / rKL” 方向搞混、或 KL 数值解释不通，也可以补看：

agentic_rl/sft_rl_fkl_rkl.ipynb


0. 背景：我们在优化什么
强化学习（policy gradient）最朴素的目标是最大化期望回报：
$$\max_\theta \ \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]$$
但直接对回报做梯度很难，于是我们引入可优化的代理目标（surrogate objective），用“概率比 + 优势函数”把“回报最大化”转成可微的 loss。
当你训练的是 LLM（token 级动作）或 Agent（工具调用/检索/规划动作），形式几乎一样，只是：

动作可能是 token，也可能是工具调用、检索 query、规划步骤
优势可能来自 reward model、对比评分器、规则评测、人工偏好、任务成功率等


1. 记号与核心中间量（非常重要）
一条轨迹里第 $t$ 步：

状态：$s_t$（例如 prompt + history）
动作：$a_t$（token / 工具调用 / 选择哪个候选等）
新策略：$\pi_\theta(a_t|s_t)$
旧策略：$\pi_{\text{old}}(a_t|s_t)$
优势：$A_t$（“这步动作比平均水平好多少”）

重要采样比（importance ratio）：
$$r_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)} = \exp(\log \pi_\theta(a_t|s_t) - \log \pi_{\text{old}}(a_t|s_t))$$
为什么要 ratio？

你的数据通常是用旧策略采样的（rollout 时的策略），但你想更新新策略
ratio 允许你在“旧数据”上估计“新策略”目标（近似、但有效）


2. Vanilla Policy Gradient（从这里出发）
最常见的一种 PG 代理目标（最大化形式）：
$$L_{\text{PG}}(\theta) = \mathbb{E}[\log \pi_\theta(a_t|s_t) \cdot A_t]$$
工程里我们一般写成“最小化 loss”，所以常见写法是取负号：
$$\text{loss}_{\text{PG}} = -\mathbb{E}[\log \pi_\theta(a_t|s_t) \cdot A_t]$$
问题是：如果你直接这么做，更新会非常不稳定。
原因直观上是：$\log \pi_\theta$ 的梯度可能会推动策略一步跨得很大，尤其当 $A_t$ 大、或某些动作在旧策略里概率极小导致 ratio 爆炸时。

3. PPO-clip：用 clip 做“近似 trust region”
PPO 的核心代理目标（最大化形式）：
$$L_{\text{clip}}(\theta)= \mathbb{E}\big[\min\big(r_t A_t,\ \text{clip}(r_t, 1-\epsilon, 1+\epsilon)A_t\big)\big]$$
其中 $\epsilon$ 通常是 0.1 或 0.2。
在工程实现里，你几乎一定会看到最小化 loss 的等价写法（HF PPO / verl 常用）：
$$L^{\text{PPO}} = \mathbb{E}_t[\max(-r_t\hat A_t,\ -\text{clip}(r_t,1-\epsilon,1+\epsilon)\hat A_t)]$$
它就是上面 maximize 版本取负号后，把 min 翻转成 max 得到的。
3.1 直觉：限制“新策略相对旧策略”的变化幅度

$r_t$ 表示“新策略把这个动作概率放大/缩小了多少”
clip 把 $r_t$ 限制在 $[1-\epsilon, 1+\epsilon]$

所以 PPO-clip 做的事情是：你可以变，但别变太猛。
3.2 什么时候 clip 会“生效”（梯度变 0 或被截断）
把情况按 $A_t$ 的符号拆开看最清楚：

当 $A_t &gt; 0$（这个动作是“好动作”，我们希望提升概率）

如果 $r_t &gt; 1+\epsilon$：提升得太猛了，会被 clip，等价于“这部分梯度被截掉”


当 $A_t &lt; 0$（这个动作是“坏动作”，我们希望降低概率）

如果 $r_t &lt; 1-\epsilon$：降低得太猛了，会被 clip



你会发现：PPO 的 clip 本质是在限制“朝着对优势有利的方向”更新过头。
3.3 训练时看什么：reward 曲线优先，其次看这些“稳定性指标”
PPO-clip 的 loss 是 surrogate objective，不一定和“任务真的变好”单调对应；实践里更应该关注 rewards curve（你定义的任务 reward / RM reward / success rate 等），loss curve 只当作辅助信号。
除了 reward 曲线，建议至少监控这些量（很多训练框架会直接给出）：

actor entropy：熵太快掉到很低，常见是探索不足/模式塌缩的前兆。
KL（对 reference / SFT / old policy 的偏离）：漂移过大常见会让输出质量崩。
clip fraction：被 clip 的样本比例（token-level 或 seq-level，看实现）。
advantage 的统计量：均值/方差/分位数，尤其注意优势刻度突然变大或分布漂移。

clip fraction 怎么解读？

直觉：被裁剪的样本通常“不给有效梯度”，等价于 有效 batch 变小。clip fraction 越高，更新越被少数未裁剪样本主导，学习容易停滞或变得不稳定。
不要背“黄金区间”。更靠谱的做法是把它和 approx_kl(old,new)、学习率、ppo_epochs 联动看（我在 veRL/verl 那篇里给了更系统的诊断表）：Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）

clip fraction 和 KL 的耦合（非常实用的排查思路）

clip fraction 高 + KL 也高：常见是步子太猛（学习率大、PPO epoch 多、优势刻度过大、reward 过陡等）。
clip fraction 高但 KL 不高：常见是对同一批数据复用太多（epoch 太多 / 复用过度），把 $r_t$ 推到边界但整体分布偏离并不大。
clip fraction 长期接近 0：常见是步子太小（学习率小、优势过小、KL/entropy 系数过大把更新压住）。

3.4 一个经常被忽略的现实：PPO 在 LLM-RL 里很容易“轻度 off-policy”
很多解释会说 PPO 是 on-policy，因此更稳定。但在真实训练里你几乎一定会做：

rollout 很贵，所以同一批数据要做多个 epoch 更新（ppo_epochs &gt; 1）
一个 rollout batch 很大，所以会切成很多 mini/micro 更新

结果是：当你跑到第 2/3 个 epoch 时，当前策略已经不是产生数据的策略，你会看到这些典型信号：

ratio 分布变尖（大量样本贴到 clip 边界）
clip fraction 上升
approx_kl(old,new) 上升
reward 不一定更好，甚至出现 reward hacking（因为你在“反复榨同一批数据”）

所以我更建议你把 ppo_epochs 当成“数据复用强度旋钮”，而不是“训练更充分”的旋钮。通常的排查顺序是：先降 epoch 再谈别的（否则你会把很多问题误判成 reward/模型问题）。
3.5 policy_loss 曲线怎么看（注意符号约定）
不同代码对 policy_loss 的符号约定不完全一致：有的记录“要最小化的 loss”（常见为负），有的记录“maximize 的 surrogate”（常见为正）。
一个可操作的解读方式是：

如果 policy_loss（最小化形式）长期非常负：通常意味着优势项在推动策略快速提升，但也可能是 critic 跟不上 actor（critic 低估，adv 偏大），容易引入不稳定。
如果 policy_loss（最小化形式）长期为较大的正：通常是“坏动作占比大/优势为负占主导”，策略在变差或学不到东西的信号。
更理想的趋势：在 0 附近上下震荡，说明 actor/critic 在互相追赶并逐步稳定。


4. Dual-clip：为什么 PPO-clip 还不够
在实践里，很多人会再加一个 dual-clip，主要解决这样一种现象：

当 $A_t &lt; 0$（坏动作），但 $r_t$ 可能非常大（新策略反而大幅提高了坏动作的概率）时，PPO-clip 在某些写法下对这块的“保护”不够，loss/梯度可能会被放大，导致训练震荡。

你可以把 dual-clip 理解成：对某些极端 case 再加一道上限/下限，避免目标函数在异常 ratio 下过度放大。
4.1 Verl/HF 常见写法（最小化 loss 形式，对齐更容易）
先把 PPO 写成最小化 loss（忽略期望符号，只写单个样本/单个 token 的形式）：
$$L^{\text{PPO}}=\max(-r_t\hat A_t,\ -\text{clip}(r_t,1-\epsilon,1+\epsilon)\hat A_t)$$
Dual-clip（常见引用：arXiv:1912.09729）在很多实现里会对负优势样本再加一道“上界保护”：
$$L^{\text{dual-clip}}=\min(L^{\text{PPO}},\ -c\cdot \hat A_t), \quad c&gt;1$$
直觉解释：

当 $\hat A_t &lt; 0$ 时，$-c\hat A_t$ 是正数，它会把某些极端情况下的巨大正 loss 截住（避免 ratio 异常大导致 loss/梯度过度放大）。
当 $\hat A_t \ge 0$ 时，这个额外项通常不启用或不会改变结果（不同代码会用条件分支实现）。

4.2 参数 $c$ 怎么选
经验上很多实现会用 2 到 5 的量级（比如 3）。它不是越大越好：

$c$ 太小：保护太强，学习可能变慢
$c$ 太大：保护太弱，起不到稳定作用


5. Entropy：为什么要鼓励“更随机”
entropy bonus 常见形式（加入到最小化 loss）：
$$\text{loss} \leftarrow \text{loss} - \beta \cdot H(\pi_\theta)$$
5.1 从 logits 计算 token-level entropy（verl 常见写法）
很多实现会直接从 logits 计算每个 token 的熵（注意输出仍然是 $[B,T]$，后面要走 agg_loss 聚合）：
$$H = \log\sum \exp(\text{logits}) - \sum p\cdot \text{logits}, \quad p=\text{softmax}(\text{logits})$$
import torchdef entropy_from_logits(logits: torch.Tensor):    pd = torch.nn.functional.softmax(logits, dim=-1)    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)    return entropy  # [B, T]
直觉：

在早期训练，鼓励探索（别过早塌缩到某一种输出模式）
在 LLM 训练里，entropy 太低会导致输出变得呆板，甚至出现 mode collapse

怎么调参：

$\beta$ 太大：模型“太随机”，reward 上不去
$\beta$ 太小：容易塌缩或过拟合到奖励漏洞


6. KL：为什么用 KL 约束，而不是 JS
很多 LLM-RL（尤其 RLHF / RLAIF / 各种 agentic RL 变体）都会加 KL 正则，典型是约束当前策略别偏离参考策略 $\pi_{\text{ref}}$（常见是 SFT 模型或某个 frozen baseline）：
$$\text{loss} \leftarrow \text{loss} + \beta \cdot \text{KL}(\pi_\theta ;||; \pi_{\text{ref}})$$
先说我的观点：在大多数 LLM-RL 工程里，真正决定训练行为的不是 “KL vs JS”，而是：

你在约束哪一个 KL（方向）：$\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})$ 还是 $\mathrm{KL}(\pi_{\text{ref}}|\pi_\theta)$？
你怎么算 KL（full distribution 还是 sampled 近似）？
你怎么聚合（token/seq/group）？

这三件事只要有一件变了，KL 的数值尺度与梯度性质就会明显变化，配方参数基本不可迁移。
6.1 KL 在工程上“很好算”
在 token-level 的实现里，你往往只需要：
$$\text{KL} \approx \mathbb{E}[\log \pi_\theta(a|s) - \log \pi_{\text{ref}}(a|s)]$$
（实际会有更精细的估计方式，但核心是：你只要能拿到 logprob 就行）
而 JS 散度需要混合分布：
$$\text{JS}(P||Q) = \tfrac{1}{2}\text{KL}(P||M) + \tfrac{1}{2}\text{KL}(Q||M),\ \ M=\tfrac{1}{2}(P+Q)$$
这意味着你要显式构造/评估 $M$，工程上更麻烦、也更贵。
6.2 KL 更符合“单向约束”的需求
在 RLHF 里，你常见的真实需求是：

“在提高 reward 的同时，别偏离 reference 太远。”

这本质是一个单向的、带约束的优化问题，KL 非常自然。
JS 虽然对称、有界，但它的对称性并不一定是你想要的，而且在高维动作空间（比如大词表）里，JS 的数值/梯度性质也未必更稳定。
6.3 更重要的一点：PPO/TRPO 的理论也经常围绕 KL
TRPO 的 trust region 约束是直接写 KL 的；PPO 也是在用 clip 近似这个约束。
所以当你把 PPO-clip + KL penalty 放在一起看，会发现它们在做同一类事情：

PPO-clip：约束“这一步更新别太大”
KL penalty：约束“整体别偏离 reference 太远”

6.4 KL 放在 reward 里 vs 放在 loss 里（RLHF/LLM-RL 很常见）
在 LLM-RL 里，KL 有两种非常常见的“落点”：

加在 loss 里：policy_loss += kl_loss * kl_coef（verl/HF 里常见）。
写进 token-level reward（reward shaping）：把 KL 看成一个“密集惩罚”，例如常见形式

$$r_t = r_{\text{RM}} - \beta \log\frac{\pi_\theta(a_t|s_t)}{\pi_{\text{SFT/ref}}(a_t|s_t)}$$
其中 $r_{\text{RM}}$ 往往只在序列最后一个 token 非零（或以序列级 reward 形式给出），而 KL 惩罚是 token 级的“每步都扣分”。两者目标一致：限制偏离，但优化动态、日志解读、以及和 advantage/GAE 的耦合会有差异。
6.5 我更建议你关注 KL 的“行为偏好”：它天然会削尖分布
只要你用的是 $\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})$（很多 RLHF/RLVR 常见写法），它在优化上更像一种 mode-seeking 的约束：跑到 ref 低概率区域会被强烈惩罚，因此更容易出现：

多样性下降（entropy 下降）
输出更模板化（distribution sharpening）
某些“低概率但正确”的路径被压没（你在 RLVR 那篇里已经见过这种现象）

这不是说 KL 不好，而是提醒你：当你看到“模型更稳但更保守”时，首先怀疑的不是“模型退化”，而是 KL/clip/聚合把探索掐掉了。这时更有效的动作往往是：

把 KL 当作控制量：设目标区间，按日志闭环调（而不是背一个常数系数）
先减数据复用强度（ppo_epochs），再谈增大 KL（否则你会把 off-policy 问题误归因到 KL）
明确你要的是“单次命中率提升”还是“能力边界拓展”（这两者的系统策略完全不同）


7. 聚合（Aggregate）：token / sequence / group 的差别
LLM 的 logprob、advantage 很多时候都是 token-level 的张量（形状 $[B, T]$）。
你最终需要把它聚合成一个标量 loss 才能 backward()。常见聚合方式：

token mean：对所有有效 token 取平均
sequence mean：先对每个序列求平均，再对 batch 平均
token sum：对 token 求和（注意会放大梯度尺度）
group mean（GRPO 常见）：先在 group 内聚合，再在 group 间聚合

核心结论：聚合方式会改变梯度的等效尺度，从而影响你对学习率、clip、KL 系数等超参的感受。
如果你把 mean 换成 sum，很多时候你会发现“同样的超参突然不稳定了”，原因往往就是梯度整体变大了。
7.1 常见 loss_agg_mode 的数学定义（带 mask）
设 loss 矩阵 $L\in\mathbb{R}^{B\times T}$，mask $M\in{0,1}^{B\times T}$（为 1 的位置计入损失）：

token-mean

$$\mathcal{L}=\frac{\sum_{i=1}^{B}\sum_{j=1}^{T}L_{i,j}M_{i,j}}{\sum_{i=1}^{B}\sum_{j=1}^{T}M_{i,j}}$$


seq-mean-token-sum

$$\mathcal{L}=\frac{1}{B}\sum_{i=1}^{B}\Big(\sum_{j=1}^{T}L_{i,j}M_{i,j}\Big)$$


seq-mean-token-mean（很多论文把它当成“sample-level loss”）

$$\mathcal{L}=\frac{1}{B}\sum_{i=1}^{B}\Big(\frac{\sum_{j=1}^{T}L_{i,j}M_{i,j}}{\sum_{j=1}^{T}M_{i,j}}\Big)$$


seq-mean-token-sum-norm（一种“按长度做归一化”的 token-sum）

$$\mathcal{L}=\frac{\sum_{i=1}^{B}\sum_{j=1}^{T}L_{i,j}M_{i,j}}{T}$$



7.2 和 GRPO / DAPO / DrGRPO 的对应（为什么会影响长 CoT 稳定性）
以 verl 文档/实现里的口径总结（不同实现会有细微差异，但核心直觉一致）：

GRPO 原始论文常用 seq-mean-token-mean（sample-level），在长 CoT 场景可能更不稳定。
DrGRPO 倾向使用 seq-mean-token-sum-norm（降低长度偏置/数值不稳）。
DAPO 倾向使用 token-mean（直接在 token 维度平均，梯度尺度更直观）。

如果你的任务输出长度差异很大（例如 deep research 报告长短不一），聚合方式几乎一定会影响训练是否稳定，甚至会直接改变“模型更偏好长输出还是短输出”（长度偏置）。
7.3 GSPO/GRPO：序列级 ratio、长度偏置与 stop-gradient trick
很多 LLM-RL 的不稳定来自两个因素叠加：

token-wise ratio 在长序列上更容易数值爆炸/塌缩
长序列天然贡献更多 token，导致“长输出占更多梯度份额”（长度偏置）

以 GSPO 的一种常见写法为例，会先定义序列级的归一化 ratio：
$$s_i(\theta)=\Big(\frac{\pi_\theta(y_i|x)}{\pi_{\text{old}}(y_i|x)}\Big)^{1/|y_i|} =\exp\Big[\frac{1}{|y_i|}\sum_t \log\frac{\pi_\theta(y_{i,t}|x,y_{i,&lt;t})}{\pi_{\text{old}}(y_{i,t}|x,y_{i,&lt;t})}\Big]$$
再把它“分摊回 token 级”，并用 stop_gradient（记作 sg[·]）避免破坏梯度结构：
$$\log s_{i,t}(\theta)=\text{sg}[\log s_i(\theta)] + \log\pi_\theta(y_{i,t}|\cdot) - \text{sg}[\log\pi_\theta(y_{i,t}|\cdot)]$$
直觉：让每个样本（序列）在 ratio 上先被长度归一化，再决定 token 级梯度怎么分配，从而缓解长度偏置和 token-wise 数值问题。

8. 从 PG 到 TRPO 到 PPO-clip（视频 02 推导补全）
这一节对应视频 [Agentic RL] 02 策略梯度基础，从 PG 到 TRPO 到 PPO-Clip 核心公式简单推导 的主线（配套 notebook 在 modern_genai_bilibili/agentic_rl/deep_RL/pg/from_pg_to_ppo-clip.ipynb）。
它主要回答 3 个“看公式经常卡住”的问题：

PG loss 里为什么有 log π？
r_t = π/π_old 这个 ratio 是怎么来的？
为什么 PPO-clip 公式里看起来“没有 log”？

8.1 REINFORCE：为什么会出现 log π
最朴素的 RL 目标（最大化期望回报）是：
$$J(\theta)=\mathbb{E}_{\tau\sim\pi_\theta}\Big[\sum_{t=0}^{T}\gamma^t r_t\Big]$$
你想对 $J(\theta)$ 求梯度，但麻烦在于：期望是对“由 $\pi_\theta$ 采样出来的轨迹分布”取的，属于对分布求导。
核心技巧就是 log-trick（score function estimator）：
$$\nabla_\theta \mathbb{E}_{x\sim p_\theta}[f(x)] =\mathbb{E}_{x\sim p_\theta}\big[f(x)\nabla_\theta\log p_\theta(x)\big]$$
应用到轨迹 $\tau$ 上，并利用环境转移与奖励不依赖 $\theta$（依赖的是策略），得到经典的 REINFORCE / Policy Gradient 形式：
$$\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\Big[\sum_t G_t\nabla_\theta\log\pi_\theta(a_t|s_t)\Big]$$
其中 $G_t$ 是 return-to-go（从 $t$ 往后累计的折扣回报）。
工程里我们通常把“最大化 $J$”改写成“最小化 loss”，构造一个等价梯度方向的 surrogate loss：
$$\mathcal{L}_{PG}(\theta)=-\mathbb{E}\big[G_t\log\pi_\theta(a_t|s_t)\big]$$
这就是你在代码里看到的 PG loss / policy loss 的最原始版本（再加上后续的 clip、entropy、KL 等项，就变成 PPO 系列）。
8.2 Advantage：为什么可以从 $G_t$ 变成 $A_t$
PG 的方差很大，所以会引入 baseline $b(s_t)$ 来降方差而不引入偏差。关键性质是：
$$\mathbb{E}[\nabla_\theta\log\pi_\theta(a_t|s_t)]=0$$
因此：
$$\mathbb{E}\big[(G_t-b(s_t))\nabla_\theta\log\pi_\theta(a_t|s_t)\big] =\mathbb{E}\big[G_t\nabla_\theta\log\pi_\theta(a_t|s_t)\big]$$
通常把 $G_t-b(s_t)$ 叫做优势（advantage）：
$$A_t \doteq G_t-b(s_t)\approx Q(s_t,a_t)-V(s_t)$$
于是常用的 PG loss 形式就是：
$$\mathcal{L}_{PG-adv}(\theta)=-\mathbb{E}\big[A_t\log\pi_\theta(a_t|s_t)\big]$$
8.3 Importance Sampling：ratio $r_t$ 怎么来的，为什么 PPO 公式里“没 log”
现实训练里，你经常会：

先用旧策略 $\pi_{old}$ rollout 一批轨迹（采样数据很贵）
再在同一批数据上做多次更新（多 epoch / 多 step）

此时你的期望在数学上应该写成“在旧分布上做重要性采样”：
$$r_t(\theta)\doteq \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)}$$
对应得到 CPI/TRPO 常写的 surrogate objective：
$$L_{CPI}(\theta)=\mathbb{E}_{\pi_{old}}[r_t(\theta)A_t]$$
一个很关键的观察（视频里强调的点）是：在 $\theta=\theta_{old}$ 附近，
$$\nabla_\theta L_{CPI}(\theta)\Big|_{\theta=\theta_{old}} =\mathbb{E}\big[\nabla_\theta\log\pi_\theta(a_t|s_t)\cdot A_t\big]$$
也就是说，ratio 版本的目标在旧策略附近的一阶梯度方向与 vanilla PG 是一致的。
这也解释了“为什么 PPO 公式里看起来没有 log”：PPO 的目标写成了 $r_tA_t$ 的形式，但在代码里你仍然会先算 logp_new/logp_old，然后做：
$$r_t=\exp(\log\pi_\theta-\log\pi_{old})$$
log 并没有消失，只是被你通过 exp(logp_new - logp_old) 隐藏到了 ratio 里。
8.4 TRPO：把“更新别太猛”写成 KL 约束（trust region）
仅用 $L_{CPI}$ 还是可能出现“策略一步跨太大”导致性能崩溃的问题。TRPO 的做法是加一个 trust region 约束：
$$\max_\theta\ \mathbb{E}_{\pi_{old}}\big[r_t(\theta)A_t\big] \quad\text{s.t.}\quad \mathbb{E}_{\pi_{old}}\big[KL(\pi_\theta|\pi_{old})\big]\le \delta$$
直觉：在“以旧策略为中心”的 KL 球里，找一个能让 surrogate 最大的更新。
缺点：这个约束优化在实现上通常要用二阶近似/共轭梯度等技巧，不太适合直接塞进深度学习常见的 mini-batch SGD 训练循环。
8.5 PPO-clip：用 clip 做一个“好用的 TRPO 近似”
PPO-clip 用一个非常工程友好的方式近似 TRPO 的 KL trust region：
$$L_{CLIP}(\theta)=\mathbb{E}\Big[\min\big(r_t(\theta)A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\cdot A_t\big)\Big]$$
最值得记住的 case-by-case 结论（你在实现/排查时会天天用）：

$A_t&gt;0$（好动作）

$r_t&gt;1+\epsilon$：涨太猛，clip 生效，梯度被截平


$A_t&lt;0$（坏动作）

$r_t&lt;1-\epsilon$：降太猛，clip 生效，梯度被截平



换句话说：PPO 的 clip 主要限制“朝着对优势有利的方向”更新过头，从而把“稳定性”换成“训练效率/实现简单”。
这也回扣到本文主标题：在实际工程里，所谓 “PG loss components” 里最核心的一块，就是这个 PPO surrogate（通常写成 clip 版本），再叠加 entropy/KL/dual-clip/聚合等工程项。

9. PyTorch 伪代码：把这些项拼起来（可直接对照你用的框架）
下面给一个“最常见”的结构示例，假设你已经有：

logp_new: 当前策略对采样动作的 logprob，形状 [B, T]
logp_old: rollout 时旧策略 logprob（需要 detach），形状 [B, T]
advantages: 优势（需要 detach），形状 [B, T]
mask: 有效 token mask（padding 位置为 0），形状 [B, T]
entropy: 可选，形状 [B, T]
logp_ref: 参考策略 logprob（可选），形状 [B, T]

import torchdef masked_mean(x, mask, eps=1e-8):    return (x * mask).sum() / (mask.sum() + eps)def ppo_pg_loss(    logp_new, logp_old, advantages, mask,    clip_eps=0.2,    dual_clip=None,        # e.g. 3.0    ent_coef=0.0,    entropy=None,    kl_coef=0.0,    logp_ref=None,):    # ratio = exp(log pi_new - log pi_old)    ratio = torch.exp(logp_new - logp_old)    # PPO clipped surrogate (minimize loss)    pg1 = -advantages * ratio    pg2 = -advantages * torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps)    pg = torch.max(pg1, pg2)  # because we minimize negative of the &quot;min&quot; objective    # Dual-clip: cap extreme loss on negative-adv samples    # Note: different codebases implement slightly different variants.    if dual_clip is not None:        # Common (verl-like) form: for advantages &lt; 0, cap the loss by min(L_ppo, -c * A)        cap = (-dual_clip) * advantages        pg = torch.where(advantages &lt; 0, torch.min(pg, cap), pg)    loss = masked_mean(pg, mask)    # Entropy bonus (encourage exploration)    if ent_coef != 0.0:        assert entropy is not None        loss = loss - ent_coef * masked_mean(entropy, mask)    # KL penalty (keep close to reference)    if kl_coef != 0.0:        assert logp_ref is not None        kl = (logp_new - logp_ref)  # common estimator on sampled actions        loss = loss + kl_coef * masked_mean(kl, mask)    return loss
9.1 读代码时最容易踩坑的点

最大化 vs 最小化：论文写 maximize，代码写 minimize；min/max 会翻转。
advantages / logp_old 要 detach：它们是“常数”，不应对它们反传。
mask 一定要做：padding token 不能算进 loss。
聚合方式会改变梯度尺度：mean/sum 会影响你对 lr/clip/kl 的直觉。


9.2 如果你用的是 verl：如何“真的改到代码里”
modern_genai_bilibili 的笔记里给了一个很实用的路线图（这里总结成可执行的 checklist）：

全局搜 .backward()，先找到训练主循环真正反传的 loss（通常是 loss.backward()）。
在 core_algos.py（或同名文件）里找 compute_policy_loss，它通常是 “PG loss 的注册中心”。
通过类似 @register_policy_loss(&quot;xxx&quot;) 的注册机制添加新的 pg_loss 变体（例如把 entropy/KL/特殊 ratio 写法组合进去）。
如果你要改的是 training logic（比如动态采样、rollout 管理、异步 agent loop），通常需要自定义 Trainer（笔记里提到的 RayDAPOTrainer 就属于这类）。


10. 这和 Agentic RL / Deep Research 有什么关系
如果你做的是“deep research agent”（检索 + 阅读 + 归纳 + 写作），你不一定要微调 LLM 本体。
更现实的路线是：

LLM 用 API（当成 black-box policy 或 planner）
你训练的是 agent 的决策层：何时检索、检索什么、读哪些文档、如何分解任务、如何分配预算
reward 来自任务指标：答案质量、引用覆盖、事实一致性、时间/成本、用户满意度等

这时 PPO/GRPO 这些 loss 仍然是核心工具，只是动作空间从“token”扩展到了“工具与策略”。
如果你具体做的是“deep research 报告型任务”，建议尽早把评测闭环定下来：

RACE：评估报告质量（覆盖度/洞察/指令遵循/可读性），并用相对评分避免“打分膨胀”。
FACT：逐条核查“每一个引用是否支撑对应陈述”，把 citation trustworthiness 从纯生成质量里拆出来单独评。


11. 小结

PPO-clip 的本质：用 ratio + clip 做稳定的 surrogate objective
dual-clip 的本质：对极端 case 再加一道保护，减少震荡
entropy/KL 的本质：一个防塌缩，一个防漂移
aggregate 的本质：改变梯度尺度，影响所有超参的手感

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>PPO</tag>
        <tag>LLM</tag>
        <tag>RLHF</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AREINFORCE%204%20LLM%EF%BC%88Reward%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%20PG-KL%20Loss%20%E7%BB%86%E8%8A%82%EF%BC%89/</url>
    <content><![CDATA[这一篇对应视频 04：“REINFORCE 4 LLM，设计 reward，如何维护和计算 PG/KL loss，KL loss 计算细节”（BV1Ya1LB1EDM）。
它的价值不在于“给你一个能把大模型训得多强的框架”，而在于把 RL4LLM 里最容易被 verl/openrlhf/... 这些工程封装遮住的核心逻辑，拆成一个可手写、可调试、可逐行对齐公式的最小闭环：

LLM 作为 policy：state/action/trajectory 怎么映射
REINFORCE/PG loss 在 LLM 上怎么落地：联合 logprob 怎么维护
KL penalty 为什么是“语言能力 vs reward-max”的拨杆
KL 计算最容易踩坑在哪里：sum vs mean / fKL vs rKL / mask 与聚合

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


建议阅读顺序（我按“先理解再工程”的顺序排）：


02&amp;03：PG→TRPO→PPO + GRPO&#x2F;On-policy 补充（推导与代码对齐）


01：PG loss 组件详解（PPO-clip &#x2F; Dual-clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）

你正在读的这一篇（04：REINFORCE + reward + KL 的最小闭环）

09：Reward Model Insights（Bradley-Terry &#x2F; MLE &#x2F; 深度学习）



0. 资料对齐（视频 + GitHub）

视频 04：BV1Ya1LB1EDM

https://www.bilibili.com/video/BV1Ya1LB1EDM/


配套仓库（你本地已下载）：wdkns/modern_genai_bilibili

GitHub：https://github.com/wdkns/modern_genai_bilibili/tree/main/agentic_rl


对应 Notebook（讲思路 + 展示现象）：

agentic_rl/deep_RL/align/align_demo.ipynb


对应训练脚本（本文主线）：

agentic_rl/deep_RL/scripts/reinforce_align.py


KL 方向（fKL/rKL）补充材料：

agentic_rl/sft_rl_fkl_rkl.ipynb



本文会以 reinforce_align.py 为主线，把视频里强调的关键点“落回代码”，同时把最常见的 KL 实现坑一次讲清楚。
0.1 这个 demo 刻意简化了什么（你迁移到真实任务必须补上什么）
先把边界讲清楚：reinforce_align.py 的定位是 “把 RL4LLM 的骨架拆开给你看”，而不是“可直接拿去训强模型的配方”。它刻意简化/省略了很多真实训练里决定成败的东西。
你把它迁移到真实任务（尤其是 agentic RL / deep research）时，至少要补齐这些点，否则你会遇到“曲线很好看但行为变坏/训练不稳定/指标不可解释”：

方差控制（baseline / advantage / whiten）

REINFORCE 直接用 reward 当权重，方差极大；现实里要么用 critic/GAE（PPO），要么至少做 per-prompt/per-group 标准化（GRPO）。


变长序列的 mask 与归一

必须明确 EOS 后还算不算 KL / logprob；长度归一按“有效 token 数”做，而不是除以固定常数。


训练集 vs 评测集（防 reward hacking）

reward model/verifier 很容易被 hack。你需要 holdout prompts + 定期采样输出做 case analysis。


KL 的“种类”别混用

KL(π_new || π_ref)（anchor 到参考模型） 和 KL(π_new || π_old)（trust region）是两类东西；日志里 approx_kl 往往又是 sampled proxy。


奖励的尺度与分布

reward 的均值/方差、clip、归一、温度/采样策略都会直接改变梯度尺度；你需要把 reward 的统计量记录进日志（mean/std/quantile）。


系统层面的吞吐与成本

对 LLM 来说 rollout 才是大头。你需要推理引擎（vLLM）和 batching 逻辑，否则训练速度会慢到不可迭代。



把这些补齐之后，你才算真的进入“可复现、可解释、可对比”的 RL4LLM 训练范畴。

1. 把 LLM 看成 policy：state/action/trajectory 是什么
把自回归生成写成 RL 形式，你可以用下面这个映射来理解：

状态 $s_t$：prompt + 目前已经生成的 token 前缀（history）
动作 $a_t$：下一个 token（离散动作空间大小约等于 vocab size）
策略 $\pi_\theta(a_t|s_t)$：LM 在当前上下文下的 next-token distribution
轨迹 $\tau$：整段生成序列（从 prompt 开始，到 EOS/长度上限结束）

很多 RL4LLM demo（包括本期）更像 contextual bandit：reward 常常是整句/整段生成结束后一次性给出（outcome reward）。

2. REINFORCE/PG 的核心：你不是在最小化“监督学习 loss”，而是在构造正确梯度
先把目标写清楚。对一条生成序列 $y$（包含 token 序列）定义 reward $R(y)$，我们想最大化：
$$J(\theta)=\mathbb{E}_{y\sim\pi_\theta}[R(y)]$$
麻烦点在于：期望是对“由 $\pi_\theta$ 采样出来的分布”取的，属于对分布求导。REINFORCE 用 log-trick 把它改写成对 logprob 求导：
$$\nabla_\theta J(\theta)=\mathbb{E}_{y\sim\pi_\theta}\big[R(y)\cdot\nabla_\theta \log \pi_\theta(y)\big]$$
工程里我们习惯写成“最小化 loss”，所以常见的 PG loss（负号）是：
$$\mathcal{L}_{PG}(\theta)=-\mathbb{E}_{y\sim\pi_\theta}\big[R(y)\cdot \log \pi_\theta(y)\big]$$
关键点：这里的 $\mathcal{L}_{PG}$ 不是什么“预测误差”，它是一个为了得到正确梯度而构造的 surrogate objective。你日志里看到它上下波动很正常，真正像“性能曲线”的通常是 reward/任务指标。

3. LLM 的联合概率：为什么要维护的是整段输出的 logprob（而不是某个 token 的概率）
LLM 的序列概率是 token 条件概率的连乘。对数空间就是累加：
$$\log \pi_\theta(y)=\sum_{t=1}^{T}\log \pi_\theta(a_t|s_t)$$
这就是脚本里下面这行在做的事（逐 token 累加）：
log_probs_accumulated[active_mask] += dist.log_prob(next_tokens).unsqueeze(-1)
注意：视频里反复强调的是“联合概率”。你维护联合 logprob，才能把 rollout 的整段输出直接带回 REINFORCE 公式里。
一个工程细节：脚本里把联合 logprob 再除以 NUM_TOKENS 做长度归一：
normalized_log_probs = log_probs_accumulated / NUM_TOKENS
它的直觉是：否则长句天然更负（logprob 连加得更多），梯度尺度会随长度漂移，训练会很难调。
更严谨的版本通常会按“有效生成长度”归一（EOS 后不再计入），而不是除以固定常数 NUM_TOKENS。这属于 demo 简化，你在做真实任务时建议修正。
3.1 长度控制的三种办法：别把“归一”当成唯一解
很多初学者会把“长度归一”当成万能药，但它本质只是在改梯度尺度。更系统的做法是把“长度”当成你要显式管理的变量，常见有三条路：

只做尺度归一（不改最优点）

用 logprob_sum / effective_len 主要是让不同长度的样本梯度量级可比，方便调参。
但你要意识到：它也在引入长度相关的 reweighting，可能会轻微偏好短序列。


在 reward 里显式惩罚/奖励长度（改最优点）

例如 R_total = R_task - λ * length 或者把 reward 写成 “每 token 的平均质量”。
这更可控：你明确告诉策略“长一点/短一点”的代价，而不是通过一个隐式归一去偷改梯度。


用任务协议把长度关掉

对 research agent 来说，经常可以通过输出格式约束（必须包含哪些段落/必须引用几篇/最大字数）把长度问题转化为“格式是否合规”的 verifier 问题。



工程里最推荐的顺序是：先用协议/格式约束把长度控制住；必要时再做尺度归一；最后才考虑把长度写进 reward（因为它最容易引入副作用）。

4. 这个 demo 的 reward：embedding cosine similarity（好用但危险）
视频用一个非常直观的目标来做“RL4LLM 祛魅”：

不管 prompt 是什么，都让模型倾向输出“和猫相关”的语义。

reward 的定义很简单：用一个冻结的 sentence-transformer 把生成序列编码成 embedding，再和 cat 的 embedding 做 cosine similarity：
sequence_embeddings = embedding_model.encode(sequences, convert_to_tensor=True)reference_embedding = embedding_model.encode(&quot;cat&quot;, convert_to_tensor=True)rewards = util.pytorch_cos_sim(reference_embedding.unsqueeze(0), sequence_embeddings).squeeze()
优点：

密集、稳定、实现快
很适合用来观察“policy 在 reward 驱动下会怎么变形”

风险（几乎必然出现 reward hacking，视频里也观察到了）：

反复刷 “cat/cat/cat…”
输出结构与可读性变差
分布漂移后语言能力下降（不加 KL 时尤其明显）

4.1 奖励函数的“最小单元测试”：别等 RL 把你训歪了才发现
不管你用 RM、verifier 还是 embedding 相似度，只要它进了训练闭环，你就应该把它当成一个“会被攻击的系统组件”。最小的工程防线是做 reward 的单元测试：

极值样本检查：对同一批 prompt 采样一堆输出，把 reward 排序，人工看 Top/Bottom 各 20 条，问一句：高分真的更好吗？有没有明显投机模式？
不变性测试：同一语义、不同表述（同义改写/打乱格式/加冗余词）reward 是否稳定？如果 reward 对“表面模式”过敏，RL 一定会放大它。
长度敏感性：reward 和长度是否强相关？如果相关性很强，策略会先学会控长度而不是学会做任务。
跨 prompt 泛化：把一部分 prompts 留出来不参与训练，只做周期性评测。reward 提升不代表泛化，可能只是对训练 prompts 的模式记忆。

这几个检查成本很低，但能提前把 80% 的“训练后行为变怪”在训练前暴露出来。
所以你应该把它当成“看清机制的玩具环境”，不要把这种 reward 直接当 deep research 的终极评价函数。真实的 research agent reward 里至少还需要：

结构化程度（是否按要求分段、是否有引用）
事实一致性（citation-grounded / verifier）
覆盖度（是否检索到关键文献、是否覆盖关键观点）
成本与时延（token/工具调用预算）
以及更可靠的自动评测闭环（否则 RL 只是在“钻漏洞”）


5. 从 REINFORCE demo 到 PPO-RLHF：为什么 PPO 会把 KL 变成“每步的稠密代价”
视频里顺便对比了 PPO 用在 LLM-RL/RLHF 时 reward 的一个典型写法（你在很多框架里都会看到）：

终点 reward 来自 reward model：只在最后一步加上
中间每一步都加一个 token-level 的 KL 代价（把“偏离参考策略”当成稠密惩罚）

一个常见的 token-level shaping 形式是：
$$r_t = -\beta\cdot\big(\log\pi_\theta(a_t|s_t)-\log\pi_{\text{ref}}(a_t|s_t)\big)$$
终点再加上 RM 的序列级 reward（简写成 $r_T \mathrel{+}=R_{\text{RM}}$）。
然后你会看到：

用这些 $r_t$ 算 return-to-go：$G_t=\sum_{k\ge t}\gamma^{k-t}r_k$
再用 critic/value baseline 得到 advantage：$A_t=G_t - V(s_t)$
最后 actor 用 PPO-clip / GRPO 的目标更新

这就把“KL 约束”从一个抽象正则，变成了“每一步都能提供训练信号的稠密 reward shaping”。

6. KL penalty：它到底在约束什么
你可以把 KL penalty 理解成一个 prior / anchor：

不加 KL：策略会为了 reward 最大化而狂奔，快速漂移到奇怪分布（语言能力掉得也快）
加 KL：允许为了 reward 改变，但要求“别离参考策略太远”，从而得到一个 trade-off

在“显式 loss”写法下，经常写成：
$$\mathcal{L}(\theta)=\mathcal{L}_{PG}(\theta)+\beta\cdot \mathrm{KL}(\pi_\theta|\pi_{\text{ref}})$$
其中 $\beta$ 就是拨杆：

$\beta$ 小：更激进追 reward，更容易 drift
$\beta$ 大：更像在原模型附近做 steering，稳但 reward 上升慢


7. KL 计算细节：最常见的坑（视频 04 的重点）
视频里点名了两个实现层面的坑：

聚合方式：KL 是对类别分布的求和，不能在 vocab 维度做 mean
方向：你到底在算 forward KL 还是 reverse KL，梯度性质完全不同

7.1 KL 的定义：sum over vocab，不是 mean
对离散分布 $p,q$：
$$\mathrm{KL}(p|q)=\sum_i p_i(\log p_i-\log q_i)$$
注意是 sum over vocab。如果你写成 mean(dim=-1)，你相当于把 KL 除以了 vocab size（例如 50k），数值会小几个数量级，迫使你把 kl_coef 人为调得很大，训练动态会非常别扭。
这也是视频里提到的现象：当前实现的 KL 数值很小，乘以 vocab size 才接近“正常量级”。
7.2 forward vs reverse：你算的是哪个 KL
先给你一个最不容易搞混的写法（建议你在代码里直接手算，而不是依赖 F.kl_div 的 input/target 语义）。
记：

logp = log_softmax(logits_cur)，p = exp(logp)
logq = log_softmax(logits_ref)，q = exp(logq)

forward KL（常见“约束新策略别偏离 ref”）：
kl_fwd = (p * (logp - logq)).sum(dim=-1)  # KL(cur || ref)
reverse KL：
kl_rev = (q * (logq - logp)).sum(dim=-1)  # KL(ref || cur)
两者的梯度性质不同（直觉层面：fKL 更“cover”，rKL 更“mode-seeking”）。你到底该用哪一个，取决于你希望它把策略往哪里推。
视频里还提到一个工程现实：很多框架并不是算 full-distribution KL，而是用 sampled action 的 logprob 差来近似（更便宜），例如：
$$\widehat{\mathrm{KL}} \approx \mathbb{E}\big[\log\pi_\theta(a_t|s_t)-\log\pi_{\text{ref}}(a_t|s_t)\big]$$
注意：这是一个常用近似，但它并不等价于 full-distribution KL；你要知道自己在用哪一种。
7.3 reinforce_align.py 里的具体问题（你在读代码时要意识到）
脚本当前实现（第 100 行附近）是：
kl_div = F.kl_div(    F.log_softmax(logits_active, dim=-1),    F.log_softmax(ref_logits_active, dim=-1),    reduction=&quot;none&quot;,    log_target=True,)kl_div_accumulated[active_mask] += kl_div.mean(dim=-1).unsqueeze(-1)
这里至少有两个你需要“显式做出选择”的点：

mean(dim=-1)：如上所述，这不符合 KL 的定义，通常应该改成 sum(dim=-1)，然后重新调 KL_FACTOR。
F.kl_div(input=logp_cur, target=logp_ref, log_target=True) 实际算出来更接近 KL(ref || cur)（reverse KL）。如果你本意是 KL(cur || ref)，就不要用这段，直接手算最清晰。

7.4 为什么大家更常用 KL，而不是 JS 散度
你之前问过一个很好的问题：训练里做约束/正则，为啥常用 KL，而不是 JS？
工程上最常见的原因通常有四个：

计算方便：KL 能直接用 log_softmax + exp 在一个 forward 里算出来；JS 需要构造混合分布 $m=\frac12(p+q)$，等价要算两次 KL 并且要显式拿到 $m$。
和 trust region/自然梯度的关系更直接：TRPO 的约束项就是 KL；PPO/LLM-RL 生态沿用了“用 KL 控制更新幅度”这条主线。
小步近似下足够用：你把策略变化限制在小范围内时，KL 作为局部度量已经能很好地起到“别跑太远”的效果（并且你只需要一个可调强度的惩罚项）。
梯度与惩罚强度更“硬”：JS 是有界的（上限是 $\log 2$），当两个分布差得很远时，它的惩罚/梯度容易变得不够敏感；而 KL 是无界的，能在策略明显漂移时给出更强的约束信号（当然代价是它的非对称性需要你显式选方向）。

不是说 JS 不能用，而是 KL 在“好算 + 好调 + 和已有理论/工程惯例对齐 + 在漂移时惩罚更硬”这几件事上更划算。
如果你担心 KL 的非对称性带来“mode-seeking/分布塌缩”（例如 rKL 更容易把概率压到少数模式），工程上更常见的解法是：

用 entropy/采样温度保留探索；
监控并约束 clip_fraction/approx_kl，别让更新太激进；
必要时在 reward 侧引入多样性/覆盖度指标，而不是直接换成 JS。


8. 一个更“对定义”的 KL 实现（可直接替换脚本核心片段）
下面这段给的是 token-level forward KL（cur||ref），并且是 sum over vocab：
import torch.nn.functional as Flogp = F.log_softmax(logits_active, dim=-1)      # [B_active, V]logq = F.log_softmax(ref_logits_active, dim=-1)  # [B_active, V]p = logp.exp()kl_fwd = (p * (logp - logq)).sum(dim=-1)         # [B_active]kl_div_accumulated[active_mask] += kl_fwd.unsqueeze(-1)
如果你想要 reverse KL，把 $p$ 换成 $q=\exp(\text{logq})$，并把括号里的差翻过来即可：
q = logq.exp()kl_rev = (q * (logq - logp)).sum(dim=-1)         # [B_active]
做完这件事，你会发现：

KL 数值会上一个量级（不再需要“乘 vocab size 才像样”）
你的 KL_FACTOR 需要重调（通常会变小很多）
KL 曲线会更可解释（不会出现“系数很大但 KL 还几乎为 0”的错觉）


9. 维护 PG/KL loss 的工程 checklist（照着做基本不会炸）
结合视频 04 + 脚本经验，总结成一个很实用的 checklist：

rollout 生成时至少维护：

output_ids（当前生成的 token 序列）
logp_cur(a_t|s_t) 的累加（联合 logprob）
logits_cur 与 logits_ref（如果你做 full-distribution KL）
active_mask（EOS 后别继续采样/别继续累计 KL）


reward 计算必须 torch.no_grad()，并且最好和 policy 网络解耦（否则梯度会串）
KL 必须明确：

方向（fKL / rKL / sampled 近似）
聚合（vocab 维 sum；再按 token/seq 平均）


训练日志优先看：

reward/成功率（主）
KL（主）
输出样例（主，防止 reward hacking）
policy_loss 数值（辅助，不要迷信它单调）


容易被忽略但很关键：

tokenizer/model 要对齐（否则 decode 和 eos 行为会怪）
长度归一最好按有效 token 数（EOS 后别算）




10. 你下一步怎么把它迁移到 Agentic RL / Deep Research
这个 REINFORCE demo 的意义不是让你“真的用它训一个大模型”，而是让你能：

从零写出 RL4LLM 的最小闭环（rollout -&gt; reward -&gt; PG -&gt; KL）
看懂 PPO/GRPO/verl 里 loss 组件到底在对应哪条公式
更重要的是：知道 reward/KL/聚合这些细节会怎样影响训练动态

如果你要做 deep research agentic RL，我建议把精力优先投到：

评测闭环：reward 设计 + 自动评测的可信度（否则 RL 只是“骗指标”）
action space 设计：对 research agent 来说，动作往往是“检索/阅读/引用/工具调用/计划”的离散决策，而不是直接训每个 token
再决定要不要训 token policy（LLM 本体）。很多实际系统完全不微调 LLM，而是把 RL 用在上层决策与工具策略上

后续如果你希望我把这个 demo 按“更接近 PPO-RLHF 的形态”（dense reward、critic、advantage、clip objective、KL 监控）继续扩展成可跑的 notebook，我也可以在这个系列里把它一步步补齐。
]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>LLM</tag>
        <tag>RLHF</tag>
        <tag>REINFORCE</tag>
        <tag>KL</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：RLVR 的边界（Base vs RL、pass@k、PPL 与 vLLM 评测细节）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9ARLVR%20%E7%9A%84%E8%BE%B9%E7%95%8C%EF%BC%88Base%20vs%20RL%E3%80%81pass@k%E3%80%81PPL%20%E4%B8%8E%20vLLM%20%E8%AF%84%E6%B5%8B%E7%BB%86%E8%8A%82%EF%BC%89/</url>
    <content><![CDATA[这一篇对应视频 07：“limits of RLVR，base vs. RL, pass@k, ppl 基于 vLLM 计算细节以及采样效率”（BV1pWSvBtEAk）。
我把它拆成三条主线：

Base vs RL 的“能力”到底在对比什么：RLVR 更像分布削尖（distribution sharpening）还是能力外推（capability uplift）？
为什么一定要看 pass@k 而不只看 pass@1：以及怎么低方差地估算整条 pass@k 曲线。
怎么用 vLLM 可靠地算 PPL / entropy（评测细节）：不踩坑地得到能解释现象的指标。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


配套资料（你本地已有）：

视频 07：BV1pWSvBtEAk

https://www.bilibili.com/video/BV1pWSvBtEAk/


代码/笔记仓库（对应你本地下载的 modern_genai_bilibili-main）：

GitHub：wdkns/modern_genai_bilibili/agentic_rl
本文主要对齐的 notebook：agentic_rl/reasoning/rlvr.ipynb
你本地路径：/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reasoning/rlvr.ipynb


论文/复现参考（视频里提到的方向）：LeapLabTHU 的 limit-of-RLVR

https://github.com/LeapLabTHU/limit-of-RLVR




1. 先把“结论冲突”说清楚：Base 很强，为什么还要 RLVR？
在 math/coding 这种 verifiable reward 任务上（做对就是 1，做错就是 0，或者有一个可程序验证的 score），RLVR 的一个常见现象是：

RL 模型的 pass@1（只采样 1 次就做对的概率）会涨；
但 Base 模型只要你肯多采样（例如 128/256/1024 次），pass@k 可能已经很高了；
更糟的是，RL 训练经常会让输出分布更尖，多样性下降，一些“低概率但正确”的路径会被压没。

这就引出视频 07 的核心问题（也是你做 agentic RL / deep research 训练时必须面对的）：

RLVR 到底是在“让模型更会推理”，还是只是把 Base 已经会的那一小撮正确轨迹的概率拉高，让它更容易一次采样命中？

如果你只看 pass@1，你会高估“能力提升”；如果你把 pass@k、PPL、entropy 一起看，你更容易看清它到底是在做什么。

2. pass@k：为什么它比 pass@1 更接近“能力上限”
2.1 pass@k 的定义（直觉）
pass@k 的语义很简单：

你对同一道题采样 $k$ 次；
只要这 $k$ 个答案里至少有一个是正确的，就算“这题被解决”。

这对 coding/math 的意义尤其大：Base 模型可能偶尔能走到正确解，但概率小。只要你愿意多采样，你会看到它的“潜力上限”。
形式化写法（论文里常见写法）：
$$\text{pass}@k := \mathbb{E}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right]$$
其中：

$n$ 是你为该题采样的总次数（例如 128/256/1024）；
$c$ 是其中正确的样本数；
$\frac{\binom{n-c}{k}}{\binom{n}{k}}$ 是“从 $n$ 个里抽 $k$ 个，抽到的全是错的”的概率；
所以 $1-\cdot$ 就是“至少有一个对”的概率。

2.2 为什么论文爱固定 $n$，再算一条完整的 pass@k 曲线
工程上你往往会固定一个比较大的 $n$（例如 1024），然后对所有 $k&lt;n$ 估算出整条 pass@k 曲线：

这样你能比较“采样次数增加”带来的收益；
也能更清楚地对比 Base vs RL：RL 是否真的让曲线整体上移，还是只让小 $k$ 的点变好。

2.3 数值实现：别直接算组合数（会溢出）
rlvr.ipynb 里给了两个实现，组合数版在大 $n$ 时会数值爆炸，工程里更常用乘积版：
import numpy as npimport mathdef pass_at_k(n, c, k):    if n - c &lt; k:        return 1.0    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))def pass_at_k_comb(n, c, k):    if n - c &lt; k:        return 1.0    return 1 - math.comb(n - c, k) / math.comb(n, k)
这里的一个关键边界条件：

如果错的样本数 $n-c &lt; k$，你抽 $k$ 个一定会抽到至少一个正确样本，所以 pass@k 直接是 1。

2.4 pass@k 的“隐藏前提”：采样相关性会让你误判潜力上限
pass@k 很强，但它也非常容易被“采样细节”影响，导致你误把“采样策略变了”当成“模型能力变了”。你至少要注意两点：

样本相关性（correlation）

同一 prompt 下的多次采样并不是独立同分布的“理想抽样”，尤其当你用的是低温度、强约束（top-p 很小、强 repetition penalty）或带模板的生成。
相关性越强，你的“多采样潜力”会被高估或低估，曲线的形状也会变得更难解释。


pass@k 对解码超参极度敏感

同一个 Base 模型，temperature/top_p/top_k 一改，pass@k 曲线可能整体平移。
所以对比 Base vs RL 时，除了报告 pass@k，你还应该固定并报告解码策略（最好把 Base 和 RL 放在同一组解码设置下对比）。



我建议你把 pass@k 当成“系统行为”的指标，而不是纯粹“模型静态能力”的指标：它反映的是 模型分布 + 解码策略 + 评测器 的合成效果。

3. Sampling Efficiency Gap：把“潜力上限”和“单次命中率”放到同一个数里
视频 07 里另一个很关键的指标是采样效率差距（Sampling Efficiency Gap）：
$$\Delta_{\text{SE}} = \text{pass}@k_{\text{Base}} - \text{pass}@1_{\text{RL}}$$
直觉上它在衡量：

Base 模型“只靠多采样”能到哪（上限）；
RL 模型“只采 1 次”离这个上限还有多远（效率）。

你可以把它理解成一种很现实的 tradeoff：

RL 的意义很多时候不是“发明新能力”，而是把 Base 的潜力变成可用的采样效率（从 256 次变成 1 次）。
但如果 $\Delta_{\text{SE}}$ 仍然很大，说明你当前 RL 算法（PPO/GRPO/变体）仍然没把 Base 的潜力吃干榨净。

这对 agentic RL 非常重要：你做 deep research agent 时，很多能力也可能“Base 偶尔能做到”，但成本太高。你的目标往往是把它变成稳定可用的行为，而不是在 benchmark 上秀 pass@1。

4. 用 PPL 识别“分布削尖”还是“边界拓展”
4.1 Base 对 RL 输出的困惑度：RL 是否逃逸出 Base 分布？
rlvr.ipynb 里写了一个非常有用的诊断：

固定 Base 模型 $P_{\text{Base}}$；
让 RL 模型生成答案 $Y_{\text{RL}}$；
计算 Base 模型对这些 token 的负对数似然，并转成 perplexity。

$$\text{PPL}_{\text{Base}}(Y_{\text{RL}}\mid x)=\exp\left(-\frac{1}{T}\sum_{t=1}^{T}\log P_{\text{Base}}(y_t\mid x,y_{&lt;t})\right)$$
判读要点：

如果 $\text{PPL}{\text{Base}}(Y{\text{RL}}\mid x)$ 很低，说明 RL 输出在 Base 看来“很合理”，更像是在 Base 分布内部做 reweighting（削尖）。
如果它明显升高，说明 RL 输出更像“跑出了 Base 的高概率区域”，有边界拓展的迹象（但也可能是胡言乱语，需要结合 correctness）。

4.2 distillation 为什么可能更“扩边界”
视频与笔记里对比了 distillation：如果你引入 teacher（例如强推理模型）的输出再蒸馏，学生模型的推理边界可能会真的被推开。
这给你的启示是：

如果你期待“深研究 agent 真的学会新东西”，只靠 RLVR 可能不够；
你可能需要 distill / curriculum / tool-augmented data，让“新轨迹”进入可学习分布，然后再用 RL 提升单次命中率与稳定性。

4.3 PPL 的误读风险：风格/模板变化也会让 PPL 变大
用 $PPL_{\text{Base}}(Y_{\text{RL}})$ 做分布诊断很有用，但它也有一个常见误区：PPL 变大不一定代表“更强/更弱”，很多时候只是“写法变了”：

RL 改了输出风格（更啰嗦/更简洁/更多模板化 token），Base 当然会困惑。
prompt 模板、system prompt 或工具输出拼接方式不同，token 边界变化会直接影响 PPL。
Base 可能对“低概率但正确”的解法本来就不自信，正确解也可能对应高 PPL。

所以我更建议把它当成 “分布漂移报警器”：PPL 明显上升时，你应该做两件事：

人工抽样看输出是否变怪（格式、重复、幻觉、答非所问）。
联合 correctness/pass@k/entropy 一起判断：是“跑偏”还是“拓展到罕见但正确的区域”。


5. vLLM 评测细节：如何可靠地算 PPL / token entropy
视频 07 专门讲了一个“很工程但决定你指标是否可信”的点：在 vLLM 里算 logprob 的方式。
如果你发现自己还在 vLLM 的 OOM/吞吐低/ITL 差 里挣扎，建议先把推理引擎的参数体系搞清楚（否则评测脚本可能根本跑不起来）：


Agentic RL：vLLM 参数配置、显存分析与性能调优（max_num_batched_tokens）


5.1 核心思路：把 (prompt + response) 当成 prompt，取 prompt_logprobs
vLLM 的 LLM.generate 支持：

prompt_logprobs=K：返回输入文本每个位置的 Top-K token logprob；
max_tokens=1：只 forward，不真的生成（评测模式）。

因此你可以把完整文本拼起来：
full_texts = [p + r for p, r in zip(prompts, responses)]eval_params = SamplingParams(prompt_logprobs=20, max_tokens=1, temperature=1.0)outputs = llm.generate(full_texts, eval_params)
然后关键是做 masking：

你只想算 response 的 PPL，不想把 prompt 的 logprob 算进去；
所以需要找到 response 在 token 序列里的起始 index（start_idx）。

rlvr.ipynb 的做法是：重新 tokenize prompt，取其长度作为 start_idx。
5.2 最容易踩的坑：Top-K 太小，actual token 不在返回字典里
vLLM 返回的 output.prompt_logprobs 是一个 list[dict]：

每个位置一个 dict；
dict 里是 Top-K 的 &#123;token_id: Logprob&#125;。

如果你把 prompt_logprobs 设成 1，那么它只返回 top-1 token 的 logprob。
但你要算 PPL，需要的是“实际发生的那个 token”的 logprob；如果实际 token 不是 top-1，那么你在字典里根本找不到它。
因此 notebook 里强调：prompt_logprobs 应该设大一些（vLLM 目前上限通常是 20）。
下面是 notebook 里更完整的 PPL + entropy 计算逻辑（我把注释精简成“你真正要记住的”）：
def calculate_metrics(llm, prompts, responses, top_k_for_entropy=20):    full_texts = [p + r for p, r in zip(prompts, responses)]    eval_params = SamplingParams(prompt_logprobs=top_k_for_entropy, max_tokens=1, temperature=1.0)    outputs = llm.generate(full_texts, eval_params)    ppl_scores, entropy_scores = [], []    tokenizer = llm.get_tokenizer()    for i, output in enumerate(outputs):        start_idx = len(tokenizer.encode(prompts[i], add_special_tokens=False))        token_logprobs = output.prompt_logprobs        if start_idx &gt;= len(token_logprobs):            ppl_scores.append(float(&#x27;nan&#x27;))            entropy_scores.append(float(&#x27;nan&#x27;))            continue        response_log_probs = []        step_entropies = []        for step, logprob_dict in enumerate(token_logprobs[start_idx:], start=start_idx):            if logprob_dict is None:                continue            actual_token_id = output.prompt_token_ids[step]            if actual_token_id not in logprob_dict:                # Top-K 截断导致缺失：这时你的 PPL/entropy 都不可信，必须提升 K 或换计算方式                continue            response_log_probs.append(logprob_dict[actual_token_id].logprob)            # 近似 token entropy（只用 Top-K）            step_logps = np.array([lp.logprob for lp in logprob_dict.values()])            step_ps = np.exp(step_logps)            step_entropies.append(-np.sum(step_ps * step_logps))        ppl_scores.append(np.exp(-np.mean(response_log_probs)) if response_log_probs else float(&#x27;nan&#x27;))        entropy_scores.append(np.mean(step_entropies) if step_entropies else float(&#x27;nan&#x27;))    return ppl_scores, entropy_scores
你应该从这段代码里带走 3 个“评测原则”：

prompt 与 response 的 token 边界必须对齐（同一个 tokenizer，同一个模板）。
Top-K 截断会让 PPL/entropy 失真：K 太小就别算，或至少要在 log 里显式标记为 NaN/跳过。
entropy 只是 Top-K 近似，但对“分布变尖/多样性下降”的趋势判断很有用。

5.3 prompt 模板要和训练一致，否则指标不对
rlvr.ipynb 里特意强调：不要直接用默认 chat template，而是要用和训练一致的模板拼 prompt。否则：

token 边界变了；
你算的 PPL / pass@k / entropy 全都不可比。

它用的是一个手写的 Qwen 风格模板（system + user + assistant），这在 LLM-RL 评测里非常常见。

6. 对你做 Agentic RL / Deep Research 的启示
如果你想用 agentic RL 做 deep research，你需要一开始就想清楚“你究竟想优化什么”，以及“你愿意牺牲什么”：

只要你用的是 verifiable reward（可验证正确/错误），你大概率会遇到同样的问题：RL 把分布削尖，提升单次命中率，但可能降低探索与多样性。
deep research 的难点往往不是“写出一个正确答案”，而是“找到正确问题、覆盖足够证据、做出可信引用、控制成本”，它更像一个需要探索的搜索问题。

因此你至少要准备一个“反削尖”的评测闭环：

主指标：任务成功率（含引用/事实一致性/覆盖）。
稳定性指标：KL、entropy、clip fraction（你前面几篇博客讲过）。
潜力上限：Base 的 pass@k 曲线（以及 RL 的 $\Delta_{\text{SE}}$）。
分布诊断：$\text{PPL}{\text{Base}}(Y{\text{RL}}\mid x)$，看是否真的出现“边界拓展”。

如果你后续想把这一套真正落到“可复现的训练 pipeline”，下一步建议从 verl 的 rollout + reward manager + objective 开始，把训练日志里的这些量都打通（我可以继续按代码结构拆给你）。
]]></content>
      <categories>
        <category>论文笔记</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Reasoning</tag>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>RLVR</tag>
        <tag>Evaluation</tag>
        <tag>pass@k</tag>
        <tag>Perplexity</tag>
        <tag>vLLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：Reward Model Insights（Bradley-Terry、MLE 与深度学习）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AReward%20Model%20Insights%EF%BC%88Bradley-Terry%E3%80%81MLE%20%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] [RM] 09 Reward Model insights，理解概率建模（Bradley-Terry）、MLE、深度学习的关系】（BV1z4vkBBEgD）。
我不会把它写成“逐句笔记”，而是把 RM 的核心拆成一个你能反复复用的框架：

最顶层是 MLE（决定 loss），中间层是 Bradley-Terry（赋予概率意义），最底层是 Deep Learning（提供函数逼近与表征能力）。

如果你把 Reward Model 当成“又一个神经网络”，你很容易在训练/调参/诊断上迷路；但只要把它当成一个概率模型的参数化实现，很多现象会瞬间可解释（包括：为什么 RM 会偏向“拉平”、为什么 KL 约束几乎是必需、为什么 intransitive 数据会让 RM 变钝）。
系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


RL4LLM 最小闭环：reward + PG&#x2F;KL 的工程细节


PPO&#x2F;GRPO 中 KL 的意义与调参逻辑（别背经验区间）


多轮训练的分布一致性：token-in-token-out


DPO 专文：KL 正则 RL、隐式 RM 与常见坑点


配套仓库（你本地已下载）里，本文主要对齐这两份笔记：

Bradley-Terry + MLE + 梯度直觉 + intransitive 偏好：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reward_model/Bradley-Terry-RM.ipynb


DPO 视角（可选）：为什么 KL-constrained objective + BT 偏好建模可以“消掉显式 RM”

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reward_model/dpo.ipynb




0. RM 在 RLHF/Agentic RL 里到底扮演什么角色
把 RM 放回 RLHF 的链路中，你可以把它当成一个“可微的偏好评估器”：

你有一个 prompt $x$（问题/任务/上下文）。
你有两个回答（或两个轨迹）$y_w$（winner）和 $y_\ell$（loser）。
人类（或规则/verifier）告诉你 $y_w$ 更好。
你想训练一个模型 $r_\theta(x, y)$，输出一个标量分数，使得“分差越大，winner 胜出的概率越高”。

然后这个 RM 进入后续阶段：

作为 RL（PPO/GRPO）的 reward（或其中一部分）
或者作为采样/拒绝采样/重排序器（re-ranking）
或者作为 DPO/IPO/SPIN 等偏好优化的隐式中间量

你后面看到的很多工程规则（例如 RL 里要加 KL anchor、要做 holdout 评测、要防 reward hacking），本质都来自一个事实：

RM 只在它“被训练过的数据分布附近”才可靠。

所以 KL 约束不是“玄学正则”，而是“保证策略别跑出 RM 的可信域（trust region）”的工程手段。

1. Bradley-Terry：把“分数差”解释成“偏好概率”
Bradley-Terry（BT）模型的假设非常简单，也非常强：
对同一个 prompt $x$ 下的两个候选 $y_w, y_\ell$，winner 被偏好的概率由分差决定：
$$
P(y_w \succ y_\ell \mid x)=\sigma(\Delta r)=\sigma(r_\theta(x,y_w)-r_\theta(x,y_\ell)),
$$
其中 $\sigma(u)=\frac{1}{1+e^{-u}}$。
换个更“竞赛直觉”的写法（选手 A/B）：
$$
P(A \succ B)=\frac{e^{r_A}}{e^{r_A}+e^{r_B}}=\sigma(r_A-r_B).
$$
1.1 两个常被忽略、但决定你能不能调参的性质

只关心分差，不关心绝对值

如果你把所有分数加同一个常数 $c$，分差不变，概率不变。
这意味着 RM 的“零点”是任意的，你看到的 reward 绝对值本身不重要，重要的是差。


分差尺度决定“梯度多大”

BT 通过 sigmoid 把分差压到 $(0,1)$，分差一旦很大就会饱和，梯度会变小。
所以 RM 不会“无限拉大分差”，它更倾向把学习力用在“还分不清的样本”（hard negatives）上。




2. MLE：从概率模型推导出 RM 的 loss（以及它为什么长得像 BCE）
现在我们已经定义了“偏好概率”。训练就变成了统计学最常规的事情：最大化观测到的选择的概率。
给一个偏好数据集 $\mathcal{D}={(x,y_w,y_\ell)}$，MLE 目标是：
$$
\max_\theta \prod_{(x,y_w,y_\ell)\in \mathcal{D}} P(y_w \succ y_\ell \mid x).
$$
取对数（把连乘变连加）：
$$
\max_\theta \sum_{(x,y_w,y_\ell)\in \mathcal{D}} \log \sigma(r_\theta(x,y_w)-r_\theta(x,y_\ell)).
$$
写成最小化 loss（取负号）就是：
$$
\mathcal{L}_{\text{RM}}(\theta)= - \mathbb{E}_{(x,y_w,y_\ell)\sim \mathcal{D}} \left[\log \sigma(\Delta r)\right].
$$
很多实现里也会做一个按 pair 数量的归一（例如从 $K$ 个候选里构造 $\binom{K}{2}$ 个 pair），这不会改变本质，只是改变数值尺度。
2.1 这就是“二分类交叉熵”：把 $\Delta r$ 当 logit
如果你把 $\Delta r$ 当成二分类 logit，label 恒为 1（winner 赢），那么：

BCEWithLogitsLoss(logit=Δr, label=1) 等价于 -log σ(Δr)

所以 RM 训练本质上是一个非常熟悉的监督学习问题：逻辑回归 / 二元交叉熵。
这也解释了一个工程直觉：

RM 并不“直接学习人类偏好”，它学习的是“能把 winner/loser 分开的一条判别边界”。


3. 梯度长什么样：RM 为什么会自动聚焦 hard negatives
设 $\Delta r=r_w-r_\ell$，loss 为 $L=-\log \sigma(\Delta r)$。
有一个极其重要的导数（几乎所有 RM 的训练现象都能从它解释）：
$$
\frac{\partial L}{\partial \Delta r}=\sigma(\Delta r)-1.
$$
你可以把它理解成一个“权重项”：

如果模型把 winner 打得很低（$\Delta r \ll 0$），$\sigma(\Delta r)\approx 0$，梯度 $\approx -1$：狠狠改。
如果模型分不清（$\Delta r \approx 0$），$\sigma(\Delta r)=0.5$，梯度 $=-0.5$：努力改。
如果模型已经分得很开（$\Delta r \gg 0$），$\sigma(\Delta r)\approx 1$，梯度接近 0：基本不改。

这就是为什么实际训练里你会看到 RM 逐步“把简单样本放到一边”，然后对难样本更敏感。
3.1 一个经常被误解的点：饱和不是坏事
很多人看到 sigmoid 饱和就会条件反射地说“梯度消失不好”。但对 RM 来说，饱和恰恰是合理的：

RM 的目标不是给出“无限精确的分差”，而是把 winner/loser 的相对顺序学对；
一旦分差足够大，继续增大分差并不会带来更多信息，反而会把训练算力浪费在“送分题”上。


4. Deep Learning：RM 只是把 $r(x,y)$ 参数化成可学习函数
现在你可以把 RM 看成一个黑盒函数：

输入：$(x,y)$
输出：一个标量 $r_\theta(x,y)$

具体实现通常是：

把 prompt + response 拼成一段序列
用一个 encoder（LLM/Transformer）得到 hidden states
用一个线性 head 输出标量 reward（常见做法是取最后一个 token 或平均池化）

然后用上面的 pairwise loss 训练即可。
下面给一个“够用且不误导”的 PyTorch 伪代码（不绑定任何框架）：
def rm_loss(r_w: torch.Tensor, r_l: torch.Tensor) -&gt; torch.Tensor:    # r_w, r_l: [B] scalar rewards for winner/loser    delta = r_w - r_l    return torch.nn.functional.softplus(-delta).mean()  # -log sigmoid(delta)# training stepr_w = rm(x, y_w)  # [B]r_l = rm(x, y_l)  # [B]loss = rm_loss(r_w, r_l)loss.backward()optimizer.step()
注意：这里用 softplus(-Δr) 是为了数值稳定（等价于 -log sigmoid(Δr)）。

5. Intransitive Preferences：为什么偏好数据一旦“成环”，RM 会倾向“拉平”
BT 模型隐含了一个强假设：存在一个一维的隐藏分数可以排序所有候选。
但人类偏好经常不是一维可排序的，典型现象是“非传递”：

$A \succ B$
$B \succ C$
但 $C \succ A$

这在语言模型里非常常见，例如：

长文本更“详尽”胜过短文本
短文本更“干净利落”胜过幽默文本
幽默文本更“吸引人”胜过长文本

这种数据对 BT 是不可满足约束。关键点在于：log loss 会极度惩罚“非常确信地预测错”。
所以在成环数据上，BT 的 MLE 往往会倾向一个妥协策略：

把分数拉近（flattening），让每一对的胜率都接近 0.5，从而避免任何一条边出现“confident wrong”的巨大损失。

工程含义是：如果你的偏好数据里环很多，标准 RM 会变得“钝”，对细粒度风格差异不敏感。
5.1 怎么缓解（不是银弹，但能让你少走弯路）

把偏好拆成多维属性（多头 reward / 多目标优化）

把“长度/严谨/幽默/安全”等拆开建模，后续用加权组合或约束优化。


做更强的数据分层与采样

你不一定要把所有 style 混在一个 RM 里；对特定任务可以做 task-specific RM。


承认偏好噪声并显式建模

BT 是最简单的噪声模型；如果你真的需要表达更复杂的人类偏好结构，要考虑更丰富的随机效用模型（这超出本文范围，但你至少要知道 BT 的边界）。




6. 为什么 KL 约束几乎是必需：RM 的“可信域”不是无限的
你在训练 RL（PPO/GRPO）时，常见目标会长这样（省略细节）：
$$
\max_\pi \mathbb{E}[r(x,y)] - \beta D_{KL}(\pi ,|, \pi_{\text{ref}}).
$$
这里 KL 项的工程意义非常朴素：

RM 只在“训练它的数据分布附近”靠谱；
你让 policy 生成的分布跑太远，RM 就会开始乱打分；
RL 于是会利用这个漏洞进行 reward hacking，导致能力表面上升、行为实际上变坏。

所以 KL 是一种“让策略别跑飞”的护栏，而不是可有可无的装饰。

7. （可选）DPO 视角：BT + KL 约束如何“消掉显式 RM”
如果你看过 DPO，你会发现它的数学几乎就是把上面的两件事拼在一起：

KL-constrained reward maximization 有闭式解：

$$
\pi^*(y|x)\propto \pi_{\text{ref}}(y|x)\cdot \exp\left(\frac{r(x,y)}{\beta}\right).
$$

BT 偏好模型只关心分差 $r(x,y_w)-r(x,y_\ell)$。

把 1 代入 2，会得到一个非常关键的“消元”：

归一化常数（partition function）在做差时抵消；
显式 reward 变成了 log-ratio（相对参考模型的提权幅度）；
最终 loss 仍然是 -log sigmoid(·) 的二分类形式。

这就是为什么 DPO 常被描述为：

“把 RL 问题降维成一个二分类监督学习问题（BT modeling）”

它也解释了 DPO/偏好优化里为什么依然要关心 KL/参考模型：你本质上仍然在控制“偏离可信域”的程度。
（如果你想完整看这段推导，直接对照本地笔记：reward_model/dpo.ipynb。）

8. 面向 Agentic RL / Deep Research：RM 应该怎么落地
如果你的目标是 deep research agent（检索、引用、事实一致性、长链推理），我建议你把 RM 设计成“可验证信号的组合”，而不是单纯让 RM 去学一个玄学偏好：

尽量把 reward 绑定到可验证指标

引用是否可追溯、是否引用了正确段落、事实一致性校验、是否覆盖了关键文献等。


对不可验证的“写作质量/逻辑”再用偏好数据补齐

但要接受 intransitive/噪声不可避免，并通过数据分层和多头 reward 降低冲突。


始终把“分布一致性”当第一优先级

多轮 agent 场景建议坚持 token-in-token-out，否则你在 RL 阶段很难解释 logprob/ratio/KL 的统计量。




9. 小结（你真正需要记住的）

RM 不是“又一个神经网络”，它是 Bradley-Terry 概率模型的参数化实例。
MLE 推出来的 loss 就是 -log sigmoid(Δr)，等价于 BCE；它的梯度天然会聚焦 hard negatives。
偏好数据存在大量 intransitive 环时，BT 会倾向“拉平”，这是模型边界而不是实现 bug。
KL 约束的核心作用是：把策略限制在 RM 的可信域附近，避免 reward hacking 与 OOD 崩坏。

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Reward Model</tag>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>RLHF</tag>
        <tag>Bradley-Terry</tag>
        <tag>MLE</tag>
        <tag>概率建模</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：Tokenizer 编解码非对称性与 Token-in-Token-out（RL 训练崩溃的根因）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9ATokenizer%20%E7%BC%96%E8%A7%A3%E7%A0%81%E9%9D%9E%E5%AF%B9%E7%A7%B0%E6%80%A7%E4%B8%8E%20Token-in-Token-out%EF%BC%88RL%20%E8%AE%AD%E7%BB%83%E5%B4%A9%E6%BA%83%E7%9A%84%E6%A0%B9%E5%9B%A0%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[veRL] tokenizer 编解码的非对称性，RL 训练崩溃到 Agent loop 中的 token in token out】（BV1b2pDzYEY2）。
我不会把它写成“视频复述”，而是把它抽象成一个你做 RL4LLM / Agentic RL / Multi-turn Tool Use 一定会遇到的工程定律：

在 RL 训练里，token_ids 才是“行为（action）”本体；把它 decode 成文本、再 encode 回去，往往已经不是同一个行为了。

一旦你在 rollout 的链路里出现 decode → encode（尤其是 multi-turn），你就可能让 PPO/GRPO 训练变成“在错误分布上算 logprob”，表现为：

approx_kl/clipfrac/loss 统计异常
reward curve 不上升，甚至彻底不收敛
multi-turn agent loop 越跑越乱（历史拼接后 token 逐步漂移）

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


Tool-use agent 的训练闭环（cold-start SFT + async rollout + response_mask）


SFT 工程：交叉熵 &#x2F; loss mask &#x2F; scheduler


PG&#x2F;PPO loss 组件：ratio&#x2F;clip&#x2F;KL&#x2F;entropy&#x2F;聚合


配套仓库（你本地已下载）中，这篇文章会重点对齐这两个 notebook：

tokenizer 编解码非对称性示例与复现：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/tokenizer/encode-decode.ipynb


agent loop 的“token in token out”原因、async rollout、prompt_ids/response_mask：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_details.ipynb




0. 先把术语讲死：什么叫“非对称性”
下面用两个函数记号（不用纠结数学，目的是把问题说清楚）：

E(text)：tokenizer encode，把文本转成 token ids
D(token_ids)：tokenizer decode，把 token ids 还原成文本

你在直觉上可能希望它们互为逆：

D(E(text)) == text（大多数情况下“差不多对”，但也可能有空白符/特殊 token 等细节差异）
E(D(token_ids)) == token_ids（这句在工程上经常是错的）

关键在第二句：LLM 生成的是 token ids 序列，但 tokenizer 的 encode 是“给定文本的规范分词”。两者不是同一个过程。
encode-decode.ipynb 里用一句话总结得很准确：

D(E(text)) = text（更常见）
E(D(token_ids)) ≠ token_ids（非常常见）

只要你理解了这点，很多“RL 不收敛”的怪现象就会变得可解释。

1. 为什么 E(D(ids)) 可能改变 ids：模型的生成路径不是“规范分词”
tokenizer（BPE / Unigram / SentencePiece 等）的 encode 往往有一个“规范性”：

贪心 / 最长匹配 / 最优分词：给定同一段文本，encode 通常会选择一个“唯一或偏唯一”的 token 切分。

但模型生成是逐步选 token，它可能走出一种“非规范拼法”：

词表里同时存在 &lt;think&gt; 和 &lt;, think, &gt; 这些 token；
模型完全可能一步步采样出 &lt;、think、&gt;；
你把它 decode 成字符串 &quot;&lt;think&gt;&quot; 后，再 encode 回去，tokenizer 可能直接用 &lt;think&gt; 这个 token（或另一种更长切分）。

于是：

原始生成 ids：[ &quot;&lt;&quot;, &quot;think&quot;, &quot;&gt;&quot; ]
回编码 ids：[ &quot;&lt;think&gt;&quot; ]

这不是罕见 corner case，而是“只要词表允许，训练早晚会遇到”的常态。
1.1 repo 里的最小复现（直接对照 notebook）
encode-decode.ipynb 给了一个非常具体的例子（示意）：
# 两条不同的 token 路径，decode 后可能是同一个字符串 &quot;&lt;think&gt;&quot;D([13708, 766, 29]), E(D([13708, 766, 29]))D([27, 26865, 29]), E(D([27, 26865, 29]))
这类例子在 LLM 里尤其多见，因为：

你会用大量“协议 token”（XML/JSON/工具调用标签/role tag）
词表里经常同时存在“整段标签”和“字符级碎片”


2. 为什么这在 serving/agent 产品里不算致命，但在 RL 训练里是致命的
先说结论：

Serving/agent 产品：你 decode 出来能看、能用、能继续对话，通常就没事。
RL 训练：你必须保证 rollout 的轨迹真的是从 policy 分布里采样出来的，否则 logprob/ratio/KL 的意义会被破坏。

PPO/GRPO 等算法依赖的最核心事实是：

你在计算的 log π_\theta(a_t|s_t)，必须对应“真实执行过的 action”。

如果你把 action（token ids）换成了另一组 ids，你就变成：

用 行为 A 产生 reward
却用 行为 B 计算 logprob/ratio/KL

这等价于一种非常隐蔽的 off-policy mismatch，轻则不收敛，重则直接发散。
2.1 最常见的触发点：multi-turn 把历史存成 messages，然后每轮 apply_chat_template
几乎所有 agent 框架（LangGraph / CrewAI / LlamaIndex…）都会：

把对话历史存成 messages（role/content/tool）
每次生成前把整段 messages 重新拼成 prompt string
再 encode 给模型

在推理产品里这很正常；但在 RL 训练里，它会引入一个关键不一致（encode-decode.ipynb 里点得很直白）：

encode(messages) != (prompt_ids ⊕ response_ids)
即：对“最终 messages”做一次整体 encode，可能不等于你每轮把 prompt_ids 与 response_ids 拼起来得到的 token 序列。

特别是在你经历过：

模型生成了“非规范 token 切分”（上一节说的 &lt;, think, &gt;）
tool output / observation 被插入到历史里（包含大量特殊字符/空白/换行）
chat template 自动插入 separator / BOS/EOS / role tokens

这种不一致会越来越大，最终 PPO 会表现得像“完全不在一个坐标系里训练”。

3. Token-in-Token-out：把“轨迹 = token ids”当作不可变事实
veRL/verl 的解决方案非常工程化，也非常值得你在自己的 agentic RL 系统里照搬：

输入用 token ids，输出也用 token ids；把文本当日志，而不是训练数据的主形态。

也就是所谓 token in, token out：

in：给 LLM server 的是 prompt_ids: list[int]，而不是 messages/text
out：拿到的是 response_ids: list[int]，并把它 append 回 prompt_ids

在 agent_loop_details.ipynb 里，AgentLoop 甚至强调：

“避免 Chat Template 在多轮对话中反复 Encode/Decode 导致的不一致问题”
AsyncLLMServerManager.generate(prompt_ids=...)（输入是 token ids）

3.1 一个足够接近真实系统的伪代码
你可以把整个多轮过程理解成“只做 token 级拼接，不做 message 级重建”：
prompt_ids = encode_initial(system_and_user_messages)  # 只做一次while not done:  # 生成：输入 token ids  response_ids = llm_generate(prompt_ids)  # 训练所需的轨迹：直接存 token ids  prompt_ids += response_ids  # 如果触发 tool call：把 tool output 也 encode 成 ids 并追加（但这部分不算 action）  tool_output_ids = encode_tool_output(tool_output_text)  prompt_ids += tool_output_ids
注意：这里的 encode_tool_output 不是为了“让模型训练 tool output”，而是为了让它在下一轮生成时能看到 observation。真正训练时，你会用 response_mask 把 observation token 排除掉（在 tool-use 那篇里我已详细解释）。

4. Debug 清单：你怎么判断自己踩中了这个坑
我建议你在 RL 训练（尤其 multi-turn）里把下面 3 个检查写成单元测试/日志断言：
4.1 检查 1：是否出现了不该出现的 decode → encode
任何一条路径只要出现：

text = tokenizer.decode(ids)
ids2 = tokenizer.encode(text)

你就要默认它会改变 ids，除非你证明不会。
4.2 检查 2：整体 apply_chat_template(messages) 是否等于增量 token 拼接
对同一条轨迹，做两种构造：

最终 messages 整体 encode（常见 agent 做法）
每轮增量拼接 prompt_ids ⊕ response_ids（token-in-token-out）

如果二者不一致，你就不要在 RL 训练里用 messages 方式做“训练用 token”。
4.3 检查 3：出现“不合物理直觉”的 PPO 统计量
典型症状：

approx_kl 很大但模型输出看起来没怎么变
clipfrac 长期极端（接近 0 或接近 1）
reward 曲线停滞但 loss 各项在剧烈抖动

这些现象当然也可能由学习率/优势刻度等引起，但 token mismatch 是你应该优先排除的一类“结构性错误”。

5. 这对你的 deep research / tool-use agent 意味着什么
deep research 类 agent 往往具备“天然放大这个问题”的特征：

多轮：history 很长，累计偏差更大
interleaved：tool output 文本复杂、包含换行/代码/引用，chat template 更容易插入或规范化
强依赖 logprob：你用 RL 训练时需要稳定的 logprob/ratio/KL/entropy 统计来诊断

因此我的建议是：

从第一天就把 token-in-token-out 作为 agent loop 的底层接口，不要等 PPO 不收敛了再改。
把“messages 形态”降级成 UI/日志形态；训练与评测闭环都基于 token ids。
tool output 永远当 observation，并用 response_mask 排除在 policy gradient 之外。


6. 小结
这篇的核心只有一句话：

RL 训练的轨迹必须是“策略真实生成的 token ids”；任何把 token 变回文本、再变回 token 的流程，都可能让你在错误分布上优化，导致不收敛。

如果你后续继续按 veRL/verl 的路线做 multi-turn tool-use/coding agent，我建议你把这篇当作“工程底线”，比任何超参经验都更优先。
]]></content>
      <categories>
        <category>工程实践</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>Tokenizer</tag>
        <tag>Token-in-Token-out</tag>
        <tag>Agent Loop</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：vLLM 参数配置、显存分析与性能调优（max_num_batched_tokens）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AvLLM%20%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E3%80%81%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%88max_num_batched_tokens%EF%BC%89/</url>
    <content><![CDATA[这一篇对应视频 05：“vLLM 参数配置、显存分析与性能调优 max_num_batched_tokens”（BV1QnSFBkEZU）。
这期的核心不是“教你把服务跑起来”，而是给你一个可以复用的调参心智模型：

vLLM 的显存到底被谁吃掉（权重 / KV cache / peak activation / CUDA Graph / 杂项）。
max_model_len、max_num_seqs、max_num_batched_tokens 之间到底是谁在限制并发与吞吐。
为什么 max_num_batched_tokens 既影响“吞吐”，又会反过来影响“能留给 KV cache 的空间”（因为它参与了 profile 的 peak activation 测量）。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


配套仓库（你本地已下载）：wdkns/modern_genai_bilibili，本文主要对齐两份笔记：

agentic_rl/infra/inference/vllm_sglang.ipynb（本期视频几乎就是这个 notebook 的讲解版）
agentic_rl/verl/vllm-rollout.ipynb（把这些参数放回 RL rollout 场景：prefill+decode 混跑的调度直觉）


0. 一句结论（先给你调参抓手）
vLLM 调参可以简化成一个闭环：

先定 max_model_len（你业务上允许的单请求最大上下文，越大越吃 KV）。
设 gpu_memory_utilization（给 vLLM 一个“预算”，但要记住 CUDA Graph 显存不算在预算里）。
从日志里读出：weights/peak_activation/non_torch/cudagraph/kv_cache 的分账。
再调：

max_num_seqs：你希望系统“最多并发多少条序列”（逻辑上限）。
max_num_batched_tokens：每一步前向“最多塞多少 token”（决定 decode 并行度，也决定 chunked prefill 的块大小，也决定 profile 的峰值激活）。


以日志里的 GPU KV cache usage 为核心指标：

如果 KV 利用率长期很低，且你吞吐不高，通常说明 有效 batch 太小：提高 max_num_seqs 或 max_num_batched_tokens。
如果 OOM 或 KV cache 太小导致并发上不去：降低 max_model_len 或降低 max_num_batched_tokens（因为它会抬高 peak activation，从 KV 预算里“扣钱”）。




1. vLLM 的两个推理场景：offline batch vs online serve
视频先把推理场景分成两类（这点很重要，因为你调参目标不一样）：

offline batch generation

目标是吞吐（tokens/s），对单条请求的延迟不敏感。


online 部署（OpenAI API compatible server）

同时关心吞吐和延迟（特别是 TTFT/ITL）。



另外还有一个“对 Agentic RL 更关键”的第三类场景：rollout generation（在 RLHF / agentic RL 的 on-policy 训练里作为生成引擎），这也是 verl/openrlhf 为什么会强依赖 vLLM 的原因：用 transformers 做 rollout 太慢。
1.1 指标口径：tokens/s、TTFT、ITL 你到底该优化哪个
同样是“推理性能”，不同场景的指标口径完全不同。把它说清楚，你后面的调参动作就不会互相打架：

吞吐（throughput，tokens/s）

你希望 GPU 每秒吐出更多 token。
max_num_seqs 和 max_num_batched_tokens 往往是主要旋钮：更大 batch 通常更高吞吐。


TTFT（Time To First Token）

从请求进来到首 token 返回的时间，主要被 prefill（处理 prompt）决定。
长 prompt + 大 batch + 不合理的 prefill 调度，会显著拖慢 TTFT。


ITL（Inter-Token Latency）

streaming 输出时，相邻两个 token 的间隔，主要被 decode 的 step 装配与 batch 大小影响。
你把 max_num_batched_tokens 拉得很大，吞吐可能上去，但每一步要处理的 token 也更多，ITL 往往会变差。



最常见的坑是：你以为你在“优化性能”，其实你只是在把“吞吐”和“延迟”互相兑换。线上如果用户关心交互体验，你应该优先保证 TTFT/ITL 的稳定，再追 tokens/s；而 RL rollout 更偏吞吐优先，但也要避免 tail latency 把整个 pipeline 卡死。

1.5 一个能跑起来的 vllm serve 命令模板（建议你从这里开始改）

下面这条命令基本覆盖了视频里提到的关键参数；你先跑通，再按日志调。

export VLLM_LOGGING_LEVEL=DEBUGvllm serve Qwen/Qwen2.5-7B-Instruct \  --host 0.0.0.0 \  --port 8000 \  --served-model-name qwen2.5-7b \  --api-key ABC123 \  --gpu-memory-utilization 0.85 \  --max-model-len 8192 \  --max-num-seqs 256 \  --max-num-batched-tokens 8192 \  --enable-prefix-caching \  --enable-auto-tool-choice \  --tool-call-parser hermes
几个你一定要建立直觉的参数（对齐视频讲解）：

--max-model-len：单请求（prompt+output）最大长度。越大越吃 KV cache，并发会被拉低。
--gpu-memory-utilization：vLLM 的显存预算系数（权重+KV cache，不包含 CUDA Graph）。
--max-num-seqs：并发序列逻辑上限，真正能跑多少取决于 KV cache 的物理容量。
--max-num-batched-tokens：每一步 forward 的 token 预算，也是 chunked prefill 的“切块上限”，还是 profile 里的 dummy 长度（会影响 peak activation）。
--enable-prefix-caching：跨请求共享 prompt 前缀（对 agent 场景很常用：system prompt、工具 schema、固定模板）。
--enforce-eager（可选）：关闭 CUDA Graph（显存紧张或 graph capture OOM 时救急，但性能会掉一些）。


2. 先把 3 个关键参数讲清楚：max_model_len / max_num_seqs / max_num_batched_tokens
下面这三者一定要一起理解：
2.1 max_model_len：单请求的最大 token 总长

定义：单条请求允许的最大 token 数（prompt + output）。
直觉：它越大，每条请求的 KV cache 上限越大，并发越容易被“拉低”。

2.2 max_num_seqs：并发序列的逻辑上限

定义：一次最多允许多少条活跃序列。
直觉：它不是“物理保证能同时跑这么多条”，只是调度器允许的上限。真正能跑多少，还要看你 KV cache 的物理容量。

2.3 max_num_batched_tokens：每一步前向的 token 预算
这是本期重点：

定义：一次 GPU forward（一个 step）里，所有请求加起来最多处理多少 token（单位是 token，不是请求数）。
默认值通常是 2048。
关键联动：

在 decode 阶段：每条序列每步只生成 1 token，所以 tokens_in_step ≈ num_decode_seqs。
在 prefill 阶段：prompt token 可以 batch 处理；而且 vLLM 默认启用 chunked prefill，会把长 prompt 切成“每块不超过 max_num_batched_tokens”的片段。



一句话：max_num_batched_tokens 同时是

decode 的并行度上限（你能一口气并行多少条 decode）
prefill 的 chunk 大小上限（长 prompt 每步最多灌多少 token 进 KV）
vLLM 启动时 profile 的 dummy 输入长度（决定 peak activation memory）


3. 为什么说“读日志”是 vLLM 调参的前置：打开 DEBUG
视频强调要先打开 debug 日志，否则你几乎是盲调：
export VLLM_LOGGING_LEVEL=DEBUG
你需要从日志里读到这些量：

Prompt throughput / Generation throughput（prefill 与 decode 的 tokens/s）
GPU KV cache usage（paged attention 下“页表用了多少页”，是最关键的利用率指标）
Prefix cache hit rate（如果你开了 --enable-prefix-caching）
启动时的 memory profiling：weights / peak activation / non-torch / cudagraph / kv cache 的分账


4. 显存分账：vLLM 到底把你的 24GB 显存怎么花掉的
这段建议你直接对齐仓库笔记里的日志解析：

agentic_rl/infra/inference/vllm_sglang.ipynb（含原始 log 与逐项解释）

一个典型日志长这样（以 24GB 卡为例，数字会随模型与参数变化）：
Free memory on device (23.2/23.65 GiB) on startup.Desired GPU memory utilization is (0.85, 20.1 GiB).Actual usage is 14.25 GiB for weight,0.28 GiB for peak activation,0.04 GiB for non-torch memory,and 0.17 GiB for CUDAGraph memory.Current kv cache memory in use is 5.54 GiB.
你应该建立的“账本公式”是：
$$\text{KVCacheBudget} \approx (\text{TotalVRAM}\times u) - \text{Weights} - \text{PeakActivation} - \text{NonTorch}$$
其中 $u$ 是 --gpu-memory-utilization。
重要细节（视频里专门强调）：CUDA Graph 显存不计入 gpu_memory_utilization 的预算里。所以你把 $u$ 拉到很接近 1 可能会在 graph capture 时 OOM。
如果你显存极紧，可以用 --enforce-eager 关闭 CUDA Graph（牺牲部分性能换可用性）。

5. KV cache 能“挂住”多少 token：从 GiB 到 tokens 的换算
vLLM 启动时通常会直接给你打印：
Available KV cache memory: 5.54 GiBGPU KV cache size: 103,712 tokens
但真正有价值的是你要会“反推为什么是这个数”，因为这决定你的最大并发。
5.1 单 token 的 KV cache 大小（最常用的工程估算）
对大多数 decoder-only 模型，KV cache 的单 token 成本可以写成：
$$\text{Size}_{token} = 2 \times L \times N_{kv} \times D_{head} \times P_{byte}$$

2：因为 K 和 V 各一份
$L$：层数（num_hidden_layers）
$N_{kv}$：KV heads 数（注意 GQA/MQA 下它小于 attention heads）
$D_{head}$：head_dim
$P_{byte}$：dtype 字节数（FP16/BF16 通常是 2）

仓库里给了一个 Qwen2.5-7B 的例子（可直接对齐 config）：
2 * 28 * 4 * 128 * 2 = 57344 bytes/token
也就是每个 token 约 56 KB（这就是为什么长上下文一上来并发就掉得很快）。
5.2 最大并发（物理上限）怎么估
最粗但很实用的估算是：
$$\text{MaxConcurrency} \approx \frac{\text{KVCacheTokens}}{\texttt{max_model_len}}$$
仓库笔记里给了两个对比（同样的 KV cache tokens，不同 max_model_len）：
# --max-model-len 32768GPU KV cache size: 98,960 tokensMaximum concurrency for 32,768 tokens per request: 3.02x# --max-model-len 8192GPU KV cache size: 98,960 tokensMaximum concurrency for 8,192 tokens per request: 12.08x
注意：这里的 “x” 是指“能同时容纳多少条满长度请求”。真实线上请求通常更短，所以实际并发可以更高，但这个估算对你理解上限很有用。

5.3 Paged Attention：为什么日志里叫 “GPU KV cache usage”（像在看“页表利用率”）
这也是 vLLM 和传统推理实现最不一样的地方之一：KV cache 的显存管理方式。
传统实现更像“每条序列分一整块连续显存”。问题是：

并发一高、序列长度不一，显存容易碎片化
你可能“总量够但找不到一整块连续空间”，也会 OOM

vLLM 的做法更像操作系统的分页内存：

把 KV cache 切成很多固定大小的小块（block/page）
用一个 block table（页表）记录“逻辑上连续的上下文”对应到“物理上离散的 block”
所以显存可以像分页一样动态分配、回收，碎片问题大幅缓解

因此日志里的 GPU KV cache usage 更像：

“现在 KV cache 的 page 用了多少”
你可以把它当成一个非常关键的利用率指标，用来判断当前并发/有效 batch 有没有把 GPU 喂饱


6. max_num_batched_tokens 的本质：一步 forward 的 token 预算 + prefill chunk 大小
这一段你可以把视频的“prefill vs decode”理解和 agentic_rl/infra/inference/vllm_sglang.ipynb 的例子合在一起看。
6.1 decode 阶段：tokens_in_step ≈ num_decode_seqs
在纯 decode 阶段：

每条活跃序列每步只产 1 个 token
所以 step 内 token 数约等于并发序列数

结论：

如果你 max_num_batched_tokens=2048，但你只有几十条并发，那 token 预算根本打不到上限。
此时提升吞吐的第一杠杆往往是 提高 max_num_seqs（让更多请求一起 decode）。

6.2 prefill（chunked prefill）阶段：token 预算被用来“切长 prompt”
chunked prefill 的直觉是：避免一个超长 prompt 一次性占满 prefill，拖垮其他请求的 ITL。
调度器在一个 step 内要满足：
$$\sum(\text{prefill_chunk_tokens}) + \sum(\text{decode_tokens}) \le \text{max_num_batched_tokens}$$
视频举了一个很清晰的例子（这也是你在线上最常见的混跑形态）：

设 max_num_batched_tokens = 8192
正在 decode 的并发数 N_decode = 100

decode 占用 100 个 token 预算


则本步还剩 prefill 预算：

prefill_budget = 8192 - 100 = 8092


如果队列里插入一个 20k 的长 prompt，本步就会切出 8092 token 做一次 prefill chunk。

这也是为什么你会在日志里看到：
Chunked prefill is enabled with max_num_batched_tokens=8192
6.3 它还会影响 peak activation（很多人忽略的“反直觉”点）
vLLM 启动时会做一次 memory profiling：

用一批 dummy token（长度 = max_num_batched_tokens）跑一次前向
记录这次前向的 peak activation memory
然后把它当成运行时预留，从你的 budget 里扣掉

所以：你把 max_num_batched_tokens 调得很大，可能导致 peak activation 变大，反过来挤压 KV cache 的预算，从而降低最大并发。
这就是为什么它不是“无脑越大越好”的参数。

7. Prefix caching vs KV cache：别把两个“缓存”混了
视频专门强调了一次区别（这个对 agent 场景非常关键）：

KV cache：单请求内部自回归过程中缓存 K/V，用于 decode 加速。
Prefix caching：prefill 阶段跨请求共享（适合大量共享系统提示词/工具 schema 的在线服务）。

你开了 --enable-prefix-caching 后，日志里会出现 prefix cache hit rate，一定要看这个指标，否则你不知道自己开了有没有收益。

8. 调参 SOP（我建议你按这个顺序来）

先把模型跑起来（别一上来就追“极限吞吐”）
打开 debug：export VLLM_LOGGING_LEVEL=DEBUG
读清楚启动时分账（weights / peak activation / non-torch / cudagraph / kv cache）
先定 max_model_len

你允许的最大上下文是多少（这决定物理并发的上限）


再调 max_num_seqs

如果 decode 阶段 token budget 没打满，通常先加它


再调 max_num_batched_tokens

如果 GPU KV cache usage 低，且你并发也不高，说明有效 batch 小，可以加它
但每次加都要看：peak activation 是否变大、KV cache tokens 是否被挤小


如果 graph capture OOM 或显存太紧：

临时 --enforce-eager（关 CUDA Graph）救急


面向 RL rollout（verl）时，把同一套直觉迁移过去

参考：agentic_rl/verl/vllm-rollout.ipynb




9. 把它放回 Agentic RL：为什么 rollout 更吃这些参数
视频开头提到：vLLM 不只是 serving，还会用于 RL4LLM 的 rollout generation。原因很直接：

rollout 是 on-policy 的：你要不停生成新样本
现在的 reasoning 模型 decode 很长，生成成本极高
rollout 吞吐几乎决定了你整个训练 pipeline 的 wall-time

所以当你后面做 agentic RL（deep research / tool-use / long-CoT）时：

你可以不微调 LLM 本体，也要把 rollout 引擎调到合理吞吐
max_num_batched_tokens/chunked prefill/kv cache budget 对“长上下文 + 多并发”的场景非常关键

9.1 rollout 场景的额外成本：你可能需要 logprobs（不是只要文本）
很多人第一次把 vLLM 接进 RL 框架，会忽略一个事实：训练时你往往不只要生成的文本，还要概率信息（至少是 logprob 差异），用来算：

PPO/GRPO 的 ratio：r = exp(logp_new - logp_old)
KL（对 ref 或对 old 的约束/监控）：无论是 sampled 近似还是 full-distribution

这会带来两个工程后果：

你需要额外的前向计算（有时甚至是第二个模型的前向），吞吐会比“只生成文本”更低。
你需要更严格的 token 边界与 mask（prompt/response 切分、EOS 之后不计入等），否则 KL/ratio 统计会非常诡异，导致你以为是“算法不稳”，其实是“指标算错”。

所以在 rollout 里调 vLLM，建议你明确自己到底要返回什么：

只要文本：优先把 tokens/s 打满
还要 logprobs/KL：吞吐目标要更保守，并把 logprob 计算的开销算进预算（这时 max_num_batched_tokens 往往要更谨慎）

如果你下一步要用 vLLM 做更“像论文”的评测（pass@k / PPL / token entropy），并且希望指标可信，建议直接对照这一篇的评测细节与坑点：


Agentic RL：RLVR 的边界（Base vs RL、pass@k、PPL 与 vLLM 评测细节）



10. 你可以直接对齐的仓库材料

agentic_rl/infra/inference/vllm_sglang.ipynb

本期核心参数解释、日志分账、KV token 估算公式、chunked prefill 的例子


agentic_rl/verl/vllm-rollout.ipynb

站在 RL rollout 的视角解释 “prefill chunk + decode 混跑” 的 step 装配过程



如果你希望我把这两份 notebook 的关键内容进一步抽象成“可复制粘贴的调参模板 + 常见故障排查（OOM/吞吐低/ITL 差）”，我可以在这个系列里再加一篇专门做 troubleshooting。
]]></content>
      <categories>
        <category>工程实践</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>vLLM</tag>
        <tag>推理部署</tag>
        <tag>性能优化</tag>
        <tag>显存分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL AgentLoop 全流程与计算细节（Async Rollout、状态机、Tool-Interaction）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20AgentLoop%20%E5%85%A8%E6%B5%81%E7%A8%8B%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%BB%86%E8%8A%82%EF%BC%88Async%20Rollout%E3%80%81%E7%8A%B6%E6%80%81%E6%9C%BA%E3%80%81Tool-Interaction%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] 14 verl AgentLoop 全流程与计算细节，async rollout 实现，状态机，tool / interaction】（BV18d6sBpEZq）。
我会用“系统 + 数据契约”的方式把 AgentLoop 讲清楚：不是复述视频，而是把你真正会卡住的点拆开，直到你能做到：

读 AgentLoop 相关代码时，能快速定位“当前在数据流的哪一段”。
你能解释清楚：为什么 agent loop 输出必须是交错轨迹（LLM token + tool obs token），以及为什么必须带 response_mask。
你能把“tool / interaction / termination / reward attach”这几个最容易写乱的逻辑写成一套可 debug 的状态机。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


建议先读（否则你会觉得本文有点“infra 细节过密”）：


12 先建立 AgentLoop 的架构直觉（async &#x2F; sticky &#x2F; hybrid &#x2F; mask）


13 先把 AgentLoop 插到 RayPPOTrainer 的主循环里


mask 影响 logprob&#x2F;ratio&#x2F;KL 的计算口径


本文主要对齐你本地仓库的这些笔记（推荐只看其中的关键段落，不要从头刷到尾）：

AgentLoop 的状态机、mask、tool/interaction、输出字段与 reward attach：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_details.ipynb




0. 先讲一句最重要的：AgentLoop 不是“推理编排”，而是“可训练轨迹生成器”
LangChain/LangGraph 也能做 tool-use agent 的编排，但它们通常不会强迫你保留这些“训练所需的数据”：

token ids（每一轮到底生成了哪些 token）
action vs observation 的边界（tool 输出到底算不算 policy 的 action）
reward 的挂载位置（哪一个 token 上有非零 reward）
训练阶段能否 token-in-token-out 地重算 logprob（否则 ratio/KL 全崩）

而 veRL/verl 的 AgentLoop 是为 RL 训练服务的：它输出的不是“最终答案字符串”，而是一条可用于计算 PG/PPO/GRPO loss 的轨迹。
你可以把它理解成：

AgentLoop = 逻辑状态机 + 轨迹数据结构（契约） + 异步执行框架（吞吐）

本文就是把这三件事拆开讲清楚。

1. 数据契约：AgentLoopOutput / AgentData / DataProto 三者如何对齐
agent_loop_details.ipynb 把 AgentLoop 的输出 schema 写得很明确（我建议你把这段当成“接口文档”背下来）：
class AgentLoopOutput(BaseModel):    &quot;&quot;&quot;Agent loop output.&quot;&quot;&quot;    prompt_ids: list[int]    &quot;&quot;&quot;Prompt token ids.&quot;&quot;&quot;    response_ids: list[int]    &quot;&quot;&quot;Response token ids including LLM generated token, tool response token.&quot;&quot;&quot;    response_mask: list[int]    &quot;&quot;&quot;Response mask, 1 for LLM generated token, 0 for tool response token.&quot;&quot;&quot;    response_logprobs: Optional[list[float]] = None    &quot;&quot;&quot;Log probabilities for the response tokens.&quot;&quot;&quot;    reward_score: Optional[float] = None    &quot;&quot;&quot;Reward score for the trajectory.&quot;&quot;&quot;    num_turns: int = 0    &quot;&quot;&quot;Number of chat turns, including user, assistant, tool.&quot;&quot;&quot;    metrics: AgentLoopMetrics    &quot;&quot;&quot;Auxiliary performance metrics&quot;&quot;&quot;    extra_fields: dict[str, Any] = &#123;&#125;    &quot;&quot;&quot;Extra fields for dynamic addition.&quot;&quot;&quot;
这里面最关键的是三项：

response_ids：交错轨迹（LLM 生成 token + tool 返回 token）都塞在一起。
response_mask：告诉训练代码，“哪些 token 属于 policy action”。
num_turns：不是装饰，它常常被用来做 reward shaping / debug（例如鼓励 tool 调用）。

而在状态机内部，通常会用一个 AgentData 来累积轨迹（agent_loop_details.ipynb 也给了骨架）：
class AgentData:    prompt_ids: list[int] = []      # 不断追加    response_mask: list[int] = []   # 标记哪些是 LLM 生成的    response_logprobs: list[float] = []    turn_scores: list[float] = []   # 每轮 interaction 的 reward    tool_rewards: list[float] = []  # 每次工具调用的 reward    user_turns = 0    assistant_turns = 0
最后，AgentLoopManager/Worker 的批处理层会把一堆 AgentLoopOutput 合并成一个 DataProto batch 返回给 Trainer（上一期/上上期已经串起来了）。
1.1 response_mask 的“硬约束”：tool 输出必须是 observation（mask=0）
agent_loop_details.ipynb 给了最精简的规则：
# LLM 生成的 token: mask = 1agent_data.response_mask += [1] * len(agent_data.response_ids)# Tool 返回的 token: mask = 0agent_data.response_mask += [0] * len(response_ids)
背后原因很简单，但几乎是所有 agentic RL 崩溃的第一现场：

tool 输出不应该参与 policy gradient（它不是 policy 的 action 分布）。
一旦你把 tool token 当 action token 去算 ratio/KL，loss 在数学上就失真了，训练曲线会看起来“有梯度”，但其实在优化一个虚假的目标。

你可以把它当作一条底线：

任何进入 compute_log_prob / ratio / KL / entropy 的 token，都必须是 LLM 真正生成出来的 action token。


2. 状态机：把 tool / interaction / termination 写成可 debug 的结构
AgentLoop 的核心是一个显式状态机（5 状态），agent_loop_details.ipynb 给了非常标准的版本：
stateDiagram-v2    [*] --&gt; PENDING: Start    state &quot;PENDING (初始化)&quot; as PENDING    state &quot;GENERATING (模型生成)&quot; as GENERATING    state &quot;PROCESSING_TOOLS (执行工具)&quot; as PROCESSING_TOOLS    state &quot;INTERACTING (环境/用户交互)&quot; as INTERACTING    state &quot;TERMINATED (结束)&quot; as TERMINATED    PENDING --&gt; GENERATING : 准备好 Prompt    GENERATING --&gt; PROCESSING_TOOLS : 模型决定调用工具    GENERATING --&gt; INTERACTING : 无工具调用 &amp; 配置了交互环境    GENERATING --&gt; TERMINATED : 完成任务 / 达到最大轮数 / 达到长度限制    PROCESSING_TOOLS --&gt; GENERATING : 工具执行完毕 (更新上下文)    PROCESSING_TOOLS --&gt; TERMINATED : 上下文超长    INTERACTING --&gt; GENERATING : 收到外部反馈 (更新上下文)    INTERACTING --&gt; TERMINATED : 交互结束 (should_terminate=True)    TERMINATED --&gt; [*]: End
把每个状态“只干一件事”，你会得到一个很强的工程性质：

tool 的并发、超时、重试都集中在 PROCESSING_TOOLS。
user/env simulator 的终止规则都集中在 INTERACTING。
token/mask 的维护集中在 GENERATING 与 tool/interact 的 append 动作。

agent_loop_details.ipynb 还给了每个状态的职责摘要（非常值得抄到你自己的代码注释里）：

PENDING：初始化 prompt_ids、apply_chat_template
GENERATING：调用 LLM 生成，解析 tool_calls
PROCESSING_TOOLS：并行执行工具，收集结果
INTERACTING：调用环境获取反馈（可能返回 should_terminate）

2.1 tool vs interaction：不要把两者混成“外部 API”
从 RL 视角看，两者都算“环境反馈”，但工程上差异很大：

tool：通常是“输入一段结构化调用，得到结构化输出”（可并发、可重试、可缓存）。
interaction：更像一个“有状态环境”（可能需要 instance_id、可能依赖历史、可能决定终止）。

如果你把 interaction 当 tool 来写，你会遇到：

状态丢失（回合之间上下文不一致）
终止条件分散在各处（debug 很痛）

所以单独一个 INTERACTING 状态是合理的设计，不是“架构洁癖”。

3. async rollout 的实现：三层并发，两个同步点
这一段是视频 14 的核心，也是 veRL agent loop “厉害但容易误解”的地方。
agent_loop_details.ipynb 给了一个从 Manager 到 Server 的分步伪代码，你可以把它当作真实实现的骨架：

Manager 拆 batch → 分发到多个 Worker（Ray 并行）
Worker 为每个 sample 启动一个 asyncio task（协程并行）
每个 task 运行状态机，在 GENERATING 里 await 推理，在 PROCESSING_TOOLS 里 await 工具

# Step 1: Manager 分发到 Workerschunkes = prompts.chunk(len(self.agent_loop_workers))outputs = ray.get([worker.generate_sequences.remote(chunk) for worker, chunk in zip(...)])# Step 2: Worker 内部为每个 sample 创建 async tasktasks = []for i in range(len(batch)):    tasks.append(asyncio.create_task(self._run_agent_loop(...)))outputs = await asyncio.gather(*tasks)  # 并发执行# Step 3: 每个 task 运行状态机state = AgentState.PENDINGwhile state != AgentState.TERMINATED:    ...# Step 4: 状态机调用 LLM Server（sticky）output = await self.server_manager.generate(    request_id=agent_data.request_id,    prompt_ids=agent_data.prompt_ids,    sampling_params=sampling_params,)
3.1 两个同步点在哪里（你调吞吐必须先知道这个）

batch 级同步点：ray.get([...]) 之后 concat。

这会让“最慢 worker”决定这个 batch 的 wall time。


worker 内同步点：await asyncio.gather(*tasks) 之后 postprocess。

这会让“最慢 trajectory”决定这个 worker 的完成时间。



你会发现：async 并不是消灭了同步，而是把同步点挪到了“更合理的位置”，让 GPU 可以在同步点之间持续工作。
3.2 为什么 async 能吃满 GPU：continuous batching + 见缝插针
agent_loop_details.ipynb 讲得很清楚：

客户端：asyncio 让 CPU 端“非阻塞发射请求”
服务端：vLLM/SGLang continuous batching 让 GPU 端“见缝插针执行请求”

如果你只做 async，不用 continuous batching，收益会明显缩水；
如果你只做 continuous batching，但 batch 同步推进多轮（sync），收益也会被 straggler 拖垮。
它们是组合拳。

4. Tool 调用的并发：asyncio.gather 很香，但要有节制
agent_loop_details.ipynb 提到“工具调用可以并行”，并给了一个骨架：
tasks = []for tool_call in agent_data.tool_calls[:self.max_parallel_calls]:    tasks.append(self._call_tool(tool_call, agent_data.tools_kwargs))responses = await asyncio.gather(*tasks)
这里我想补三点工程经验（视频里不一定强调，但你写系统时一定会遇到）：

并发上限必须有：max_parallel_calls 不是可选参数。没有上限，你会把外部 API 打爆，或者把本机 fd/线程池打爆。
超时与 fallback 必须统一：建议在 _call_tool 内统一做 timeout + exception mapping，把错误也变成 observation（mask=0）的一部分，让状态机继续推进或可控终止。
把 tool 的“结构化输出”与“写进 prompt 的字符串”分离：结构化输出用于 debug/metrics，字符串用于继续对话；不要把 JSON/debug 信息直接塞进 prompt（污染 token 分布）。


5. 计算细节 1：reward attach 不是小事，它决定了“你在优化什么”
很多同学会把 reward 当作一个标量塞进 batch，但在 token-level RL（LLM-RL）里，reward 最终会变成 token 级别的 advantage 信号。
你怎么 attach，会决定 credit assignment 的形态。
agent_loop_details.ipynb 给了一个很典型的做法（retool 相关）：

reward 先由 reward_manager 计算出来
_postprocess 把 reward 写到最后一个有效 token 的位置

它甚至给了一个非常具象的赋值逻辑（概念上）：

rm_scores[torch.arange(response_mask.size(0)), response_length] = scores

这类做法对应一种“稀疏回报”建模：

只有最后一个位置 reward 非零，前面全是 0
优势函数（GAE）会把这个信号向前传播，但传播形态依赖你的 value/critic 质量

5.1 tool reward shaping：鼓励工具调用到底在鼓励什么
agent_loop_details.ipynb 里还给了一个很“工程化”的 shaping：用 num_turns 给负分样本加一点补偿（鼓励尝试工具）：
tool_call_reward = (num_turns - 2) / 2 * 0.1result[&quot;score&quot;] = min(-0.6, result[&quot;score&quot;] + tool_call_reward)
我对这种 shaping 的态度是：可以用，但要把它当成“暂时的训练脚手架”，并且你必须监控两个副作用：

agent 可能学会“无意义地拉长对话/多调工具”来骗分
num_turns 与真实任务质量的相关性可能很弱（甚至负相关）

如果你的目标是 deep research agent，我更建议你把 shaping 做在：

tool 调用的有效性（是否提高证据覆盖、是否减少幻觉）
任务闭环指标（例如检索到的引用是否能支撑最终结论）

而不是 “turn 数越多越好”。

6. 计算细节 2：为什么 rollout 阶段通常不算 logprob
AgentLoopOutput 里有 response_logprobs，但很多框架（含 verl）会选择：

rollout 阶段只保 token ids + mask
training 阶段再用 actor/ref worker 重算 logprob（并在 update 前缓存 old_logprob）

原因很现实：

rollout 阶段的瓶颈是 GPU 推理与 tool latency，把 logprob 计算塞进去会让路径更复杂、吞吐更差。
PPO 需要的 old_logprob 必须对应“update 前”的策略；最干净的做法就是在 fit 的开头统一重算并冻结。

这也是为什么在你看 ray_trainer.py 时，会看到类似流程（上一期已经提到过）：
old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
当你把这个设计想清楚，你就会更敢于把 AgentLoop 写得“只关注轨迹”，把训练逻辑留给 Trainer。

7. Debug Checklist：一眼定位是状态机错、mask 错，还是 reward 口径错
我建议你最少打这些日志（没有这些，你就是在盲调）：

每条 trajectory：request_id / num_turns / response_len / mask_ones_ratio
每轮 GENERATING：生成 token 数、解析到的 tool_calls 数、是否终止
每轮 PROCESSING_TOOLS：tool 名称、耗时、成功/失败、返回 token 数
interaction：should_terminate、reward、反馈字符串长度
reward attach：reward 写入的位置 index（最后 token 还是 turn token？）

如果你只能加一个指标：加 mask_ones_ratio。
它能极快暴露“tool token 混入 action token”的灾难性 bug。

8. 小结：把 AgentLoop 写对，agentic RL 才有可能写对

AgentLoop 的价值不在于“能跑 multi-turn”，而在于它输出的轨迹能被严谨地用于 token-level RL。
response_mask 是契约，不是优化项：你只要把 observation token 误当 action token，训练目标就会被污染。
async rollout 是系统设计：三层并发（Ray + asyncio + continuous batching），两处同步点（ray.get + gather）。
tool 与 interaction 建议分状态处理：并发/超时/重试/终止规则会更清晰。

下一篇如果你继续给链接，我会优先写“排障与性能调优专题”：包括 tool 长尾、ray.get straggler、sticky session 缓存命中、以及 reward 前置与 training-phase 口径一致性怎么做。
]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>PPO</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>AgentLoop</tag>
        <tag>Async Rollout</tag>
        <tag>Tool Use</tag>
        <tag>Interaction</tag>
        <tag>Ray</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL FSDP SFT Trainer 补充（Teacher Forcing、Shift Labels/Logits、Loss Mask）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20FSDP%20SFT%20Trainer%20%E8%A1%A5%E5%85%85%EF%BC%88Teacher%20Forcing%E3%80%81Shift%20Labels-Logits%E3%80%81Loss%20Mask%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：“[veRL] fsdp sft trainer 补充，teacher forcing、shift labels shift logits、loss mask”（BV1eWjtzbEdP）。
它是上一篇 SFT trainer 文章的“补充篇”，专门把三个最容易写错、但一错就会把模型训歪的细节讲透：

Teacher forcing：SFT 到底在“喂什么”给模型，喂错会导致什么偏差。
Shift labels / shift logits：为什么 causal LM 的 CE loss 天生存在“错一位”，实现里你必须显式对齐。
Loss mask：multi-turn + tool-use 数据里，你到底要监督哪些 token；mask 在 shift 前后怎么对齐。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议先看主篇再看补充）：


veRL：FSDP SFT Trainer 主篇（交叉熵 &#x2F; loss mask &#x2F; scheduler）


veRL：MultiTurn Tool Use &#x2F; Coding Agent SFT（Cold Start for RL）


Tokenizer 非对称性与 Token-in-Token-out（RL 训练崩溃的根因）


配套仓库（你本地已下载）里，这篇最相关的材料：

multi-turn SFT 的 loss_mask 可视化与数据结构：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/ReTool-sft.ipynb


tokenization 一致性检查（不一致会让 mask 与 RL 都一起崩）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/tokenizer/encode-decode.ipynb


一个可跑的 SFT 启动脚本（FSDP2 + torchrun）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/scripts/run_qwen2_7b_sft.sh




1. Teacher forcing：SFT 在训练什么“条件分布”
“Teacher forcing” 在 LLM 的 SFT 里几乎是默认行为：训练时每一步都把 ground-truth token 当作历史喂给模型。
设一条序列是 $x_0,x_1,\dots,x_{T-1}$，causal LM 学的是：
$$\max_\theta\ \sum_{t=0}^{T-2}\log p_\theta(x_{t+1}\mid x_{\le t})$$
这意味着：

训练时在位置 $t$ 的条件是 $x_{\le t}$（全是“老师给的正确历史”）
但推理时在位置 $t$ 的条件变成了模型自己生成的 $\hat x_{\le t}$（历史里混入了“自己的错误”）

这个 train/test mismatch 通常被叫做 exposure bias。它不是一个“理论问题”，而是你在 agent 场景里马上会踩到的工程问题：

SFT 能把格式训得很稳（JSON/tool-call/引用结构），但只要历史里出现一次小错，后续格式就可能崩。
所以你后面做 RL（PPO/GRPO/RLOO 等）时，rollout 是“自回归采样”，会天然暴露并放大这些问题。

结论（工程视角）：

SFT 阶段必须把协议训稳（工具调用、引用、结构化输出）。否则 RL 的探索只会把漏洞学得更快。
RL 阶段不是替你发明协议，而是在已有轨迹附近做 reweighting（尤其还有 KL anchor 时）。


2. 为什么要 shift：CE loss 的“错一位”从哪来
causal LM 的 logits 是一个长度为 $T$ 的序列：logits[t] 表示“在看到 $x_{\le t}$ 后对下一个 token 的预测”。
因此 logits 的时间轴 与 labels 的时间轴天然错开 1：

logits[t] 用来预测 labels[t+1]

把它写成最常见的实现形式就是：

logits[:, :-1] 对齐 labels[:, 1:]

对应数学形式：
$$\mathcal{L}_\text{CE} = -\sum_{t=0}^{T-2}\log p_\theta(x_{t+1}\mid x_{\le t})$$
你在工程里看到的“shift logits / shift labels”，本质上就是把“预测 $x_{t+1}$ 的那一项”对齐到同一个 index 上去算交叉熵。
这件事看起来基础，但它和 loss_mask 一起时会出现非常典型的 off-by-one bug：

你以为你在训 assistant response
实际你训到 prompt（甚至训到 system 工具说明）
或者你把 response 的第一个 token 的监督丢掉（模型会学得很慢、且开头质量差）


3. Loss mask：你到底要监督哪些 token（以及 shift 前后如何对齐）
3.1 先约定一个最重要的语义：mask 标的是“哪些 label 要被预测”
我强烈建议你在工程里把 loss_mask 理解成：

loss_mask[t] = 1 表示 token x_t 这个 label 需要被预测，也就是它应该出现在 CE 的 target 里。

这样做的好处是：它天然和 “labels=-100 ignore_index” 等价，也能和 HuggingFace 的内置 shift 对齐。
如果你的 mask 标的是“哪些输入 token 参与条件”（也就是 prompt 部分），那它在 CE 上没有直接意义，必须再映射一次，极容易出错。
3.2 一个 4-token 的最小例子：用表格把错位讲明白
假设我们把 prompt 和 response 拼在同一条序列里：

prompt: P0 P1
response: R0 R1

input_ids（原始 token 序列）：



index
token




0
P0


1
P1


2
R0


3
R1



“只监督 response”的直觉是：我们希望模型学会生成 R0 R1，也就是：

labels[2]=R0、labels[3]=R1 参与 loss
labels[0], labels[1] 不参与

但是 CE 真正在算的是 logits[t] -&gt; labels[t+1]，所以 预测 R0 的那一项在 t=1（看到 P0 P1 后预测 R0）。
这就是为什么：

如果你手写 per-token loss matrix，mask 通常要跟着 shift：loss_mask_shifted = loss_mask[:, 1:]
如果你用 HF 内置 loss（传入 labels），那就把“不要监督的 label”设成 -100 即可，shift 交给框架

3.3 两种实现方式（推荐优先用 A）
A) 推荐：用 labels=-100 让模型内部 shift（最少坑）
核心是：

labels = input_ids.clone()
对不监督的位置设 -100（ignore_index）
把 labels 直接传给 model(...)，让模型内部做 shift

示例（关键逻辑，适配 HF 大多数 causal LM）：
input_ids = batch[&quot;input_ids&quot;]          # [B, T]loss_mask = batch[&quot;loss_mask&quot;].bool()   # [B, T] 1=supervise label tokenlabels = input_ids.clone()labels[~loss_mask] = -100out = model(input_ids=input_ids, labels=labels)loss = out.loss
你只要保证 loss_mask 的语义是“哪些 label token 要监督”，这套就稳定。
B) 手动实现：自己 shift + 自己做 masked CE（调试/定制时用）
当你要做 packed sequence、或者要拿到 token-level loss matrix 做复杂聚合（比如 veRL 里常见的 agg_loss），你可能需要手动实现。
要点：

logits = logits[:, :-1, :]
labels = labels[:, 1:]
loss_mask = loss_mask[:, 1:]

然后对齐算 CE 并按 mask 聚合：
logits = out.logits[:, :-1, :]              # [B, T-1, V]labels = input_ids[:, 1:]                   # [B, T-1]mask = loss_mask[:, 1:].float()             # [B, T-1]per_tok = torch.nn.functional.cross_entropy(    logits.reshape(-1, logits.size(-1)),    labels.reshape(-1),    reduction=&quot;none&quot;,).reshape_as(labels)                        # [B, T-1]loss = (per_tok * mask).sum() / (mask.sum() + 1e-8)
这段代码写错一个 1:，你就会得到“看起来还能训练，但就是不对劲”的模型。

4. multi-turn + tool-use：mask 到底该怎么标（以及为什么别训 tool output）
仓库里的 ReTool-sft.ipynb 给了一个我认为非常正确的工程习惯：把每条样本逐 token decode 并按 loss_mask 上色可视化。
它的默认策略可以概括成一句话：

system/user/tool 这三类 token 一般不参与 loss
只训练 assistant 的输出（包括你认为需要稳定生成的 tool-call 协议片段）

为什么强烈建议不要训 tool output？

tool output 本质是“外部世界事实”，它不应该被模型“生成”出来
一旦你监督了 tool output，模型会倾向于把它当作可自由编造的 continuation
到 RL 阶段，这会被 reward 漏洞放大（模型学会“编造工具返回”骗 verifier/judge）

如果你确实需要模型“复述工具结果”，更稳的做法是：

tool output 仍然作为输入条件（mask=0）
assistant 的摘要/解释作为输出监督（mask=1）


5. Debug Checklist：一眼抓住 off-by-one / mask 漂移
这份 checklist 的目标是：不用跑大训练，10 分钟内确认你没把 CE 训歪。

抽 20 条样本做 mask 可视化（推荐直接用 ReTool-sft.ipynb 的思路）
打印 masked_tokens / nonpad_tokens 的分布：

过低：有效监督太少，loss 会很抖且学习慢
过高：你可能在训 prompt/system/tool


做一次“强一致性测试”：

把同一条 multi-turn 数据用两种 tokenization 拼接方式编码
确认 token 与边界一致（参考 encode-decode.ipynb 的警告）


用一个极小 batch 过一遍 forward：

开启手动 shift 版（B）和 labels=-100 版（A）
对比两者 loss 数值是否在同一量级（不要求完全相等，但不应差一个数量级）




6. 为什么这篇补充对 agentic RL 很关键
你后面做 PPO/GRPO 时，核心计算都是 token-level logprob：

logp_new(a_t|s_t)、logp_old(a_t|s_t)、logp_ref(a_t|s_t)

这些 logprob 的“对齐方式”与这里的 shift 完全同构：你还是在拿某个位置的 logits 去对应某个位置的 action token。
所以你可以把这篇文章当成一个基本功定位：

SFT 阶段把 shift + mask 写对了
RL 阶段你计算 ratio/KL/entropy 时就不容易把统计量算错（也更容易 debug）

如果你愿意，我下一步可以把这套“shift + mask 对齐”进一步延伸到 RL 端：

logp 计算的对齐（prompt/response 的边界如何切）
response_mask 与 loss_mask 在 RL 里各自是什么语义
vLLM rollout 下 prompt_logprobs / token_logprobs 的坑点如何验证

]]></content>
      <categories>
        <category>工程实践</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>SFT</tag>
        <tag>FSDP</tag>
        <tag>Teacher Forcing</tag>
        <tag>Cross Entropy</tag>
        <tag>Loss Mask</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL FSDP SFT Trainer（SFT vs RL、交叉熵损失、Loss Mask、LR Scheduler）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20FSDP%20SFT%20Trainer%EF%BC%88SFT%20vs%20RL%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E3%80%81Loss%20Mask%E3%80%81LR%20Scheduler%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：“[veRL] FSDP SFT trainer，SFT vs. RL，交叉熵损失 | loss mask | learning rate scheduler”（BV1CkJgzAEAG）。
补充篇（更聚焦 teacher forcing / shift labels-logits / loss mask 对齐）：


veRL：FSDP SFT Trainer 补充（Teacher Forcing &#x2F; Shift &#x2F; Loss Mask）


进一步把 SFT 接到 tool-use agent 的 cold start（MultiTurn Tool Use / Coding Agent）：


veRL：MultiTurn Tool Use &#x2F; Coding Agent SFT（Cold Start for RL）


但我会把它写成一份“可落地的工程读物”，而不是视频逐句复刻。你看完应该能回答这些问题：

为什么做 agentic RL / RLHF 之前，SFT 反而是你最不该糊弄的一步？
causal LM 的交叉熵损失到底在算什么？loss_mask 和 labels=-100 到底是一回事吗？
multi-turn 数据里，哪些 token 应该参与 loss？如果 mask 搞错，会把模型训成什么鬼样？
FSDP SFT trainer 到底解决的是什么瓶颈（显存/吞吐/可扩展性）？FSDP2 又是什么？
learning rate scheduler 为什么在 SFT 阶段更关键（甚至比 RL 阶段更“可解释”）？

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


veRL 训练参数理解（PPO&#x2F;GRPO、Batch、KL、Entropy）


veRL 核心算法（GRPO&#x2F;RLOO&#x2F;REINFORCE++）与 Baseline


PG loss 组件详解（PPO-clip &#x2F; KL &#x2F; Entropy &#x2F; 聚合）


Tokenizer 非对称性与 Token-in-Token-out（RL 训练崩溃的根因）



0. 资料对齐（视频 + 本地仓库）
视频：

BV1CkJgzAEAG

https://www.bilibili.com/video/BV1CkJgzAEAG/



配套仓库（你本地已下载）里，这篇最相关的材料我建议按重要程度看这几份（不要求你都读完）：

multi-turn SFT 的 loss_mask 可视化与数据结构：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/ReTool-sft.ipynb


SFT 与长序列的显存瓶颈（micro batch / SP / remove padding 等）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/训练及调参经验/sft.ipynb


FSDP 与并行（DP/TP/PP、FSDP vs FSDP2）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/3D/DP_TP_PP.ipynb
/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/3D/fsdp_fsdp2.ipynb


一个真实可跑的 SFT 启动脚本（FSDP2 + torchrun）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/scripts/run_qwen2_7b_sft.sh



另外一个“很关键但经常被忽略”的坑点（会直接让 RL 不收敛，也会污染 SFT 的 mask 对齐）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/tokenizer/encode-decode.ipynb

里面强调了 encode(messages) != (prompt_ids ⊕ response_ids) 这类 token 不一致问题，在训练里是致命的。




1. SFT vs RL：不要把它理解成“先热身再上强度”
很多人把 pipeline 简化成一句话：先 SFT，再 RL。然后就自然得出一个误判：

SFT 只是 warmup，真正的提升靠 RL。

在 agentic RL（工具调用、多轮对话、长上下文）场景里，这个直觉经常是反的：

SFT 决定“动作空间”是否可学

tool call 的 JSON schema、引用格式、分段结构、思考模板，本质上都是一种“可执行协议”。
你协议都没训稳，RL 只能在一个错误的动作空间里瞎探索，最后学到的往往是“投机取巧地骗 judge/骗 verifier”。


RL 往往被 KL anchor 限制在 SFT 附近

你后面的 PPO/GRPO 训练几乎一定会用 KL 把策略约束在 ref（通常就是 SFT）附近。
这意味着：SFT 的上限直接限制 RL 的上限。


SFT 的 loss 可解释、可 debug，RL 的 reward 不一定

交叉熵的下降是“真的在拟合数据分布”。
reward 的上升可能是“骗指标”，你要花更多成本做验证。



所以一个更实用的心智模型是：

SFT：把“能做正确动作的先验”写进模型（格式、工具、引用、推理风格）。
RL：在这个先验附近做 reweighting，让某些轨迹更常出现（采样效率、稳定性、成本约束）。


2. 交叉熵损失：它到底在优化什么（以及为什么要 shift）
对 causal LM（自回归模型），我们给定输入序列 token：$x_0,x_1,\dots,x_{T-1}$，模型在位置 $t$ 预测下一个 token 的分布 $p_\theta(\cdot|x_{\le t})$。
标准的 token-level 负对数似然（也就是交叉熵）是：
$$\mathcal{L}_{\text{CE}} = -\sum_{t=0}^{T-2}\log p_\theta(x_{t+1}|x_{\le t})$$
工程实现里这就是你常见的“shift”：

logits[:, :-1] 对齐 labels[:, 1:]

为什么要强调这个？因为 loss_mask/labels=-100 也必须跟着 shift 对齐，否则你以为你在训 assistant，实际在训 user prompt。
2.1 loss_mask 与 labels=-100：本质上是一回事
你可以用两种等价方式做“只对某些 token 计算 loss”：

mask 版：先算出每个 token 的 loss matrix，再乘以 mask 做聚合。
ignore_index 版：把不参与训练的位置 label 设成 -100（PyTorch/HF 默认 ignore）。

如果把它写成一个统一公式（用 mask 表达更直观）：
$$\mathcal{L} = \frac{\sum_t m_t\cdot\left(-\log p_\theta(x_{t+1}|x_{\le t})\right)}{\sum_t m_t + \epsilon}$$
其中 $m_t\in{0,1}$ 表示“这个位置是否计入 loss”。
工程里我更推荐你在日志里同时打印两件事：

masked_token_count = sum(m_t)（否则你根本不知道有效 batch 有多大）
loss_agg_mode（你是 token-mean 还是 seq-mean）

因为这两项经常比“学习率大小”更决定训练动态。

3. loss mask：你在 multi-turn SFT 里到底该训哪些 token
3.1 最常见原则：只训 assistant 说的话
在 chat / tool-use 数据里，一条样本往往包含：

system：系统提示
user：用户输入
tool：工具返回（或者工具 schema）
assistant：模型应该输出的内容（可能包含 reasoning、tool_call、final answer）

ReTool-sft.ipynb 的总结非常实用：system/user/tool 都应该 mask 掉，只对 assistant 的输出计算 loss。
如果你没这么做，你很容易把模型训坏：

训练它复读 system prompt（线上看起来像“人格绑定”）
训练它复读 user 的提问（答非所问）
训练它去“生成工具输出”（这是最危险的：工具输出本来应该是外部世界的事实）

3.2 multi-turn 的难点：assistant 的“哪一段”算输出
tool-use 体系里 assistant 可能输出两类东西：

直接回答用户（final answer）
输出工具调用（&lt;tool_call&gt;&#123;...&#125;&lt;/tool_call&gt; 或 JSON）

要不要对 tool_call 计算 loss？
工程上我建议你按目标来：

如果你的 agent 需要稳定地产生可解析的 tool_call：tool_call 必须训（mask=1）。
如果 tool_call 会被上层 planner/DSL 强约束生成：可以考虑只训 final answer，把 tool_call 交给规则或 structured decoding。

但无论哪一种，system/user/tool output 一般都不应该训。
3.3 最可靠的做法：把 mask 可视化（别凭感觉）
ReTool-sft.ipynb 给了一个很好的工程习惯：把 loss_mask 逐 token decode 并染色打印出来，肉眼一眼就能看出你到底在训练哪里。
你不需要照抄 notebook 里的彩色控制符，核心逻辑就两步：

decode 每个 token
同步打印 mask=1 的位置（比如用 [] 包起来）

示例伪代码（核心思想，非 veRL 专用）：
def debug_print_mask(tokenizer, input_ids, loss_mask, max_tokens=400):    pairs = list(zip(input_ids, loss_mask))[:max_tokens]    out = []    for tid, m in pairs:        tok = tokenizer.decode([tid])        out.append(f\&quot;[&#123;tok&#125;]\&quot; if m == 1 else tok)    print(\&quot;\&quot;.join(out))    print(\&quot;masked_tokens:\&quot;, sum(loss_mask), \&quot;/\&quot;, len(loss_mask))
你应该把它当成 SFT 的“单元测试”。跑 3 条样本都对齐了，再开始跑大训练。

4. 一个容易被低估的坑：token 不一致会让 mask 和训练目标一起漂
encode-decode.ipynb 里有一句我强烈建议你记住：

encode(messages) != (prompt_ids ⊕ response_ids)

直觉上我们会以为：

把 messages 用 apply_chat_template 编码一次
等价于“每轮把 prompt 编码，再把 response 拼上去”

但在真实框架里，这两者可能不等价（模板中间插入的控制 token、空格处理、tool role 的拼接方式都可能导致差异）。
对 SFT，这会导致两类问题：

你以为 mask 在 assistant response 上，但 token 边界其实错位了
你以为你在拟合“最终消息序列分布”，实际拟合了一个不存在的拼接分布

对 RL（PPO/GRPO）更致命：它会让轨迹偏离策略分布，直接导致 ratio/KL 统计失真，训练不收敛。
工程建议（不一定要完全照 veRL，但思想要一致）：

训练用的 tokenization 路径必须和线上/rollout 一致（同一个 template，同一个拼接方式）。
只要涉及 multi-turn + tool，你就应该加入 “token-level 对齐检查”（mask 可视化只是第一步）。


5. FSDP SFT Trainer：它解决的不是“更快”，而是“能跑”
当你开始做长上下文（8k/16k/32k）+ 多轮 + 大模型，SFT 最先爆的往往不是算力，而是显存。
5.1 FSDP 的一句话解释：把“模型本体”切碎分到多卡上
在传统 DDP 下，每张卡都有一份完整模型参数、梯度、优化器状态。
FSDP（Fully Sharded Data Parallel）的核心是：把这些大头都 sharding 掉：

参数（weights）分片
梯度（grads）分片
优化器状态（optimizer states）分片

于是每张卡只存自己那一片。计算时需要哪一片，就在前向时 all-gather，反向时 reduce-scatter。
这就是你在很多笔记里看到的关键词：

all-gather / reduce-scatter

5.2 FSDP2（fully_shard）你可以先理解成“更细粒度、更少常驻副本”
如果你不想陷进实现细节，一个够用的工程直觉是：

FSDP1：以模块为单位 all-gather（FlatParameter），副本驻留窗口相对大
FSDP2：更细粒度按需 shard/reshard，副本驻留窗口更窄，可重叠机会更多

仓库里的 run_qwen2_7b_sft.sh 也直接把 model.strategy=fsdp2 当作默认推荐。
5.3 SFT 训练时，你真正要盯的是“micro batch 才是显存开关”
SFT 的工程规律非常朴素：

真正把你打爆显存的通常是 激活（activations）+ KV/attention 的中间量，而不是参数本身。

这也是为什么即便用了 FSDP，你仍然需要：

micro batch 小（data.micro_batch_size_per_gpu）
gradient accumulation 做大 effective batch
remove padding / packed sequence（减少无效 token）
必要时上 sequence parallel（SP）或 activation checkpointing

你可以把它和你前面那篇 veRL 参数文章联系起来：很多“看起来像训练参数”的东西，本质是在调显存与吞吐的 tradeoff。

6. learning rate scheduler：SFT 阶段反而更值得你认真设计
SFT 的 loss 是交叉熵，它的形态比 RL 的 reward 更“线性可解释”，所以 scheduler 的收益也更可预期。
6.1 一个够用的默认策略：warmup + cosine decay
如果你没有特别理由，SFT 我建议你先用：

warmup：1% - 3% 的总步数
cosine decay 到一个较小的 min_lr（比如 lr * 0.1 或更低）

直觉：

warmup 解决 early-stage 的梯度不稳定（尤其是长上下文、混合精度、FSDP 通信下）
cosine decay 让后期收敛更平滑，减少“后半程学坏”的风险

6.2 scheduler 的最大坑：你用的是 optimizer step 还是 micro step
有 gradient accumulation 时：

micro step（每次 forward/backward）不等于 optimizer step（真正更新参数）

scheduler 应该跟着 optimizer step 走，否则你的 lr 会被错误地 decay 得过快。
工程上最推荐你在日志里打印三件事：

global_step（optimizer step）
lr（真实学习率）
tokens_per_step（有效训练 token 数）

你一旦把“lr 曲线”和“有效 token 曲线”对齐，很多训练异常（loss spike/不收敛/学坏）会变得很好解释。

7. SFT 写在 agentic RL 计划里：我建议你怎么做
如果你的最终目标是 deep research / tool-use agentic RL，我会把 SFT 当成一个明确的阶段目标，而不是“先跑个 baseline”：

先用 SFT 把协议训稳：

tool schema、引用格式、输出结构（可解析、可验证）


然后再用 RL 优化你真正关心的指标：

引用正确率、事实一致性、覆盖度、成本/时延、pass@k 等



原因很现实：RL 不能替你把协议发明出来，RL 只会把已有轨迹的概率 reweight。协议都不稳定，RL 只会放大漏洞。

8. 一份能直接用的工程 checklist（建议你贴到 Notion 里）
SFT 跑之前：

抽样可视化 loss_mask（至少 20 条样本）
统计每条样本的 masked_token_ratio（太低说明你在训很少 token，训练会很慢且不稳定）
确认 tokenization 路径一致（尤其 multi-turn + tool）

SFT 跑起来之后（每 200-500 step 一次）：

看训练/验证 loss 是否同时下降（只看 train loss 容易被过拟合骗）
看输出样例是否“协议稳定”（工具调用是否可解析、引用是否仍存在）
看 lr/grad_norm/tokens_per_step 是否健康（scheduler + 有效 batch 闭环）

如果你愿意把这一步做扎实，后面的 RL（PPO/GRPO）会容易一个数量级。
]]></content>
      <categories>
        <category>工程实践</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>SFT</tag>
        <tag>FSDP</tag>
        <tag>Cross Entropy</tag>
        <tag>Loss Mask</tag>
        <tag>LR Scheduler</tag>
        <tag>工程实践</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL Infra AgentLoop 代码串讲（Multi-turn 推理与 Ray Trainer）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20Infra%20AgentLoop%20%E4%BB%A3%E7%A0%81%E4%B8%B2%E8%AE%B2%EF%BC%88Multi-turn%20%E6%8E%A8%E7%90%86%E4%B8%8E%20Ray%20Trainer%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] 13 verl infra AgentLoop 代码串讲，multi-turn 推理与 ray trainer】（BV1MizSBJEbi）。
但我不会把它写成“逐句视频笔记”。定位是：把 verl 的 AgentLoop 代码路径和 RayPPOTrainer 的训练主循环串起来，让你做到：

读代码不迷路：知道入口在哪、数据在哪流、每个模块的边界是什么。
能把 multi-turn 推理跑成一个稳定的服务：知道要保什么状态、哪些状态必须 sticky、哪些必须 mask。
能 debug：遇到卡死/吞吐低/奖励对不上/轨迹错位，知道该查哪层日志。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


前置建议（不然你会“看懂一堆类名，但不知道为什么要这么做”）：


12 先建立 AgentLoop 的架构直觉（async &#x2F; sticky &#x2F; hybrid &#x2F; mask）


多轮训练的底线：token-in-token-out


本文主要对齐你本地仓库的这些笔记（按阅读顺序）：

Ray 训练系统的 worker group / 资源池 / 单控制器数据流：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/basics/架构概念.ipynb
/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/verl-arch.ipynb


AgentLoop 的关键入口（能直接定位到 ray_trainer.py / agent_loop.py）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_code.ipynb


Ray 的 remote / ObjectRef / wait 的基本语义（理解为什么这里用 ray.get，以及可以怎么改）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/ray/remote.ipynb


multi-turn 的一个典型 config 入口（开启 async + tool agent loop）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/basics.ipynb



下一篇（延伸：把“全流程与计算细节”补齐）：


14 veRL AgentLoop 全流程与计算细节（async rollout &#x2F; 状态机 &#x2F; tool-interaction）



0. 先把“三层 remote”分清楚：Ray remote / 推理 RPC / Tool IO
很多人看 verl agent loop 会混淆三种“远程调用”，导致排障时乱跳层：

Ray remote（控制面）：worker.generate_sequences.remote(chunk)

这是“把一段 Python 函数/方法丢给 Ray 调度器执行”。返回的是 ObjectRef（未来值）。


推理引擎 RPC（数据面）：AsyncLLMServerManager -&gt; (vLLM/SGLang server)

这是“让 GPU 上的推理服务生成 token”。


工具 IO（外部世界）：search / sandbox / mcp / env simulator

这是“非确定延迟”的根源（straggler 的根源）。



把它们分开，你才知道：

吞吐低：先看推理服务 batch（vLLM/SGLang），再看工具长尾，再看 Ray 调度。
卡死：先看 tool 超时/死锁，再看 Ray actor 崩溃重启，再看推理 server backlog。
训练不稳定：先看 token-in-token-out、mask、reward 口径；最后才怀疑 PPO loss。


1. RayPPOTrainer 的系统观：单控制器 + 多 WorkerGroup
verl 的 Ray 架构核心是：一个 Trainer（单控制器）串起一条“数据流图”，把重活下发给多个 worker group（多控制器并行）。
架构概念.ipynb 给了一个最关键的骨架（我保留原结构，便于你对照源码思路）：
global_pool_id = &quot;global_pool&quot;resource_pool_spec = &#123;    global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,&#125;self.mapping[Role.ActorRollout] = global_pool_idself.mapping[Role.Critic] = global_pool_idself.mapping[Role.RewardModel] = &quot;global_pool&quot;self.mapping[Role.RefPolicy] = &quot;global_pool&quot;resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=self.mapping)self.role_worker_mapping[Role.ActorRollout] = ray.remote(actor_rollout_cls)self.role_worker_mapping[Role.Critic] = ray.remote(CriticWorker)self.role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)self.role_worker_mapping[Role.RefPolicy] = ray.remote(ref_policy_cls)trainer = RayPPOTrainer(config,                        role_worker_mapping=self.role_worker_mapping,                        resource_pool_manager=resource_pool_manager, ...)trainer.init_workers()trainer.fit()
1.1 你应该形成的 mental model
你可以把它理解成下面这个“资源与角色图”（示意）：
graph TD  T[RayPPOTrainer (single-controller)] --&gt; WG_AR[actor_rollout_wg]  T --&gt; WG_REF[ref_policy_wg]  T --&gt; WG_C[critic_wg]  T --&gt; WG_RM[reward_model_wg]  subgraph GlobalPool[resource pool: global_pool]    WG_AR    WG_REF    WG_C  end  subgraph RMPool[resource pool: reward_pool (optional)]    WG_RM  end
关键点：Trainer 负责“顺序编排”（dataflow），每个 WG 内部再并行（SPMD/DP/TP/PP）。
这就是 架构概念.ipynb 里说的 single-controller vs multi-controller。

2. Training Step：Ray trainer 的“同步序列”长什么样
verl-arch.ipynb 把一个典型训练 step 的顺序写得很清楚（这是你读 ray_trainer.py 的导航图）：

generate_sequences()     → rollout 数据生成
compute_rm_score()       → RM / verifier 给分
compute_log_prob()       → actor 重算 logprob（用于 ratio / KL）
compute_ref_log_prob()   → ref policy logprob（用于 KL）
compute_values()         → critic values（用于 advantage）
update_critic()          → critic 更新
update_actor()           → actor 更新

当你引入 AgentLoop + async rollout 后，这条链路会发生两个“结构性变化”：

generate_sequences() 不再只是 “LLM 单轮生成”：它变成“多轮交错轨迹生成（LLM + tool）”，输出必须带 response_mask。
compute_rm_score() 可能被前置进 rollout 阶段：尤其是 async 模式下（上一期文章已解释）。

所以，第 13 期代码串讲的重点，其实是：

这条训练流水线里，AgentLoop 插在哪，插进去后哪些步骤的“输入输出契约”变了？


3. 真正的分水岭：rollout.mode=async 触发的代码路径
agent_loop_code.ipynb 里把关键分支点点出来了：

ray_trainer.py:init_workers

如果 self.config.actor_rollout_ref.rollout.mode == &quot;async&quot;：
实例化 self.async_rollout_manager = AgentLoopManager(...)


ray_trainer.py:fit

gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch_output)


ray_trainer.py:_validate

self.async_rollout_manager.generate_sequences(test_gen_batch_padded)



我建议你把它画成一个“入口级 call graph”，以后你 debug 就按这个树往下钻：
graph TD  A[RayPPOTrainer.fit] --&gt; B&#123;rollout.mode == async?&#125;  B -- No --&gt; C[actor_rollout_wg.generate_sequences]  B -- Yes --&gt; D[AgentLoopManager.generate_sequences]  D --&gt; E[AgentLoopWorker.generate_sequences (Ray remote)]  E --&gt; F[AgentLoop.run (async state machine)]  F --&gt; G[AsyncLLMServerManager.generate (sticky + lb)]  G --&gt; H[vLLM/SGLang server]  F --&gt; I[Tools / Env / Simulator]  E --&gt; J[postprocess: mask / reward attach / metrics]  J --&gt; D  D --&gt; A
注意：这张图里有两个“并行域”：

Ray 并行：AgentLoopManager 把 batch chunk 到多个 Worker（ray.get([...remote...])）。
Worker 内并行：单个 Worker 用 asyncio 并发跑很多条 agent loop（上一期讲过）。


4. AgentLoopManager.generate_sequences：你真正需要读懂的 40 行代码
agent_loop_code.ipynb 直接给了 AgentLoopManager.generate_sequences 的实现骨架（这段代码是你读 infra 的关键锚点）：
def generate_sequences(self, prompts: DataProto) -&gt; DataProto:    if self.config.actor_rollout_ref.rollout.free_cache_engine:        self.wake_up()    if self.reward_model_manager and self.config.reward_model.rollout.free_cache_engine:        self.reward_model_manager.wake_up()    chunkes = prompts.chunk(len(self.agent_loop_workers))    outputs = ray.get(        [            worker.generate_sequences.remote(chunk)            for worker, chunk in zip(self.agent_loop_workers, chunkes, strict=True)        ]    )    output = DataProto.concat(outputs)    if self.config.actor_rollout_ref.rollout.free_cache_engine:        self.sleep()    if self.reward_model_manager and self.config.reward_model.rollout.free_cache_engine:        self.reward_model_manager.sleep()    metrics = [output.meta_info.pop(&quot;metrics&quot;) for output in outputs]    timing = self._performance_metrics(metrics, output)    output.meta_info = &#123;&quot;timing&quot;: timing, **outputs[0].meta_info&#125;    return output
4.1 这段代码背后的“系统假设”

rollout 与 train 分阶段：所以有 wake_up/sleep（hybrid 才需要，standalone 可以关）。
DataProto 是第一公民：chunk/concat 都发生在 DataProto 上，而不是 list[dict]。
Worker 是无共享状态的并行单元：batch 级同步点在 ray.get，拿到全部 outputs 才 concat。
metrics 是跨层传播的唯一办法：子 worker 负责采样/工具/推理耗时；manager 汇总成 timing。

你在扩展 agent loop 时，千万别破坏这几个假设，否则代码会从“好 debug”变成“玄学系统”。
4.2 我对这段实现的一个批判：ray.get 会放大长尾
它等价于“等最慢那个 worker 结束”。
如果你的 tool latency 长尾很重，你会看到训练吞吐被 straggler 拖下去。
更激进的设计是用 ray.wait 边完成边消费（remote.ipynb 里就演示了 wait 模式的写法）。
但这会改变 DataProto concat 的节奏，进而影响：

训练 step 的同步边界
reward/advantage 的 batch 统计（例如优势标准化）

所以我建议你先接受 ray.get 的简单性，把 correctness 跑稳；真要榨吞吐，再引入 ray.wait 的“流式合并”。

5. DataProto：为什么它是 AgentLoop 和 Ray trainer 的“共同语言”
从代码可以看出，AgentLoopManager 并不关心你做的是 search agent 还是 coding agent，它只认一件事：

输入是一批 prompts（DataProto），输出是一批 trajectories（DataProto）。

这就是 verl 设计的核心优点：把 RLHF/Agentic RL 的复杂性压缩成“Dataflow + Contracts”。
在 AgentLoop 场景，你至少要保证输出里有这些字段（概念上）：

prompt_ids
response_ids（包含 action + observation 的交错 token）
response_mask（action token=1，observation token=0）
meta_info（timing / trajectory_id / server_id / turns / tool stats）

如果你后面要接 PPO/GRPO 的 loss 计算，最关键的一条就是：

任何会参与 logprob / ratio / KL 的 token，都必须是 token-in-token-out 的 action token。

这也是我为什么强烈建议你把 response_mask 当作“训练数据 schema 的一部分”，而不是某个临时 hack。

6. Multi-turn 推理：把 AgentLoop 当成“可观测、可回放”的推理服务
agent_loop_inference.ipynb 说得很直白：纯推理模式下，agent loop 就是 ReAct，一堆框架都能做。
但如果你面向的是 agentic RL（未来要训练），你在推理阶段就该按“可训练”的标准建系统：

可观测：你必须打出 trajectory_id、turn_id、tool latency、server_id（sticky 结果）。
可回放：同一套 prompt + tool output，在 deterministic 条件下应该能重放出同样的 token ids（否则训练阶段你永远对不上 logprob）。
可切换：standalone 模式先跑通；hybrid 是后置优化项（别把系统复杂度绑死）。

你可以把 multi-turn 推理的“标准姿势”理解成：

用 AgentLoop 管对话历史、tool 调用、终止条件（逻辑层）
用 AsyncLLMServerManager 做 sticky + LB（服务层）
用 vLLM/SGLang 做 continuous batching（计算层）

如果你只用 LangChain/LangGraph 跑推理，不做这几件事，短期能跑通，长期很难演进到“可训练”。

7. 最常见的 8 个坑（按出现频率排序）

mask 错了：tool 输出 token 被当成 action token → logprob 污染 → 训练直接发散。
token-in-token-out 被破坏：chat template / tokenizer encode/decode 不一致 → 轨迹对不上。
sticky 没做好：多轮路由到不同 server → prefix cache 失效 + 状态漂移。
reward 口径不一致：rollout 前置打分 vs training phase 打分 → 同一个 batch 分数不一致。
hybrid 切换开销吞掉收益：wake_up/sleep 太重，反而比 standalone 慢。
Ray 资源池映射错：actor_rollout/ref/critic/rm 抢同一组 GPU，或 placement 不合理 → 看起来“卡死”。
工具长尾拖垮 ray.get：少数样本超时 → 整批等死。
metrics 缺失导致盲调：没有 per-turn latency / per-tool stats，只能看最终 reward 曲线瞎猜。


8. 我建议你怎么学这套东西（很实际）
如果你的目标是“做 agentic RL 来做 deep research”，你不需要一上来就把 verl 全吃透。建议按 3 阶段：

跑通 multi-turn 推理（standalone）：把 trajectory schema、mask、sticky、日志打通。
接上最小 RL 闭环：哪怕 reward 是规则/verifier，先保证能稳定更新（别追 SOTA）。
再谈 infra 优化：hybrid、ray.wait 流式、reward 前置、吞吐调优。

学的顺序对了，你会发现“读代码”不再是背类名，而是在验证你对数据契约的理解。

9. 小结

rollout.mode=async 不是一个小开关，它改变的是：rollout 产出的数据形态（交错轨迹 + mask），以及 trainer 的流水线边界。
读 AgentLoopManager.generate_sequences 的关键不在于“Ray 怎么写”，而在于：它把整个系统的同步点固定在 DataProto concat 这一刻。
multi-turn 推理要按“未来可训练”的标准去设计：可观测、可回放、可切换；否则你会在训练阶段被历史债务反噬。

下一篇（如果你希望我继续按这个流程写）我会更偏“实战排障”：针对 Ray actor 崩溃、tool 超时、reward 延迟、吞吐打不满这些问题，给出更具体的日志字段与定位路径。
]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>vLLM</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>AgentLoop</tag>
        <tag>Ray</tag>
        <tag>Trainer</tag>
        <tag>Inference</tag>
        <tag>SGLang</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL Infra AgentLoop（AgentLoopManager、Async Rollout 与 Hybrid 推训）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20Infra%20AgentLoop%EF%BC%88AgentLoopManager%E3%80%81Async%20Rollout%20%E4%B8%8E%20Hybrid%20%E6%8E%A8%E8%AE%AD%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] 12 verl infra AgentLoop 基本概念及流程，AgentLoopManager，hybrid训练与推理】（BV135zrBaEEU）。
如果你已经看完我在上一篇里写的 “Agent Loop 为什么需要 async rollout” 与 “response_mask 基本概念”，那么这篇就是 infra 深挖版：把 verl 的 AgentLoop 体系从“能用”讲到“你能改、能调、能排障”。
你看完应该能回答这些工程问题：

AgentLoopManager / Worker / AgentLoop / AsyncLLMServerManager 各自负责什么，边界怎么划？
为什么 async rollout 不是优化项，而是 multi-turn tool use 的必要条件？它和 vLLM 的 continuous batching 怎么配合？
sticky session 为什么必须有？它和 prefix cache、load balancing 是什么关系？
“hybrid 推训”到底在复用什么资源？wake_up / sleep 在做什么？什么时候应该直接用 standalone 部署？

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


先建立 AgentLoop 的基本直觉（async + 状态机 + mask）


token-in-token-out：为什么多轮对话里 encode&#x2F;decode 不一致会直接把 RL 训崩


理解 KV cache 与吞吐瓶颈：你才知道 hybrid 在省什么


下一篇：把 AgentLoop 和 Ray trainer 的代码路径串起来


配套仓库（你本地已下载）里，本文主要对齐这些笔记（按推荐顺序）：

架构鸟瞰（四层抽象 + hybrid wake_up/sleep）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_arch.ipynb


细节拆解（async vs sync、状态机、sticky session、output 结构、reward 前置）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_details.ipynb


代码入口（指向 verl 源码的 agent_loop.py）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_code.ipynb


GPU/Replica 配置（num_replicas vs engine internal DP）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_config.ipynb


推理模式（inference-only agent loop）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_inference.ipynb




0. 一句话定位：AgentLoop 是“可训练的 LangChain”，区别在于你必须保留 token 级轨迹
很多人第一次看 verl 的 agent loop，会觉得：

这不就是把 LangChain/LangGraph 的 ReAct 流程编排搬进来了吗？

某种意义上没错（agent_loop_inference.ipynb 也明确说推理模式下就是 react）。但 RL 训练让它变得更“硬核”：

你不仅要“跑通流程”，还要产出可用于优化的轨迹数据：

token ids
logp 对齐所需的信息（以及至少能重算）
response_mask（区分 action token vs observation token）
turn/reward 的挂载位置与口径


你必须在 infra 层面解决：

工具 IO 的长尾延迟（straggler）
多轮请求的路由一致性（sticky session）
推理与训练对同一组 GPU 显存的争抢（hybrid）



所以这篇文章会更像“系统设计与排障手册”，而不是算法推导。

1. 四层抽象：谁负责“怎么聊”，谁负责“怎么调度”，谁负责“怎么生成”
verl 的一个很工程化的拆分是：把 “Agent” 这件事拆成 4 层，每一层只负责一件事（agent_loop_arch.ipynb）：

AgentLoop：只关心 逻辑（怎么多轮对话、怎么决定调用工具、怎么终止）。
AgentLoopWorker：只关心 并发与编排（asyncio 并发跑很多条 loop，把结果合并）。
AgentLoopManager：只关心 分发与资源切换（Ray 分发 chunk、hybrid wake_up/sleep、汇总）。
AsyncLLMServerManager：只关心 推理服务网关（负载均衡、sticky session、RPC）。

一张图就够（来自 notebook，我稍微改了命名）：
graph TD    A[AgentLoopManager] --&gt;|1. 切分 Batch &amp; 分发| B(AgentLoopWorker)    B --&gt;|2. 实例化 &amp; 调用 run| C&#123;AgentLoop &lt;br&gt; e.g. ToolAgentLoop&#125;    C --&gt;|3. 请求生成| D[AsyncLLMServerManager]    D --&gt;|4. RPC 调用| E[Remote vLLM / SGLang Engine]    C --&gt;|5. 执行工具| F[Tools / Environment]
这个拆分真正的价值在于：你改动某个维度时，不会“牵一发动全身”。

想换工具协议/状态机：改 AgentLoop。
想调并发粒度：改 Worker（或 worker 数）。
想换推理引擎：改 ServerManager（vLLM / SGLang / OpenAI compatible server）。
想做推训资源复用：改 Manager 的 wake_up/sleep 策略。


2. AgentLoopManager：从 Trainer 接单，到 hybrid wake_up/sleep 的总控
从 agent_loop_code.ipynb 给出的入口看，训练主控（例如 RayPPOTrainer）在需要新 rollout 时会调用：

AgentLoopManager.generate_sequences(prompts=batch)

它的核心职责可以总结为 3 件事：

拆 batch：把一个大 batch 切成多个 chunk。
分发到 Worker：Ray remote 调 worker.generate_sequences.remote(chunk)。
资源切换（可选）：如果启用了 hybrid/free_cache_engine，就在生成前后做：

wake_up()：让推理引擎准备好（权重同步/显存准备/prefix cache 重置等）
sleep()：让推理引擎释放 KV cache，给训练阶段让路



一个你在工程里一定会遇到的点：

hybrid 不是“永远更快”，它是显存复用的节省方案。
你的瓶颈如果在网络 IO、tool latency、或者 reward 计算，那 hybrid 可能不会带来收益，反而增加 wake_up/sleep 的切换开销。

我的建议是：

小规模先跑 standalone（推理和训练分不同 GPU），把正确性与指标闭环打通。
再考虑 hybrid，把 GPU 利用率榨到极限。


3. AgentLoopWorker：CPU 节点上的并发器，避免 straggler 拖垮整批
agent_loop_details.ipynb 一开始就强调：

Worker 被调度到 CPU 节点上运行；
大量的工具调用与环境交互在 CPU 上异步并发；
GPU 侧只负责“生成”，通过 server manager 统一调用。

核心实现范式是：

每个 sample 一个 async task
await asyncio.gather(*tasks) 等全部完成后在 Worker 内做一次同步（merge）

这给了你一个很清晰的“同步边界”：

rollout 阶段尽可能异步；update 阶段必须同步。

这个同步边界一旦画清楚，你很多设计会自然变对：

工具输出永远是 observation（mask=0）。
只有 LLM 生成 token 是 action（mask=1，参与梯度）。
reward 如果要前置，应该在 “merge 前”完成，把分数挂载到轨迹里。


4. AgentLoop：状态机是 tool-use agent 的通用接口
对 tool-use agent，verl 的实现思路是“显式状态机”（在 agent_loop_details.ipynb 给了一个很标准的 5 状态版本）：
stateDiagram-v2    [*] --&gt; PENDING: Start    state &quot;PENDING (初始化)&quot; as PENDING    state &quot;GENERATING (模型生成)&quot; as GENERATING    state &quot;PROCESSING_TOOLS (执行工具)&quot; as PROCESSING_TOOLS    state &quot;INTERACTING (环境/用户交互)&quot; as INTERACTING    state &quot;TERMINATED (结束)&quot; as TERMINATED    PENDING --&gt; GENERATING : 准备好 Prompt    GENERATING --&gt; PROCESSING_TOOLS : 模型决定调用工具    GENERATING --&gt; INTERACTING : 无工具调用 &amp; 配置了交互环境    GENERATING --&gt; TERMINATED : 完成任务 / 达到最大轮数 / 达到长度限制    PROCESSING_TOOLS --&gt; GENERATING : 工具执行完毕 (更新上下文)    PROCESSING_TOOLS --&gt; TERMINATED : 上下文超长    INTERACTING --&gt; GENERATING : 收到外部反馈 (更新上下文)    INTERACTING --&gt; TERMINATED : 交互结束 (should_terminate=True)    TERMINATED --&gt; [*]: End
这套状态机的工程意义是：

你后续想加 “planner / memory / tool router / user simulator”，都能自然插到对应状态里。
你想做并发限制（比如 max_parallel_calls），也能在 PROCESSING_TOOLS 里做得很干净。

同时它强迫你把 “action vs observation” 分清楚：

GENERATING 输出的是 action token（参与梯度）。
PROCESSING_TOOLS / INTERACTING 输出的是 observation（不参与梯度）。


5. async 为什么不是优化项：Sync 训练会把 agent 绑成“旅行团”
如果你用 batch 同步推进 multi-turn，会发生一种非常典型的“木桶效应”（notebook 把它讲得很形象）：

同一个 batch 里，只要有一个 agent 慢（生成长 / tool 慢 / 网络慢），整个 batch 就卡住；
更糟的是：LLM 生成与 tool 执行是交替的，sync 模式会让快 agent 也必须等慢 agent 才能进入下一轮生成。

对比一下伪代码（摘自 agent_loop_details.ipynb）：
# Sync: 强制绑定的“旅行团”outputs_round_1 = llm.generate(prompts)         # 卡点 A：等所有人生成完observations = run_tools(outputs_round_1)       # 卡点 B：等所有人跑完工具prompts_round_2 = update_history(...)outputs_round_2 = llm.generate(prompts_round_2) # 卡点 C：下一轮也得等
# Async: 各跑各的“自由行”async def run_agent_loop(agent_id, prompt):    while not done:        response = await llm_server.generate(prompt)        observation = await run_tool(response)        prompt = update_history(prompt, response, observation)
真正关键的一句是：

await 只挂起当前 agent，不挂起别人。

于是 “快任务” 可以不断发起下一轮生成请求，vLLM/SGLang 的 continuous batching 就能持续吃满 GPU。

6. AsyncLLMServerManager：负载均衡 + Sticky Session（多轮必需）
6.1 为什么必须 sticky：多轮对话需要 prefix cache 命中
如果多轮请求被随机路由到不同 server：

prefix cache 命不中（每轮都当新 prompt），吞吐会掉；
更糟的是：不同 server 的 tokenizer/chat template/状态如果不完全一致，你会把 token-in-token-out 彻底破坏。

所以 verl 用了一个很直接的设计：

用 LRU 维护 request_id -&gt; server 映射（同一 trajectory 固定路由）
用最小堆维护 server 的“当前负载”（粗粒度的请求计数）

agent_loop_details.ipynb 给出了一个非常直观的代码骨架（我原样贴出来，便于你对齐实现思路）：
class AsyncLLMServerManager:    def __init__(self, config, server_handles, max_cache_size=10000):        self.server_handles = server_handles  # Ray actor handles        # min-heap for load balancing        self.weighted_serveres = [[0, (hash(server), server)] for server in server_handles]        heapq.heapify(self.weighted_serveres)        # sticky routing: request_id -&gt; server        self.request_id_to_server = LRUCache(maxsize=max_cache_size)    def _choose_server(self, request_id: str):        if request_id in self.request_id_to_server:            return self.request_id_to_server[request_id]        server = self.weighted_serveres[0][1][1]        self.weighted_serveres[0][0] += 1        heapq.heapreplace(self.weighted_serveres, self.weighted_serveres[0])        self.request_id_to_server[request_id] = server        return server
6.2 你一定要想清楚：request_id 的定义与生命周期
sticky 的关键在 request_id：

必须是 “同一 trajectory 跨轮一致” 的 id；
且要避免不同样本冲突（否则会错误粘连）；
还要考虑 LRU 淘汰后继续请求会被重新分配 server（notebook 也提示了这一点）。

工程上我更建议你：

用 trajectory_id（而不是 “当前 turn id”）作为 sticky key；
在日志里把 trajectory_id -&gt; server_id 打出来（排障时非常关键）。


7. AgentLoopOutput：交错轨迹（interleaved trajectory）必须显式带 mask
agent loop 的输出不是“纯 LLM token 序列”，而是交错的：

LLM 生成 token（action）
tool 返回 token（observation）
LLM 继续生成 token（action）

所以输出结构里至少要包含：

response_token_ids：包含 LLM 与 tool 的所有 token
response_mask：LLM token=1，tool token=0
num_turns：多少轮 user/assistant/tool（用于 shaping 或统计）

这件事在 PPO/GRPO 里会直接影响梯度：

你只能对 mask=1 的 token 计算 logp、ratio、KL、entropy；
tool 输出如果混进去，会污染分布，导致训练发散（这也是 token-in-token-out 那篇强调的点）。


8. Reward 前置：async rollout 下把 RM 打分放进 rollout 阶段
agent_loop_details.ipynb 给了一个非常重要的时序图：在 async 模式下，reward 计算被前置到了 rollout 阶段内部完成，Trainer 在 training phase 里可以直接跳过 compute_rm_score。
sequenceDiagram    participant Trainer as RayPPOTrainer    participant Manager as AgentLoopManager    participant Worker as AgentLoopWorker    participant RM as RewardManagerWorker    Trainer-&gt;&gt;Manager: generate_sequences()    Manager-&gt;&gt;Worker: distribute tasks    rect rgb(240, 248, 255)        Note over Worker: Async Rollout Phase        Worker-&gt;&gt;Worker: agent_loop.run()        Note over Worker: Reward Calculation Trigger        Worker-&gt;&gt;RM: compute_score(Full Trajectory)        RM--&gt;&gt;Worker: return reward_score        Worker-&gt;&gt;Worker: _postprocess() (Attach score to last token)    end    Worker--&gt;&gt;Manager: Return Batch (with rm_scores)    Manager--&gt;&gt;Trainer: Return Batch
这一步的好处是：

rollout 阶段就能把 “轨迹 + reward” 封装完整；
training phase 可以更纯粹地做 advantage / loss / update；
并且在多 worker 并发下，reward 的吞吐也更容易 scale。

但它也带来一个新挑战：你要更清楚 reward 的挂载口径（挂最后 token？按 turn 分配？是否加 tool shaping？）。

9. Hybrid 推训：wake_up / sleep 到底在干嘛
agent_loop_arch.ipynb 把 hybrid 的动机说得很直白：

显存宝贵；
训练（FSDP/ZeRO）和推理（vLLM/TP）对显存的占用形态完全不同；
如果推理引擎一直常驻，会吃掉大量 KV cache，训练可能直接 OOM；
所以需要在 “rollout 阶段” 与 “update 阶段” 之间切换推理引擎状态。

在 verl 里，这个机制通常通过：

wake_up()：推理引擎准备好（可能包含权重同步 / cache reset / 显存准备）
sleep()：推理引擎释放 KV cache 等显存占用

并且在 standalone 模式下可以跳过（推理和训练不在同一组 GPU）。
工程建议（经验，不是教条）：

你在单机/小集群：先 standalone（降低复杂度）。
你在多机大集群：hybrid 才值得（否则 GPU 利用率很难拉满）。


10. GPU/Replica 配置：num_replicas vs 引擎内部 DP
agent_loop_config.ipynb 给了一个很实用的公式：

num_replicas = total_gpus / (TP × DP × PP)

这里有一个很容易混的点（notebook 也专门对比了）：

num_replicas：服务级 replica（多个完全独立的推理服务），由 AsyncLLMServerManager 做负载均衡，松耦合。
data_parallel_size：引擎内部 DP（一个推理服务实例内部的并行），由 vLLM/SGLang 自己处理。

你在集群扩展与容错上，往往更依赖 num_replicas；
你在单实例吞吐上，才更依赖引擎内部并行（TP/PP/DP）。
一个典型例子（notebook 的例子）：

16 GPU 集群，模型推理需要 4 卡 TP：

rollout_world_size=4
num_replicas=16/4=4（4 个独立 server）




11. 排障 Checklist：你要记录哪些日志，才能真的 debug AgentLoop
我建议最少记录这几类日志（否则你会在“看起来卡住了”时毫无头绪）：

轨迹级路由

trajectory_id
server_id（sticky 选择结果）
LRU 是否命中/被淘汰


异步并发

每个 sample 的 llm_latency / tool_latency / turns / token_count
max_parallel_calls 触发次数


输出口径

response_mask 中 1/0 的比例
tool 输出 token 长度分布（防 OOM）


hybrid 切换

wake_up/sleep 耗时
权重同步耗时（如果有）
KV cache 释放前后显存占用（对齐你 vLLM 那篇的显存分析）




12. 小结

AgentLoop 是 agentic RL 的“数据生成器”：它要产出可训练轨迹，而不是只跑通流程。
async rollout 是 multi-turn tool use 的必要条件：它把 straggler 从系统瓶颈里移走，并让 continuous batching 真正发挥作用。
sticky session 是多轮必需品：它连接 prefix cache、token-in-token-out 与推理吞吐。
hybrid（wake_up/sleep）是显存复用策略：先把 correctness 与指标闭环跑通，再考虑复杂度更高的 hybrid。

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>PPO</tag>
        <tag>vLLM</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>AgentLoop</tag>
        <tag>Tool Use</tag>
        <tag>Ray</tag>
        <tag>SGLang</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL MultiTurn Tool Use 与 Coding Agent SFT（Cold Start for RL）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20MultiTurn%20Tool%20Use%20%E4%B8%8E%20Coding%20Agent%20SFT%EF%BC%88Cold%20Start%20for%20RL%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] [VeRL] 08 MultiTurn Tool Use, Coding Agent SFT 训练，Cold Start for RL】（BV19PmwB5ERd）。
但我不会把它写成“逐句复盘”的笔记，而是把它抽象成一套你能复用的工程方法论：如果你要做一个 multi-turn tool-use 的 coding agent，并且希望后续能接上 PPO/GRPO/DAPO 这类 RL 训练，你在 数据、mask、rollout 结构、reward 形态 上要先把哪些坑填平。
系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


SFT trainer 主篇：交叉熵 &#x2F; loss mask &#x2F; scheduler


SFT 补充：teacher forcing &#x2F; shift &#x2F; mask 对齐


RL 算法侧：GRPO&#x2F;RLOO&#x2F;REINFORCE++ 与 baseline


推理侧：vLLM 参数、显存&#x2F;吞吐调优


Tokenizer 非对称性与 Token-in-Token-out（RL 训练崩溃的根因）


配套仓库（你本地已下载）里，这篇最相关的材料（我会在文中引用这些路径）：

ReTool 的整体 pipeline（cold-start SFT + RL + interleaved rollout）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/ReTool-overall.ipynb


Multi-turn SFT 数据与 loss_mask 可视化：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/retool/ReTool-sft.ipynb


Agent loop 的架构与异步 rollout：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_arch.ipynb
/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_details.ipynb
/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_code.ipynb
/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/agent/agent_loop_config.ipynb


reward shaping（在 tool-use 场景里，经常是 cold start 成败的关键）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reward_model/reward-shaping.ipynb




0. 为什么 MultiTurn Tool Use + Coding Agent 比“普通 RLHF”难一个数量级
把“对话 SFT + RL”直接搬到 tool-use agent 上，最常见的失败模式不是“跑不起来”，而是“能跑但学不到”。
原因是 tool-use agent 的交互过程具备这些特征：

轨迹是 interleaved 的：LLM 生成 token → 调工具 → 工具返回（非策略生成）→ LLM 继续生成。
你需要两套 mask：

SFT：loss_mask（哪些 token 参与交叉熵）。
RL：response_mask / action_mask（哪些 token 是策略动作，参与 logprob、ratio、KL、entropy 等计算）。


reward 延迟且稀疏：coding agent 常见 reward 是“测试是否通过 / patch 是否正确”，中间步骤 reward 很弱。
环境噪声很大：

工具可能失败（网络/超时/权限/依赖缺失）。
代码执行的 stderr/stdout 会引入大量 token，污染序列长度与训练稳定性。



一个实用结论是：

“tool-use RL”不是在 PPO/GRPO 上加一个工具就行了，你需要先把 trajectory 的可训练部分定义清楚。

后面这篇文章会围绕这件事展开。

1. ReTool Pipeline 的核心：Cold-Start SFT → RL（带工具）→ Interleaved Rollout
ReTool-overall.ipynb 把训练 pipeline 拆得很清楚，我建议你把它当成一个“工程模板”：

Phase 1: Cold-Start SFT
Phase 2: RL with Tool Integration
Interleaved Rollout / Generation（LLM token 与 tool output 交错）

我用一张图把它画成闭环（不是论文图，是工程视角的“你会踩坑在哪里”图）：
flowchart TD  A[&quot;Cold-Start SFT (tool-use traces)&quot;] --&gt; B[&quot;Policy Initialization (可解析/可执行协议)&quot;]  B --&gt; C[&quot;Interleaved Rollout (LLM tokens + Tool I/O)&quot;]  C --&gt; D[&quot;Reward + Metrics (pass/fail, parse rate, cost)&quot;]  D --&gt; E[&quot;RL Update (PPO/GRPO/DAPO + KL anchor)&quot;]  E --&gt; B
这里最容易被低估的是 Cold-Start SFT：它不是“可选项”，而是“定义动作空间”的一步。
如果 cold start 不稳，RL 会出现两种典型崩法：

策略学会逃避工具：用胡编的自然语言绕开工具调用，骗过弱 reward 或弱 verifier。
策略学会滥用工具：疯狂调用工具制造长序列，KL/entropy/ratio 统计彻底失真，训练发散。


2. 数据结构：messages/tools 的“协议”决定你能不能做 RL
以 ReTool-sft.ipynb 的思路，multi-turn tool-use SFT 的样本基本长这样：

messages: 一个列表，元素有 role（system/user/assistant/tool）与 content（或 tool_call 字段）
tools: 可调用函数（或工具）的 schema 列表

你做 coding agent 时，常见工具可以抽象成三类：

执行器：code_interpreter / sandbox runner（输入 code，输出 stdout/stderr/returncode）
检索器：search / doc lookup（输入 query，输出结果片段）
文件系统：read/write/list（输入 path，输出内容）

工程建议（和模型/框架无关）：

工具 schema 先冻结再训练：schema 每改一次，你的“协议分布”就变一次，SFT 与 RL 的收敛都会变差。
tool output 永远不要当作监督目标：它属于环境观测（observation），不是 action。


3. 两套 mask：loss_mask（SFT）与 response_mask（RL）
3.1 SFT 的 loss_mask：只监督 assistant 的输出
这一点在你之前的文章里已经讲过，但它在 coding agent 上更关键：

system/user/tool 的 token：mask=0
assistant 的 token：mask=1（包含 tool_call 协议片段 + final answer）

如果你把 tool output 也训了，模型会倾向于“把工具返回当 continuation 生成”，这是 agent 最危险的坏习惯之一。
3.2 RL 的 response_mask：只对“策略生成的 token”算 logprob/ratio/KL
agent_loop_details.ipynb 有一个非常关键的标注（我把它翻成一句话）：

LLM 生成的 token：mask=1
Tool 返回的 token：mask=0

这件事如果错了，你的 RL 训练会出现非常诡异的现象：

KL 看起来异常大（因为你把 tool output 当成策略生成，而策略从未“生成过它”）
ratio 统计爆炸（logp_old/logp_new 对不上）
clip fraction 长期高或长期为 0（完全失真）

所以我建议你在实现里把 mask 语义写死成两句话（写进注释/日志里）：

loss_mask: supervised labels（SFT）
response_mask: policy actions（RL）

两者都可能是 [B, T]，但语义完全不同。

4. Cold-Start SFT：coding agent “能用”比“聪明”更重要
ReTool-overall.ipynb 把 cold start 放在第一阶段，我非常认同。
对 coding agent 来说，cold-start SFT 的目标不是把它训成“最强写码者”，而是：

稳定地产生可执行的工具调用
稳定地把工具返回转成下一步行动（多轮闭环）
输出结构可评测（比如最终 patch/答案有固定格式）

4.1 你应该盯的 3 个指标（比 loss 更重要）

tool-call parse rate（可解析率）
tool-call success rate（工具执行成功率）
end-to-end success 的 proxy（例如：单测通过率、静态检查通过率、编译成功率）

它们决定了你后续 RL 的“可学习信号”是否存在。
4.2 冷启动数据从哪来：不要迷信人工标注
对于 tool-use / coding agent，人工标注高质量多轮轨迹成本很高。更常见的冷启动方式是混合：

少量人工高质量轨迹（定义协议上限）
大量合成轨迹（覆盖分布）
rejection sampling（用 verifier/规则过滤）

注意：合成轨迹不是“越多越好”，而是“越能覆盖错误模式越好”。你需要刻意包含失败样例，让模型学会修复流程，而不是只学会一条成功路径。

5. RL with Tool Integration：reward 形态决定你用不用 shaping
coding agent 的 reward 典型是：

稀疏：是否通过 tests（0/1）
高噪声：依赖环境、随机种子、外部网络

这时 reward-shaping.ipynb 的思路就很实用：你可以给工具调用增加一些 过程性奖励（shaping），让 credit assignment 不至于完全靠最终成败。
常见 shaping（只要能被稳定评测，就值得）：

工具调用格式正确（JSON/schema pass）
工具执行成功（returncode=0）
关键中间产物出现（比如生成了 patch、生成了测试命令并执行）

反例（我不建议）：

用模型自评打分当 shaping（会被 reward hacking）
用“输出更长/更像思考”当 shaping（会被 length hacking）


6. Agent Loop：为什么 async rollout 是必需品（不是优化项）
tool-use agent 的最大吞吐瓶颈往往不是 GPU，而是工具 IO（网络、文件、执行器）。
agent_loop_details.ipynb 用“同步点”解释了一个关键问题：如果你按 batch 同步推进 multi-turn，你会被最慢的那个样本拖死。
工程上更合理的做法是：

每个 sample 一个 async task
每个 task 跑自己的状态机（LLM → tool → LLM → …）
全部完成后再 merge 成 batch（在“回传梯度/做更新”处同步）

你可以把它理解成：

rollout 阶段尽可能异步，update 阶段必须同步。

这也是 agent_loop_arch.ipynb 里反复强调的 server-based async rollout 的意义。
6.1 从代码抽象看架构：Manager / Worker / AgentLoop / ServerManager 各管一件事
如果你读 agent_loop_arch.ipynb，会看到一个非常“工程化”的拆分方式（我把它总结成四个角色）：

AgentLoopManager：接收 Trainer 的“生成一批 rollout”请求，把 batch 切块并分发给多个 Worker。
AgentLoopWorker：在 CPU 节点上跑并发（asyncio），每个 sample 一个 task，避免 straggler 拖垮整批。
AgentLoop（例如 ToolAgentLoop）：只负责“怎么聊/怎么调工具”的状态机逻辑（PENDING→GENERATING→TOOLS→…）。
AsyncLLMServerManager：只负责“怎么生成”（推理服务网关 + 负载均衡 + sticky session）。

你可以把它画成一条直线：
flowchart LR  T[&quot;Trainer / PPOTrainer&quot;] --&gt; M[&quot;AgentLoopManager&quot;]  M --&gt; W[&quot;AgentLoopWorker (CPU, asyncio)&quot;]  W --&gt; L[&quot;AgentLoop (ToolAgentLoop / SingleTurnAgentLoop)&quot;]  L --&gt; S[&quot;AsyncLLMServerManager (vLLM/SGLang servers)&quot;]
这个拆分的好处是：你后续想改“工具协议/状态机”，只需要改 AgentLoop；想改并发策略/worker 数量，只需要改 Worker/Manager；想换推理引擎，只需要改 ServerManager。
6.2 两个关键点：async task 粒度 + 状态机
在 async rollout 里，“并发粒度”基本决定你能不能把 GPU 吃满。
如果你按 “一个 batch 一个 coroutine” 并发，那你仍然会被最慢样本拖住；真正有效的是：

每个 sample 一个 coroutine（task）：快样本先结束，慢样本继续跑，GPU 侧的 continuous batching 可以不断接新请求。

状态机这块，你至少要建立这 5 个状态的直觉（工具调用 agent 基本都能套进来）：
stateDiagram-v2  [*] --&gt; PENDING  PENDING --&gt; GENERATING  GENERATING --&gt; PROCESSING_TOOLS  GENERATING --&gt; INTERACTING  GENERATING --&gt; TERMINATED  PROCESSING_TOOLS --&gt; GENERATING  PROCESSING_TOOLS --&gt; TERMINATED  INTERACTING --&gt; GENERATING  INTERACTING --&gt; TERMINATED  TERMINATED --&gt; [*]
最重要的一条工程原则是：“环境/工具返回”永远被当成 observation，绝不当成 action。
6.3 response_mask：interleaved trajectory 里怎么只对“策略 token”求梯度
multi-turn tool-use 轨迹是交错的：

LLM 生成（action）
tool 返回（observation）
LLM 继续生成（action）

RL 训练（PPO/GRPO）需要计算 logprob_new/logprob_old、ratio、KL、entropy 等，这些量只能对 LLM 生成出来的 token 有意义。
所以实现里会维护一个 response_mask（或 action_mask）：

LLM 生成 token：mask=1
tool 输出 token：mask=0

这样做的直接效果是：计算 policy gradient 时只对 mask=1 的 token 回传梯度，tool 输出不会污染 logprob/ratio/KL。
把它写成伪代码（对齐 agent_loop_details.ipynb 的核心片段）：
# after LLM generate:response_ids += llm_generated_idsresponse_mask += [1] * len(llm_generated_ids)# after tool execution:response_ids += tool_output_idsresponse_mask += [0] * len(tool_output_ids)
你要把这条规则当作 debug 的第一优先级：一旦 mask 语义错，后面 KL/ratio/clipfrac 全会“看起来像随机数”。
6.4 Async rollout 里的 reward 计算：为什么会被“前置”到 rollout 阶段
agent_loop_details.ipynb 里还强调了一个容易被忽略的点：

在 async rollout 模式下，reward 计算经常被放到 Worker 内部完成（rollout 阶段），并把 rm_scores 附着到轨迹上返回 Trainer；
Trainer 看到 batch 已经包含 rm_scores，就会跳过“再算一遍 reward”。

这在 tool-use/coding agent 场景很常见，因为 reward 往往需要：

真实执行（跑测试/编译/静态检查）
或者调用 verifier（额外模型/规则系统）

它们都是 IO/CPU 重，不适合卡住 GPU 推理进程。
6.5 Sticky session：request_id 为什么重要
一条 tool-use 轨迹会多次调用推理引擎：Prompt→Tokens→ToolResult→Prompt→…
当你有多个推理 server replica 时，如果每次都“随机路由”，你可能会在工程上踩到两类坑：

同一条轨迹跨 replica 迁移，KV cache / session 管理变复杂（甚至实现不支持）。
负载均衡短期抖动导致 tail latency 变大（多轮交互把延迟放大）。

因此 AsyncLLMServerManager 通常会用 request_id 做 sticky session：同一条轨迹固定路由到同一 replica（配合 LRU 缓存/heap-based 最少请求优先等策略）。
6.6 规模化时怎么数“replica”：num_replicas vs 引擎内部 DP
agent_loop_config.ipynb 里有一段很实用的“数服务实例”公式（把它当作配置 sanity check）：

一个 rollout server 实例内部可能用了 TP/DP/PP（引擎内部并行），其 world size 为：

$$\text{rollout_world_size}=\text{TP}\times \text{DP}\times \text{PP}$$

你总 GPU world size 为：

$$\text{world_size}=\text{n_gpus_per_node}\times \text{nnodes}$$

那么可启动的独立推理服务 replica 数为：

$$\text{num_replicas}=\frac{\text{world_size}}{\text{rollout_world_size}}$$
这背后最关键的直觉是：

num_replicas（服务级 DP）：多个完全独立的推理服务，靠 ServerManager 做负载均衡，容错更好，KV cache 各自独立。
data_parallel_size（引擎内部 DP）：同一推理引擎内部做 DP，耦合更紧，可能共享状态，但通信/同步也更复杂。


7. Coding Agent 的“训练闭环”：SFT 里就要内置 debug 接口
coding agent 的训练最怕的是“训练看似稳定，但实际不可用”。我建议你在 SFT 阶段就内置 2 个 debug 钩子：

抽样打印（decode + mask 上色）
真实执行（把 tool-call 真的跑一遍，记录失败原因）

你可以把它当成 agent 的单元测试：SFT 训练前先跑 50 条样本，确保协议与工具都没问题。
（这部分你可以直接复用 ReTool-sft.ipynb 的 mask 可视化代码。）

8. Cold Start for RL：一个可落地的路线（不要求你一上来就烧很多卡）
最后我给一条“能跑起来且不容易跑歪”的路线。它不追求最少步骤，而追求可控：

先做 cold-start SFT，把 tool-call 协议训稳（parse rate、success rate）
做一个弱 verifier（规则 + 小模型都行），把“明显不合法的轨迹”筛掉
RL 阶段先用强 KL anchor（离 SFT 很近），先学会“少犯错”
reward shaping 从强到弱：先用过程性 shaping 提供密集信号，再逐步退火到 end-to-end reward
最终再做“更难任务”的 curriculum（不要一开始就 SWE-bench 级别）

这条路线对应的工程直觉是：

先让 agent 变得“可执行”，再让 agent 变得“更优”
先让 reward 信号稳定，再让 reward 目标更苛刻


9. 小结
MultiTurn Tool Use / Coding Agent 的核心不是“换个 RL 算法”，而是：

你定义的 trajectory 里，哪些 token 属于 action，哪些属于 observation（response_mask）
你在 SFT 与 RL 阶段分别监督什么（loss_mask vs response_mask）
你是否有一条稳定的 cold-start 数据路线（否则 RL 只会放大坏习惯）

如果你愿意，下一篇我可以继续沿着这条线把“RL 端的实现细节”补全成可直接对照框架的 checklist：

interleaved rollout 下 logprob 的对齐与缓存
tool output 的长度爆炸如何处理（truncate/pack/summary）
reward manager 的工程分层（rule/verifier/RM/ensemble）

]]></content>
      <categories>
        <category>工程实践</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>Tool Use</tag>
        <tag>SFT</tag>
        <tag>Multi-turn</tag>
        <tag>Coding Agent</tag>
        <tag>Cold Start</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%20%E6%A0%B8%E5%BF%83%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88GRPO%E3%80%81RLOO%E3%80%81REINFORCE++%EF%BC%89%E4%B8%8E%20Baseline%20%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[这篇文章对应视频：“[veRL] 核心强化学习算法，GRPO、RLOO、REINFORCE++、REINFORCE++ baseline”（BV1d4Yvz4EXA）。
但我不会把它写成“视频逐句笔记”，而是把 veRL 里这些算法放到同一个坐标系里讲清楚：

它们本质上都在解同一个问题：不用训练 critic，也能把 outcome reward 变成稳定的更新信号。
它们真正的差异，主要集中在：baseline 怎么选、advantage 怎么归一、KL 怎么放。
你应该怎么选：不是看算法名字，而是看你的任务 reward 形态（稀疏/密集、噪声/可验证、是否能 group sampling）。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


建议先读（打通基础概念与日志诊断）：


PG→TRPO→PPO：推导与代码对齐


PG loss 组件详解（PPO-clip &#x2F; KL &#x2F; Entropy &#x2F; 聚合）


veRL 训练参数理解（PPO&#x2F;GRPO、Batch、KL、Entropy）



0. 资料对齐（视频 + 本地仓库）
视频：

BV1d4Yvz4EXA

https://www.bilibili.com/video/BV1d4Yvz4EXA/



本地仓库（你已下载）里，这篇最相关的材料我建议看这几个（不是必须全看）：

GRPO 目标与聚合：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/objectives/review_grpo.ipynb
/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/objectives/agg_loss.ipynb


KL 的“in reward vs as loss”、以及 off-policy bias：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/training_practices/review_kl.ipynb


训练崩溃（NaN / ratio 爆炸）的一类典型问题线索：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl/训练及调参经验/问题排查.ipynb


RLVR 评测侧：为什么这些算法的差异很多时候被“采样效率差距”淹没：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reasoning/rlvr.ipynb

RLVR 的边界：Base vs RL、pass@k、PPL 与 vLLM 评测细节





1. 统一坐标系：这些算法到底在优化什么
我们先把“共同的骨架”写下来。对每个 prompt $x$，policy $\pi_\theta$ 会采样输出序列 $y$（LLM 的 token 序列），我们有一个 outcome reward $R(x,y)$（verifiable reward / RM / judge 都可以）。
最原始的目标是最大化期望 reward：
$$J(\theta)=\mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[R(x,y)]$$
用 REINFORCE 的 log-trick，可以得到经典策略梯度形式（省略 $x$ 的条件）：
$$\nabla_\theta J(\theta)=\mathbb{E}[R(y)\nabla_\theta\log\pi_\theta(y)]$$
把它写成“最小化 loss”就是：
$$\mathcal{L}_{PG}(\theta)=-\mathbb{E}[R(y)\log\pi_\theta(y)]$$
到这里你会立刻遇到两类工程难题：

方差太大：reward 是 outcome-level、稀疏、噪声大，直接乘 logprob 会抖到你怀疑人生。
更新太猛：rollout 很贵，你会复用数据（多 epoch），这会让你从 on-policy 漂到轻度 off-policy，ratio 爆炸、NaN、输出塌缩都可能发生。

因此你看到的 GRPO / RLOO / REINFORCE++，本质上就是在这两个难题之间做取舍：

用 baseline / group relative 降方差（不训练 critic）。
用 clip / KL / 归一化 限制更新幅度，让训练别崩。


2. Baseline 的核心原则：降方差，但别把梯度搞偏
REINFORCE 里，一个重要结论是：你可以减去任何 baseline $b(x)$ 来降方差，只要它不依赖 action（在 LLM 里就是不依赖采样到的 $y$），梯度仍然无偏：
$$\mathbb{E}[(R-b)\nabla\log\pi]=\mathbb{E}[R\nabla\log\pi]$$
所以你会看到三类 baseline：

critic baseline：$b=V_\phi(s)$（PPO/Actor-Critic 路线，稳定但要训练 value model）。
group baseline：对同一 prompt 采样多条 $y_1,\dots,y_G$，用组内统计当 baseline（GRPO/RLOO 的路线）。
batch baseline/whiten：不一定改最优点，只是把更新尺度稳定住（REINFORCE++ baseline 常落在这里）。

这个章节的关键 takeaway 是：

你选择的 baseline 决定了“你到底在把 reward 的哪一部分当作学习信号”。
baseline 选错，最常见的不是不收敛，而是收敛到一种很会骗指标的行为。


3. GRPO：Group Relative Policy Optimization（“不用 critic 的 PPO 风格更新”）
3.1 为什么 GRPO 天然适配 RLVR（math/coding）
GRPO 的前提是：对同一 prompt，你愿意采样一个 group（比如 $G=8$ 或 $G=16$）：

$y_1,\dots,y_G\sim\pi_{\theta_{\text{old}}}(\cdot|x)$
reward：$R_i=R(x,y_i)$

然后你用组内的均值/方差做相对化，构造 advantage：
$$\hat A_i=\frac{R_i-\text{mean}(\mathbf R)}{\text{std}(\mathbf R)+\epsilon}$$
直觉：你不再关心“这条输出绝对分数是多少”，而关心“它在同一个 prompt 的候选里排第几”。
这个设计对 RLVR 很友好：

verifiable reward 常常是 0/1（或少数离散档），绝对尺度没什么意义。
你最终也会做 pass@k 多采样；GRPO 直接把“多采样”引入训练闭环。

3.2 为什么 GRPO 里会出现 PPO-clip 的 ratio
GRPO 在 veRL 的实现里通常仍然是 PPO 风格的 clipped surrogate（用旧策略采样数据，用新策略更新）：
$$\rho_{i,t}(\theta)=\exp(\log\pi_\theta(a_{i,t}|s_{i,t})-\log\pi_{\theta_{\text{old}}}(a_{i,t}|s_{i,t}))$$
$$\mathcal{L}_{\text{clip}}=\mathbb{E}\big[\min(\rho\hat A,\ \text{clip}(\rho,1-\varepsilon,1+\varepsilon)\hat A)\big]$$
这里的 “$\rho$” 是 ratio（为了和 reward $R$ 区分，我不写成 $r$）。
3.3 GRPO 最常见的两个坑：std=0 与 ratio overflow

组内 std=0

如果一个 group 里 reward 全一样（例如全部 0），$\text{std}(\mathbf R)=0$，不加 $\epsilon$ 直接 NaN。
这在稀疏 reward（大部分样本都错）时非常常见。


ratio overflow

如果 $\log\pi_\theta-\log\pi_{\theta_{\text{old}}}$ 变大，$\exp$ 会溢出为 inf，接着 loss/grad 就 NaN。
这类问题在实践里经常被描述成“ratio 爆炸，clamp 不能根治”，根因通常是 更新幅度太大（LR 太大、epoch 太多、adv 尺度太大、聚合方式导致有效学习率变大等）。



如果你看到 NaN，第一反应不是“再加 clamp”，而是回到日志闭环：

approx_kl(old,new)、clip_fraction、grad_norm 是否同时飙升？
group reward 的分布是否极端稀疏（全 0）？

然后再决定是改算法（比如用 REINFORCE++ baseline）还是改参数（更保守的 update）。

4. RLOO：REINFORCE Leave-One-Out（更“守规矩”的 group baseline）
4.1 RLOO 的核心：baseline 不用自己的 reward
GRPO 的组均值 baseline 会把 $R_i$ 自己也包含进去。严格来说，这个 baseline 对样本 $i$ 来说是 action-dependent 的（因为 $R_i$ 取决于 $y_i$），可能引入偏差。
RLOO 的解决办法是：对样本 $i$，baseline 用“除了自己之外的均值”：
$$b_i=\frac{1}{G-1}\sum_{j\ne i}R_j$$
$$A_i=R_i-b_i=\frac{G}{G-1}(R_i-\text{mean}(\mathbf R))$$
你可以把它理解成：

仍然是“组内相对比较”
但 baseline 的定义更符合“baseline 不依赖当前 action”这条经典原则

4.2 RLOO 的真实作用：不是让你更强，而是让信号更干净
在很多任务上，RLOO 的收益不是“指标突然大幅上升”，而是：

更新更可解释：你更接近在优化“超越组内其他样本的程度”。
行为更不容易被“自我干扰”：baseline 不被自身 reward 污染。

但它也有代价：

你仍然需要 group sampling（成本不低）。
$G/(G-1)$ 的缩放会放大 advantage 的幅度（如果你没有配套的 whiten/clip/KL 约束，训练反而更容易炸）。

所以它通常不是“开箱即用更稳”，而是“把 bias 的锅移开，让你更容易定位稳定性问题来自哪里”。

5. REINFORCE++ baseline：把“归一化的随机性”从 group 挪到 batch
5.1 一句话解释
在工程里，GRPO 最脆的一步往往不是 ratio，也不是 clip，而是 组内 std：

reward 稀疏时，组内 std 极不稳定，甚至频繁为 0
于是 advantage 会出现 NaN 或极端放大

REINFORCE++ baseline 的常见思路是：

baseline 仍然用组均值（做相对化）：

$$\tilde A_i=R_i-\text{mean}(\mathbf R)$$


但归一化不用组内 std，而用 batch 级统计（更稳定）：

$$\hat A_i=\frac{\tilde A_i}{\text{std}(\tilde A\ \text{over batch})+\epsilon}$$



这会带来一个很现实的收益：

只要你的 batch 足够大，std 就不会频繁为 0；
advantage 的尺度在训练过程中更一致，更容易调学习率和 KL 系数。

5.2 这不是“更正确”，而是更可控
把 std 从 group 挪到 batch，本质上是在做一个工程选择：

GRPO 的“归一化”更局部、更自洽，但也更不稳定；
REINFORCE++ baseline 的“归一化”更全局、更稳定，但它引入了跨 group 的耦合（不同 prompt 的 reward 分布会互相影响尺度）。

你该怎么选？

如果 reward 稀疏到让组内 std 经常为 0：优先考虑 REINFORCE++ baseline。
如果你希望每个 prompt 的训练信号都只由自身 group 决定：GRPO 更符合这个直觉。


6. KL 放哪：in reward vs as loss（不要把它当“实现细节”）
在 LLM-RL 里，KL 既可以写进 reward（dense shaping），也可以作为单独 loss（regularization）。这不是语法差异，它会改变训练动态：

KL in reward

你把每一步偏离 ref 的代价当成 reward 的一部分，advantage/return 会被它影响。
好处：更像“每步都扣分”的稠密信号；坏处：它会和 baseline/归一化强耦合，导致你很难判断“到底是任务 reward 在驱动，还是 KL 在驱动”。


KL as loss

advantage 主要反映任务 reward，KL 作为独立正则单独控制漂移。
好处：更好调参与诊断；坏处：如果你的任务 reward 极稀疏，单独 KL 可能会让训练更像“什么都不学，只是在原地”。



我更偏向的工程原则是：

先把 KL 当作独立 loss（便于诊断），把 approx_kl/entropy/clipfrac 闭环跑顺。
只有当你非常确定“任务 reward 太稀疏”，且你需要更稠密的 shaping，再把 KL 放进 reward（并且要重新校准 baseline/归一的尺度）。


7. 一份更实用的选型指南（给做 agentic RL / deep research 的你）
把这些算法和任务形态对齐，我给你一个不花哨但能落地的建议顺序：

reward 可验证（0/1 或少数离散档），你能 group sampling

先试 GRPO（最接近“无 critic 的 PPO”）
如果经常 std=0 或 NaN，切到 REINFORCE++ baseline（更稳定的尺度）
如果你非常在意 baseline 的无偏性，或者想把 bias 这一项从误差来源里排除，再试 RLOO


reward 噪声大（judge/RM），但你仍然 group sampling

你要先做 reward 的单元测试与对抗样本（否则 RL 一定会放大漏洞）
在算法上，REINFORCE++ baseline 往往比 GRPO 更不容易因为归一化噪声而炸


deep research 这类“多步工具调用 + 长上下文”任务

不建议一上来就把 RL 用在 token policy 上
更现实的路径是：先把 RL 用在“agent 决策层”（检索/阅读/引用/停止/预算分配），让 reward 更可控、更可验证



最后补一句：GRPO/RLOO/REINFORCE++ 解决的主要是 variance/control，不是 capability creation。如果你期待“模型学会以前不会的东西”，你仍然需要：

curriculum / distill / tool-augmented data（把新轨迹带进可学习分布）
再用 RL 把采样效率和稳定性做上去


8. 小结
把本文压缩成三句话：

GRPO/RLOO/REINFORCE++ baseline 的核心差异不是“RL 更强”，而是 baseline 与归一化设计。
训练不稳时，优先怀疑 advantage 统计与 ratio 漂移，别先怀疑模型。
你做 agentic RL 的关键不是挑一个算法名字，而是把 reward 可信度 + 评测闭环 + 日志诊断先搭起来。

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>RLHF</tag>
        <tag>RLVR</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>GRPO</tag>
        <tag>RLOO</tag>
        <tag>REINFORCE++</tag>
        <tag>Baseline</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&amp;%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&amp;%20Entropy%EF%BC%89/</url>
    <content><![CDATA[这一篇对应 veRL 视频：“从原理层面理解训练参数，PPO &amp; GRPO，batch size，kl &amp; entropy”（BV1DZL1zNEN2）。
写这篇的目标不是“照着参数表翻译”，而是把你在 verl（veRL）里最常改、最容易踩坑的配置项，和它背后的算法对象一一对齐：你改的到底是“采样分布/有效 batch/更新步长/探索强度/保守性”，还是只是在调一个看起来像超参的数字。
我不会把视频内容当成“圣经”。这类讲解里最容易出错的两件事是：

把经验区间说成普适真理（例如某个指标“应该在 0.1-0.4”）。
把实现细节当成算法本身（例如 KL 的某种近似估计，被误当成 KL 的定义）。

所以本文会更强调“你应该如何用日志闭环验证”，而不是“照着配方抄参数”。一句话：参数之间强耦合，你改一个数字，往往同时改了“有效学习率、更新步长、数据复用程度和探索强度”。
系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


延伸阅读（更偏算法本体而不是参数）：


veRL 核心算法：GRPO &#x2F; RLOO &#x2F; REINFORCE++ 与 Baseline 设计



0. 资料对齐（视频 + 本地仓库）

视频：BV1DZL1zNEN2

https://www.bilibili.com/video/BV1DZL1zNEN2/


配套仓库（你本地路径）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl


本文主要引用的笔记（按重要程度）：

agentic_rl/verl/objectives/objectives_loss.ipynb：PPO / KL / entropy / clip fraction / 训练时该看什么曲线
agentic_rl/verl/训练及调参经验/config-perf-tuning.ipynb：mini/micro batch 切分、dynamic batch、loss scale
agentic_rl/verl/objectives/agg_loss.ipynb：loss_agg_mode 为什么会改变“有效 batch”与长度偏置
agentic_rl/verl/objectives/review_grpo.ipynb：PPO vs GRPO 直觉与 DeepSeek 风格细节（adv 标准化 / off-policy masking）
agentic_rl/verl/objectives/grpo_gspo.ipynb：GRPO/GSPO 的“序列 vs token”缩放因子（更偏进阶，可选）
agentic_rl/verl/训练及调参经验/sft.ipynb：micro_batch_size_per_gpu 才是显存真开关、长序列的 sequence parallel



verl 配置文档入口（笔记里引用的链接）：

https://verl.readthedocs.io/en/latest/examples/config.html


0.1 先建立一个“不会被参数名带偏”的心智模型
verl 的参数很多，但绝大多数可以映射回 4 个“你真正关心的杠杆”。我建议你以后看任何 RLHF/RLVR 框架，都先按这个分类理解：

采样分布（Sampling Distribution）：你让旧策略怎么生成数据。
数据复用强度（Data Reuse / Off-policy Drift）：同一批 rollout 你要训练几轮，以及每轮切多细。
更新幅度（Update Magnitude / Trust Region）：你允许策略一步走多远。
探索强度（Exploration / Diversity）：你是否允许模型持续探索，而不是快速塌缩到单一模板。

同一个“症状”（比如 reward 不涨）可以来自这四类中的任意一种原因。把它们混在一起调，最常见的结果是：短期变好，长期崩；或者“看起来更稳了”，其实只是探索被掐死。
1. 你在 verl 里训练 PPO/GRPO，本质做的是哪 4 件事
把实现细节都抹掉，verl 的 RL4LLM（PPO/GRPO）训练闭环可以压缩成 4 步：

Rollout（采样）：用旧策略 $\pi_{\theta_{\text{old}}}$ 生成 response（或者每个 prompt 生成一组 response）。
打分（reward / verifier）：得到每条轨迹的 reward（verifiable reward / RM / rubric / verifier 都属于这一步）。
信用分配（advantages）：

PPO：靠 critic/GAE 把序列回报分解成 token 级 advantage；
GRPO：不用 critic，直接在“同一 prompt 的一组样本”内部做 baseline（group mean / std）。


优化（update）：对同一批 rollout 数据做若干个 epoch，按 mini-batch/micro-batch 切分，梯度累积，然后更新 actor（以及 PPO 的 critic）。

所以当你看到“训练不稳定/显存爆/速度慢/指标不涨”，不要先猜玄学超参。先问自己它属于哪一步：

rollout 太慢：推理引擎与 batching 问题；
reward 噪声大：评测/奖励设计问题；
advantage 质量差：baseline/标准化/critic 学不动；
update 太激进：batch/epoch/LR/clip/KL/entropy 的耦合问题。


1.1 “On-policy” 在 LLM-RL 里经常是一个误导词
很多人听到 PPO/GRPO 是 on-policy，就默认“只要是 on-policy 就稳”。但在工程里你几乎必然会做两件事，它们会把你推向 轻度 off-policy：

一次 rollout 很贵，所以你会对同一批数据做多个 epoch 更新（ppo_epochs &gt; 1）。
一次 rollout 的 batch 很大，所以你会切成很多 mini/micro 去更新。

当你跑到第 2/3 个 epoch 时，当前策略已经不是产生数据的那一个策略了。于是你会看到经典组合：

clip fraction 上升（大量 ratio 被裁到边界）；
approx_kl(old,new) 上升（更新跨得更大）；
reward 未必更好（甚至变差），因为你可能在反复榨同一批数据，过拟合到了某些 reward hacking 模式。

所以我更推荐你把 ppo_epochs 当成“数据复用强度旋钮”，而不是“训练更充分”的旋钮。更稳的调参顺序是：

先用较小 ppo_epochs 跑通（例如 1-2），把 KL/entropy/clipfrac 的形态看明白。
只有当你确认更新非常保守（KL 很低、clipfrac 接近 0、reward 上不去）且 rollout 成本极高时，再考虑增加 epoch。

2. Batch size：为什么你觉得改了 batch，结果像改了 learning rate
2.1 三个名字：Global / Mini / Micro（以及 GA）
从“优化器每次 step 用了多少样本”这个角度，你只需要记住三层：

Global batch（全局 batch）：一次参数更新（optimizer step）看到的样本总量。它决定梯度噪声大小与“每步更新有多稳”。
Micro-batch（微 batch）：单卡一次 forward/backward 放得下的样本量。它几乎直接决定峰值显存。
Gradient Accumulation（梯度累积，GA）：当 global &gt; micro_total 时，用多次 micro forward/backward 累积梯度，再做一次 optimizer step。

在 verl 的笔记里也强调过一句很工程的话：

真正影响显存的常常是 micro_batch_size_per_gpu，而不是 train_batch_size。

（见 agentic_rl/verl/训练及调参经验/sft.ipynb）
2.2 PPO update 阶段：mini-batch 和 micro-batch 怎么对应
config-perf-tuning.ipynb 把 actor 更新阶段的逻辑写得很直白：
for _ in range(self.config.ppo_epochs):    for batch_idx, mini_batch in enumerate(mini_batches):        if self.config.use_dynamic_bsz:            micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)        else:            self.gradient_accumulation = self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu            micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)
你可以直接把它翻译成一句话：

ppo_mini_batch_size 决定“这一次 update 的统计单位有多大”（更像优化器视角的 batch）。
ppo_micro_batch_size_per_gpu 决定“单卡一次能塞多少”（更像显存视角的 batch）。
两者的比值决定 GA 次数：mini // micro。

这也是为什么你会感觉“改 batch 像改了学习率”：

在很多实现里，loss 默认是 mean reduction；
当你改变 micro/mini 的切分方式时，梯度尺度往往也被改变了；
于是相当于你隐式改变了“每次 step 的有效步长”。

这里补一个更“从原理出发”的说法：

在 SGD 视角下，batch size 决定的是梯度噪声大小与统计稳定性。你一旦改变 loss 的归一化方式（mean/sum、token/seq 聚合），batch size 的含义就会变形，进而等价成“有效学习率”的变化。

2.3 Dynamic batch：按 token budget 切分，比按样本数切分更靠谱
LLM-RL 最大的工程痛点是序列长度差异极大：

固定样本数切 micro-batch：长序列 batch OOM，短序列 batch 显存空闲，吞吐不稳；
所以 verl 提供 use_dynamic_bsz=True：不是“每个 micro-batch 固定 K 条样本”，而是“每个 micro-batch 固定 token 上限”。

在笔记里，这个 token 上限被写成：

ppo_max_token_len_per_gpu * ulysses_sequence_parallel_size

这意味着：序列并行（SP）更多是为了省显存，不是为了加速；它把“每卡 token budget”摊到更多并行里。
2.4 Dynamic batch 必须配套一个东西：loss scale（否则梯度有偏）
Dynamic batch 里每个 micro-batch 的样本数 $|b_j|$ 不再相同。为了让最终梯度等价于“对整个 mini-batch 的平均”，verl 会对每次 micro 的 loss 乘一个 scale factor（笔记里给了对应代码解释）：

loss_scale_factor = response_mask.shape[0] / self.config.ppo_mini_batch_size

直觉上它做的是：这个 micro-batch 占整个 mini-batch 的比例是多少，就让它贡献相应比例的梯度。
如果你只开了 dynamic batch，但忽略了 scale，你训练出来的结果很可能“看似能跑，指标却怪”，因为长短样本被系统性重加权了。
2.5 你真正的 compute 单位是 token，而不是 sample
在 LLM-RL 场景，“同样的 32 条样本”可能是完全不同的算力消耗与优化难度：长 CoT 的 token 数、激活保存、通信量都更大，也更容易触发 OOM 或梯度不稳定。
所以我建议你在日志/实验记录里把 batch 写成两行：

num_samples（样本条数）
num_tokens（响应 token 总数，或平均 token）

你会发现很多“看似随机”的不稳定，其实和 token 分布的长尾强相关。

3. PPO vs GRPO：同样是“policy gradient”，参数含义完全不同
3.1 PPO：更精细，但你必须养得起 critic
PPO 的核心对象是 token 级别的 advantage $\hat A_t$，常见形式是 GAE：
$$\hat A_t = \delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \cdots$$
所以 PPO 的“稳定”来自两个东西：

critic 给 baseline：降低方差；
clip / KL trust region：限制一步更新幅度。

代价也很明显：

你要训练/维护 critic（更多显存、更多 compute、更多不稳定来源）。

我个人判断 “PPO 值不值得上” 的标准是：你是否真的需要 token-level credit assignment。

如果你的 reward 基本只在序列末端出现（典型 RLHF/RLVR），critic 很容易学成“差不多的常数”，advantage 质量不高，PPO 的复杂度未必值得。
如果你的 reward 可以拆成比较细的过程信号（例如 tool 选择、检索覆盖、证据一致性等中间指标），token-level baseline 才更可能带来稳定收益。

3.2 GRPO：没有 critic，你把 baseline 放进“同组样本”
GRPO 的典型设定是：对同一个 prompt 采样一组 completion（组大小记作 $G$），得到 reward ${R_{i,1}, \dots, R_{i,G}}$，然后用组内统计量构造 advantage。
你可以把最常见的版本记成：
$$\hat A_{i,j} = \frac{R_{i,j} - \text{mean}(\mathbf R_i)}{\text{std}(\mathbf R_i) + \epsilon}$$
笔记里也提到 DeepSeek 风格的一种简化：不除以 std，只做中心化：
$$\hat A_{i,j} = R_{i,j} - \text{mean}(\mathbf R_i)$$
所以 GRPO 里你改的“batch size”经常有两层含义：

你每个 prompt 采样的组大小 $G$（决定 baseline 质量与方差）；
你一次 update 看到多少个 prompt（决定全局梯度噪声）。

如果你把 PPO 的直觉（“batch 越大越稳”）直接搬到 GRPO 上，经常会误判，因为 GRPO 的关键瓶颈是“组内差异”是否足够大、baseline 是否有意义。
这里我补一个常被忽略的现实：组内差异很多时候来自采样温度，而不是来自模型能力。
如果你把 temperature/top_p 调得很保守，组内样本高度同质，你就会看到：

reward std 很小；
advantage std 很小（甚至接近 0）；
训练看起来“很稳”，但 reward 也不涨，因为你几乎没探索到新的轨迹。

反过来，如果你把采样调得太散，组内差异很大但正确率很低，你会得到高方差 advantage，训练会非常抖。
所以 GRPO 最核心的调参，往往不是“更大 batch”，而是 “合理的 group size + 合理的采样多样性”。
3.3 一个很容易被忽略的参数：loss 聚合方式（loss_agg_mode）
agg_loss.ipynb 讲了一个非常重要但常被忽略的问题：你怎么把 token loss 聚合成一个标量，会系统性改变训练偏好。
例如 sample-level 的 seq-mean-token-mean，会把每个样本先除以序列长度；这在长 CoT 场景可能导致：

长而正确的解被“平均”掉，梯度缩小；
短而凑巧的解反而更占优势（长度偏置）。

这不是“理论洁癖”，而是会直接体现在你训练出来的输出风格上（短/长、是否愿意展开推理）。
verl 文档也明确提到：

原始 GRPO 常用 sample-level；
DrGRPO / DAPO 等会改成更 token-level 的聚合来提升稳定性与减少长度偏置。

我建议你把 loss_agg_mode 当成“优化目标定义的一部分”，而不是“实现细节”：

你的 reward 如果本质是 sequence-level（例如 pass/fail），你用 per-token mean 去归一化，就相当于把目标改成了“单位 token 的平均收益最大化”，这会系统性偏向短输出。
如果你希望鼓励长 CoT 并且 reward 更像“总回报”，token-mean 往往更贴近直觉。

这个问题没有绝对对错，只有“你想要的行为”。

4. KL 与 Entropy：一个管“别飘”，一个管“别死”
4.1 KL：你需要先分清两种 KL
在 LLM-RL 里常出现两类 KL（含义完全不同）：

PPO 的 KL（new vs old）：衡量你这次更新跨得有多大，常配合 clip fraction 一起看，属于稳定性监控。
RLHF/RLVR 的 KL（policy vs ref）：把策略约束在 reference（通常是 SFT/base）附近，防止语言漂移、reward hacking、分布坍缩。

为了避免“大家都叫 KL，但每个人脑子里不是同一个东西”，我更喜欢把它们写成公式（LLM 里 action 就是 token）：

PPO 的 ratio 与 KL（old vs new）：

$$r_t(\theta)=\exp(\log \pi_\theta(a_t|s_t) - \log \pi_{\theta_{\text{old}}}(a_t|s_t))$$
$$\widehat{\mathrm{KL}}(\pi_{\theta_{\text{old}}}|\pi_\theta)\approx \mathbb{E}_{a\sim \pi_{\theta_{\text{old}}}}[\log\pi_{\theta_{\text{old}}}(a|s)-\log\pi_\theta(a|s)]$$


对 reference 的 KL（policy vs ref）：

$$\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})=\mathbb{E}_{a\sim \pi_\theta}[\log\pi_\theta(a|s)-\log\pi_{\text{ref}}(a|s)]$$



一个很重要但常被忽略的点：KL 的方向会改变偏好。在 RLHF/RLVR 里常用的 $\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})$ 更像“mode-seeking”的约束，它天然会惩罚跑到 ref 低概率区域的行为，所以你在很多任务里会看到分布被削尖、输出多样性下降。这不是“实现问题”，而是 KL 方向本身带来的效应。
objectives_loss.ipynb 里给的实现提示很直接：最终 policy loss 往往是“PG surrogate + KL loss”：
policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
你调 kl_loss_coef 的直觉可以粗暴理解为：

越大：越保守，越像 SFT，越不容易 reward hacking，但也越难把 reward 推上去；
越小：越激进，更容易“学会取悦 reward”，同时也更容易跑飞。

4.2 为什么“KL + entropy”经常要一起调
objectives_loss.ipynb 里还强调了 entropy（探索强度）：

entropy_coeff 默认可能是 0；
但一旦 reward 非常稀疏或你的策略容易塌缩（只会输出一种模板），entropy 可能是救命的。

entropy 的计算在笔记里也给了很清楚的 token 级公式（这里用直觉版）：
$$H(\pi(\cdot)) = -\sum_v p(v)\log p(v)$$
判读上你只需要记住：

entropy 低到贴地：模型输出非常确定，探索不足，容易 mode collapse；
entropy 持续很高：策略没收敛，可能 reward 信号太弱或 update 被各种约束压没。

4.3.1 entropy bonus 不是“补丁”，更像“探索预算”
一个常见误区是：模型塌缩了，就把 entropy_coeff 加大。
entropy bonus 的副作用很明确：它会持续奖励“分布更平”。如果你的 reward 信号本身很弱，entropy 可能会把你推向“随机但多样”的区域，reward 更难学。
更好的策略通常是：

先把 reward 设计到足够可学习（区分度、噪声、可验证性）。
再用 entropy 做“探索预算”，并考虑 schedule（前期大、后期小），而不是一把梭。

4.3 训练时你该盯的 4 条曲线（比“loss 变没变小”重要）
结合 objectives_loss.ipynb 的建议，训练中优先看这几类：

reward / task metric curve（主指标）：别盯 policy loss 的绝对值。
KL（对 ref + 对 old）：是否偏离过大、是否长期贴 0（步子太小）。
clip fraction：它是个很敏感的“步子是否过猛”的指标，但没有普适黄金区间。长期过高通常意味着更新过猛（LR 大、epoch 多、优势尺度过大），长期贴 0 通常意味着更新过保守或 advantage 接近 0。
actor entropy：探索是否快速坍缩。

如果你愿意再加一条，盯 adv/std 或者 GRPO 的 reward std，能很快看出“baseline 是否还有意义”。
4.4 别把 KL 系数当成常数“背配方”，更建议用闭环控制思路
不同实现里 KL 的估计方式（token/seq 聚合、近似形式）并不完全一致，所以你不能把某个固定系数当成可迁移经验。
比起死守一个系数，我更建议你用控制思路：

设一个你能接受的 KL 目标区间（对 ref 或对 old，看你要控制什么）。
观察 KL 与 reward 的耦合：KL 上去 reward 不涨，通常是更新在学坏；KL 很低 reward 不涨，通常是更新太保守或 reward 太弱。
让 kl_loss_coef 变成“为了把 KL 拉回目标区间”的旋钮，而不是“一个固定常数”。

4.5 采样温度 vs entropy bonus：别把同一件事调两次
temperature/top_p 是 采样分布层面的探索；entropy bonus 是 优化目标层面的探索。它们经常被同时调，导致你以为自己在“调探索”，实际上是在两处重复施力。
一个更可诊断的做法是：

先固定采样温度，用 entropy bonus 调出一个不会塌缩的训练。
再把 entropy bonus 固定住，用采样温度去控制“你愿意花多少算力去探索”。

4.6 两个常见误读（我建议你别照搬任何人的结论）

“clip fraction 0.1-0.4 是黄金区间”：这只是常见经验，不是定理。clipfrac 的合理范围依赖于 clip epsilon、advantage 尺度、epoch 数、以及你到底在算 token-level 还是 seq-level。
“policy_loss 的正负代表训练好坏”：policy loss 是 surrogate，且数据分布在变。你更应该看 reward、KL、entropy、ratio 分布形态，以及“指标是否可复现”。


5. 你要把 verl 用在 agentic RL / deep research，上来就该怎么改参数
把“深研究 agent”抽象成一个 RL 任务，最常见的失败模式是：reward 难、噪声大、探索空间巨大。
因此一个更稳的起手式通常是：

先把 rollout 推理打通并做吞吐优化（你已经有 vLLM 那篇）。
在 update 侧优先保证“不会跑飞”：KL/clip/epoch 不要太激进。
在 reward/advantage 侧优先保证“有有效差异”：GRPO 的组内方差、或者 verifier 的区分能力。

这里我再补一个更“像研究”的建议：用最小实验矩阵把耦合拆开。例如固定三样只动一样：

固定 sampling（temperature/group size），只调 update（epochs/clip/KL）；
固定 update，只调 sampling；
固定两者，只改 reward/advantage（baseline/标准化/长度归一化）。

这样你才知道“系统到底在优化什么”，也更容易判断视频里的说法是否适用于你的任务。

6. verl/veRL 框架层面的关键点（比单个超参更重要）
这一节不是“照视频”，而是我认为 verl 真正强的地方：它把 rollout 的系统问题（长尾、并发、训练-推理资源争用、tokenization 一致性）当成一等公民处理。对 agentic RL 来说，这往往比你选 PPO 还是 GRPO 更关键。
6.1 为什么 agent rollout 倾向 async：长尾（straggler）会吃掉你的 GPU
agent 任务里不同样本完成 rollout 的时间差距极大：简单样本一轮对话结束，复杂样本可能多轮推理 + 多次工具调用 + 报错重试。如果用同步 batching，整批会被最慢样本拖死，GPU 大量空转。
verl 的 AgentLoop 体系用 async 并发把“快任务”和“慢任务”交织起来，本质是在解决这个系统瓶颈（见 agentic_rl/verl/agent/agent_loop_details.ipynb）。
6.2 token-in/token-out：训练-推理一致性不是细节，是收敛性问题
agentic_rl/verl/tokenizer/encode-decode.ipynb 里强调：encode(messages) 并不等价于 prompt_ids ⊕ response_ids，decode 再 encode 也可能不可逆。在 RL 训练里，这会导致 trajectory 偏离策略分布，严重时 PPO 甚至不收敛。
因此在多轮 agent rollout 里，verl 更倾向 token-in/token-out：用 token ids 作为接口，避免 chat template 反复 encode/decode 的不一致。
6.3 tool 返回的 token 不参与 policy gradient（response_mask）
AgentLoop 里常见做法是维护 response_mask：

LLM 生成的 token：mask=1（参与 loss）
Tool 返回的 token：mask=0（不参与 loss）

这不是“实现偏好”，而是你必须明确：你究竟在优化 LLM 的什么行为。把工具输出当作 action token 来回传梯度，会把“环境反应”错当成“策略行为”，梯度变得没有意义。
6.4 Hybrid 模式：同一批 GPU 在训练和推理之间分时复用
verl 里有 wake_up/sleep 机制，用于在 rollout（vLLM/TP，占 KV cache）和 train（FSDP，占梯度/优化器状态）之间切换资源。这类机制是否好用，取决于你的集群形态：

推理训练分离（standalone）：架构更简单；
同卡复用（hybrid）：吞吐可能更高，但工程复杂度与系统抖动也更高，调参必须把“系统因素”算进去。


7. 一个更靠谱的“调参诊断表”：用现象定位到旋钮
下面这张表不是配方，是把“症状 -&gt; 可能原因 -&gt; 优先改什么”串起来。你不需要完全照做，但建议你每次改参数都能回答：我是在动哪类旋钮（采样/复用/更新/探索），我预期哪个日志指标会怎么变。

reward 完全不涨，KL(old,new) 很低，clipfrac 接近 0，entropy 也不高
可能原因：更新太保守或 advantage 接近 0（组内样本太同质 / reward 无区分度）。
优先动作：降低 kl_loss_coef 或稍增 LR/clip epsilon；提高采样多样性（temperature/top_p 或 group size）；检查 reward std / adv std。
reward 短期上涨后崩，KL(ref) 快速上升，entropy 下降很快
可能原因：在学 reward hacking 或走出 ref 分布太快。
优先动作：增大 kl_loss_coef；减少 ppo_epochs；降低 LR；检查 reward 设计是否过度偏向格式/捷径。
clipfrac 很高但 KL(old,new) 不高
可能原因：同一批数据被多 epoch 复用，ratio 被推到 clip 边界，但整体 KL 还没很大（或被聚合方式稀释）。
优先动作：先减 ppo_epochs；再看是否需要增大 ppo_mini_batch_size 或调整 loss_agg_mode；检查 advantage 尺度是否过大（reward scale / 标准化）。
GRPO 训练“很稳但没学到”，reward std 很小
可能原因：采样太确定，组内没有有效对比；或者 reward 太粗。
优先动作：提高采样温度或 group size；把 reward 拆成更可学习的分项（哪怕是 proxy）；必要时引入 verifier/自验证提升区分度。
一切看起来都正常，但换成多轮 agent 就完全不收敛
可能原因：tokenization/trajectory 不一致（chat template encode/decode），或者 tool 输出误入梯度。
优先动作：确保 token-in/token-out；确认 response_mask 把 tool token 排除；优先把单轮收敛后再扩到多轮。

如果你后续希望我把“verl 的 agent loop + rollout + reward manager + objective”按代码路径拆成能复现的工程笔记，我建议下一篇从这些 notebook 开始：

agentic_rl/verl/agent/agent_loop_arch.ipynb
agentic_rl/verl/agent/agent_loop_details.ipynb

它们会把“训练循环怎么把 agent 的 tool 交互排除在梯度外、怎么做 async rollout、怎么做 continuous batching”讲得更像一套系统，而不是零散参数。
]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>PPO</tag>
        <tag>KL</tag>
        <tag>veRL</tag>
        <tag>verl</tag>
        <tag>GRPO</tag>
        <tag>Batch Size</tag>
        <tag>Entropy</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：从 PG 到 TRPO 到 PPO-Clip（推导与代码对齐）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9A%E4%BB%8E%20PG%20%E5%88%B0%20TRPO%20%E5%88%B0%20PPO-Clip%EF%BC%88%E6%8E%A8%E5%AF%BC%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AF%B9%E9%BD%90%EF%BC%89/</url>
    <content><![CDATA[这篇文章面向“RL 基础薄弱但想快速读懂 PPO/Agentic RL 训练代码”的读者，目标是把下面三件事讲清楚，并且和你能跑起来的代码一一对应：

PG loss 到底是什么：它不是“监督学习意义上的 loss”，而是为了拿到正确梯度构造的 surrogate（代理目标）。
TRPO 想解决什么：为什么要用 KL trust region 限制策略更新幅度。
PPO-clip 为什么长这样：clip 不是拍脑袋，而是在做一个“好实现”的 trust region 近似。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


配套资料来自 B 站视频与仓库（建议你边看边对照）：

视频 02：[Agentic RL] 02 策略梯度基础，从 PG 到 TRPO 到 PPO-Clip 核心公式简单推导

https://www.bilibili.com/video/BV12NHJzTEoP/


视频 03：[Agentic RL] 03 策略梯度补充，GRPO loss 分析，优势标准化，On Policy

https://www.bilibili.com/video/BV18hsFzbEKJ/


代码/笔记仓库：

GitHub：wdkns/modern_genai_bilibili
核心 notebook：

agentic_rl/deep_RL/pg/from_pg_to_ppo-clip.ipynb（视频 02：推导主线）
agentic_rl/deep_RL/pg/policy_gradient.ipynb（视频 03：GRPO loss / 优势标准化 / On-policy + PPO case study）


一个“玩具 LLM-RL 对齐”脚本（REINFORCE + KL penalty）：

agentic_rl/deep_RL/scripts/reinforce_align.py





如果你更关心 PPO 训练工程里的 Dual-clip / Entropy / KL / Loss 聚合，建议先读我上一篇（组件解析更偏工程视角）：


Agentic RL：PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）


如果你希望把“推导”快速落到“训练调参/日志诊断”，建议你也顺带读：


Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）



0. 先校正一个直觉：PG loss 不是“性能指标”，reward curve 才更像
监督学习里，你最小化的是一个“固定数据分布上的静态目标”。
而 RL（尤其 on-policy 的 PPO/GRPO）里，你每一轮采样的数据分布都在变：

第 $k$ 次更新，用的是由 $\pi_{\theta_k}$ 采样得到的数据分布 $\mathcal{D}_k$；
第 $k+1$ 次更新，用的是 $\pi_{\theta_{k+1}}$ 采样得到的 $\mathcal{D}_{k+1}$；

所以你日志里的 policy_loss 横向并不总可比，它更像“当前这一步的更新信号强弱”，而不是“模型变好了多少”。
在 LLM-RL / Agentic RL 里更是如此：你真正关心的通常是任务指标（成功率、RM reward、事实一致性、引用覆盖、成本等），它们才是你要看的 reward curve。
0.1 为什么 RL 的 loss curve 常常“越训越大/不单调”
在很多 LLM-RL/GRPO/PPO 的实现里，你看到的目标大致可以写成“策略项 + KL 正则”的形式（这里用 PPO-clip 举例）：
$$\mathcal{L}_k(\theta) = - \mathbb{E}_{(x,y) \sim \mathcal{D}_k} \left[ \min \left( r_\theta A, \text{clip}(r_\theta, 1 - \epsilon, 1 + \epsilon) A \right) \right] + \beta \cdot \mathbb{E}_{x \sim \mathcal{D}_k} \left[ \text{KL}(\pi_\theta(\cdot|x)|\pi_{\text{ref}}(\cdot|x)) \right]$$
关键点是：第 $k$ 轮和第 $k+1$ 轮的期望分布 $\mathcal{D}k$、$\mathcal{D}{k+1}$ 不是同一个，因为它们来自不同的策略采样。所以你画出来的 “loss curve” 很可能是在比较 “apple/orange/potato”，横向可比性天然很差。
更可操作的结论：

reward curve/任务指标优先于 loss curve。
loss 组件里更值得看的通常是：KL、clip fraction、entropy、adv/std 等“稳定性指标”。

0.2 这篇文章的“工程结论”清单（不看推导也能用）
如果你读完这篇只记住几条“能用来调参/读日志”的结论，我希望是这些：

PG/PPO 的 policy loss 是 surrogate，不是性能指标：跨 iteration 横向比较往往没意义，优先看 reward/成功率/任务指标。
PPO 的关键中间量就是 ratio：r = exp(logp_new - logp_old)。一旦你做多 epoch 更新或复用数据，它就会从 1 开始漂移。
PPO-clip 的目的不是“让 loss 变小”：而是让“有利方向的过度更新”被截平，避免一步跨太大导致崩。
TRPO/PPO 的约束本质是“更新别太猛”：工程上你通常用 approx_kl / clip_fraction / entropy 这种可观测量去闭环，而不是盯着一个理论上的 $\delta$。
在 LLM-RL 里，聚合方式改变数值尺度：token-level vs seq-level，sum vs mean，会直接改变“你以为的学习率/系数大小”。先把聚合讲清楚再谈超参区间。
多 epoch = 轻度 off-policy：PPO 常被叫 on-policy，但“rollout 很贵所以复用数据”会把你推向 off-policy drift；它不是原罪，但必须用 KL/clipfrac 监控。

（如果你想要一个“看到某个现象该动哪个旋钮”的表格版诊断，直接看 veRL 那篇的 调参诊断表。）

1. 我们到底在优化什么：轨迹期望回报 $J(\theta)$
在策略梯度（Policy Gradient）视角下，我们用参数 $\theta$ 的策略 $\pi_\theta(a|s)$ 与环境交互，产生轨迹（trajectory）：
$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$$
典型的目标是最大化期望折扣回报：
$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T}\gamma^t r_t\right]$$
注意：$\tau$ 的分布是由 $\pi_\theta$ 决定的，这意味着你不是在对一个固定数据集做最优化，而是在对“会随着参数变化的采样分布”做最优化。

2. 为什么会出现 log π：log-trick（score function estimator）
你想对 $J(\theta)$ 求梯度，困难在于：期望是对一个依赖 $\theta$ 的分布取的。
关键技巧是：
$$\nabla_\theta \mathbb{E}_{x\sim p_\theta}[f(x)] =\mathbb{E}_{x\sim p_\theta}\big[f(x)\nabla_\theta\log p_\theta(x)\big]$$
把 $x$ 换成轨迹 $\tau$，并利用“环境转移不依赖 $\theta$（只依赖策略）”，可以推到经典的 REINFORCE / PG 形式：
$$\nabla_{\theta} J(\theta) = \mathbb{E}\left[\sum_t G_t \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)\right]$$
其中 $G_t$ 是 return-to-go：
$$G_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_k$$
这回答了你问的：

PG loss 里的 log π 从哪里来的？

它来自对分布求导时必须使用的 log-trick，否则你很难用采样/蒙特卡洛得到无偏梯度估计。

3. Advantage（baseline）：为什么从 $G_t$ 变成 $A_t$
REINFORCE 的梯度估计方差很大。一个经典做法是减去 baseline $b(s_t)$，不改变期望梯度，但显著降低方差：
$$A_t \doteq G_t - b(s_t)$$
直觉上，优势函数把“绝对分数”变成“相对平均水平的超额收益”：

$A_t &gt; 0$：这步动作比平均水平好，应该提高它出现的概率；
$A_t &lt; 0$：这步动作比平均水平差，应该降低它出现的概率。

在实现里，常见 baseline 就是 value function $V(s_t)$（Actor-Critic 体系），于是：
$$A_t \approx Q(s_t,a_t) - V(s_t)$$
3.1 折扣回报与 baseline 的直觉
折扣（discount）回答的是：离奖励越远的动作，对最终回报“负责得越少”。

baseline 的直觉则是：把“绝对分数”变成“相对平均水平的超额收益”。哪怕你一直在输（return 是负数），也可能出现“比平均水平输得少”的一局，这局的 advantage 仍然是正的，策略依然应该提高那局里动作的概率。

4. PG loss 是什么：为了自动微分构造的 surrogate loss
深度学习框架默认做的是“最小化 loss”，所以我们把“最大化 $J$”改写成一个等价梯度方向的 surrogate loss：
$$\mathcal{L}_{PG}(\theta) = -\mathbb{E}\left[\sum_t A_t \log \pi_\theta(a_t|s_t)\right]$$
实现里你要记住一句话：

采样出来的 ${s_t,a_t,A_t}$ 当常数（stop-grad），让梯度只通过 $\log \pi_\theta$ 回传。

这也是为什么很多框架会对 advantages / old_logprobs 做 detach()。
4.1 PG vs Supervised Learning：为什么 PG loss 看起来像“加权交叉熵”
一个非常好用的对比（来自 Karpathy 的 RL 笔记）：

监督学习（交叉熵）最大化对数似然：$\sum_i \log p(y_i|x_i)$

$y_i$ 是固定的 ground-truth label，数据集是静态的。


策略梯度（PG）最大化“带权重的对数似然”：$\sum_i A_i \log p(y_i|x_i)$

但这里没有 label，$y_i$ 是你从当前策略里采样出来的 action（token/动作），$A_i$ 是 credit assignment（优势/回报信号）。



所以你可以把 PG loss 视作一种“自采样数据上的加权 MLE”，权重就是 advantage。

5. 为什么 PPO 公式里看起来“没有 log”：重要性采样 ratio 来了
现实训练里，采样很贵。你通常会：

用旧策略 $\pi_{old}$ rollout 一批数据（轨迹/response）；
在同一批数据上做多次更新（多 epoch）。

这意味着你在优化新策略 $\pi_\theta$ 时，数据分布却来自 $\pi_{old}$。于是引入 重要性采样比率：
$$r_t(\theta) \doteq \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)}$$
于是 CPI/TRPO 常写的 surrogate objective 是：
$$L_{CPI}(\theta) = \mathbb{E}_{\pi_{old}}[r_t(\theta)A_t]$$
5.1 LLM-RL 的一个实现关键：token-level ratio 与 seq-level ratio
在 LLM-RL 里，$a_t$ 是 token，$s_t$ 是“prompt + 已生成前缀”。因此 ratio 默认是 token-level：
$$r_t=\exp(\log\pi_\theta(a_t|s_t)-\log\pi_{old}(a_t|s_t))$$
如果你试图构造“整段输出”的序列级 ratio，那么它会变成连乘（对数空间是累加）：
$$r_{\text{seq}}=\prod_t r_t=\exp\left(\sum_t (\log\pi_\theta-\log\pi_{old})\right)$$
序列级 ratio 在长序列上数值会非常极端（爆炸/下溢），也会让优化目标对“长度”异常敏感。所以你会看到大多数工程实现（verl/openrlhf/trl 等）更倾向于：

用 token-level ratio 做 surrogate；
再用某种聚合（token mean / seq mean / group mean）把它变成一个标量 loss。

这也是为什么我一直强调：你看到的每一个 loss/KL/entropy 数值，都必须问清楚它是在哪个维度聚合出来的。
一个很关键的结论（也是“从 PG 到 TRPO 到 PPO”的桥）：
在 $\theta=\theta_{old}$ 附近，$L_{CPI}$ 的一阶梯度方向与 vanilla PG 一致。
这也解释了“PPO 公式为什么看起来没有 log”：

你在公式里看到的是 $r_tA_t$；
但代码里你仍然在算 r = exp(logp_new - logp_old)；
log 并没有消失，只是被你藏进 ratio 的计算里了。


6. TRPO：用 KL trust region 明确约束“更新别太猛”
仅最大化 $L_{CPI}$ 仍可能出现“策略一步跨太大导致性能崩”的问题。TRPO 的做法是加一个 trust region 约束：
$$\max_\theta\ \mathbb{E}_{\pi_{old}}[r_t(\theta)A_t] \quad \text{s.t.}\quad \mathbb{E}_{\pi_{old}}[KL(\pi_\theta|\pi_{old})] \le \delta$$
直觉：在“以旧策略为中心”的 KL 球里找一个最好的更新。
难点：这个约束优化更接近二阶优化（共轭梯度等），不太容易直接放进你熟悉的 mini-batch SGD 训练循环里。
6.1 方向别搞错：训练里你监控的 KL 往往不是你写在论文里的 KL
TRPO 的约束形式写成 $KL(\pi_\theta|\pi_{old})$ 还是 $KL(\pi_{old}|\pi_\theta)$，在理论推导里可以通过近似互相转换（小步近似下它们都接近同一个二次型）。但在工程里你通常监控的是一种 采样估计，它的方向往往是：
$$\text{approx_kl}(\pi_{old}|\pi_\theta)\approx \mathbb{E}_{a\sim\pi_{old}}[\log\pi_{old}(a|s)-\log\pi_\theta(a|s)]$$
它和 full-distribution 的 KL 不等价，只是“足够便宜且能反映漂移幅度”的 proxy。你做调参时的要点不是“哪一个更正确”，而是：

明确你日志里打印的 KL 估计到底是哪一种；
用同一种定义做横向对比和阈值判断；
结合 clip_fraction/entropy/reward 一起看，别只盯 KL。


7. PPO-clip：用 clip 做一个“好实现”的 TRPO 近似
PPO-clip 用一个工程上非常好实现的方式近似 trust region：
$$L_{CLIP}(\theta)=\mathbb{E}\left[\min\left(r_t(\theta)A_t,\ \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\cdot A_t\right)\right]$$
你可以把它理解成：

$r_t$ 代表新旧策略概率变化倍率；
clip 把倍率限制在 $[1-\epsilon, 1+\epsilon]$；
min 让目标函数在“过度乐观”的那一侧被截平，从而更稳。

7.1 最重要的 case-by-case 结论（建议背下来）

$A_t&gt;0$（好动作）

若 $r_t &gt; 1+\epsilon$：上涨太猛，会被 clip 截平（梯度被压住）。


$A_t&lt;0$（坏动作）

若 $r_t &lt; 1-\epsilon$：下降太猛，会被 clip 截平（梯度被压住）。



一句话总结：

PPO-clip 主要限制“朝着对优势有利的方向”更新过头。

7.2 PPO-clip 的本质取舍：引入偏差换稳定性（不是免费的午餐）
clip 的效果可以理解成一个经典的 bias-variance tradeoff：

不 clip：目标更接近“真实的”重要性采样形式，但 ratio 一旦变大，方差会爆，训练很容易崩。
clip：对“过大更新”的样本把目标截平，显著降低方差与训练崩溃概率，但代价是引入偏差（你不再严格优化原目标）。

所以你在日志里看到 clip_fraction 很高时，不要只想着“更稳了”，而要问：

是不是我更新太激进（LR/epoch/batch/adv scale）导致大量样本触发剪裁，学习实际上被卡住？
是不是 reward 设计在诱导策略做极端动作，从而把 ratio 推爆？

这也是为什么 PPO 的“好用”来自一个非常工程化的前提：你得把更新幅度控制在一个合理范围里，让 clip 只在“救命”时生效，而不是常态。

8. PPO loss 为什么经常在 0 附近震荡：一个极简 case study
policy_gradient.ipynb 里用一个很实用的 toy 函数把 PPO-clip 的 per-sample 项拆开看：
import numpy as npdef ppo_terms(A, r, eps=0.2):    A = np.asarray(A, dtype=float)    r = np.asarray(r, dtype=float)    r_clip = np.clip(r, 1 - eps, 1 + eps)    term_pg = r * A    term_clip = np.minimum(term_pg, r_clip * A)    return A, r, r_clip, term_pg, term_clip
它帮你快速建立这样一个判断：

当“正优势样本多对应到 $r\ge 1$、负优势样本多对应到 $r\le 1$、且很少触发不利剪裁分支”时，$L_{clip}$ 往往偏正（最小化形式就是偏负）。
反过来，当正优势样本概率被压低、负优势样本概率大涨且被按更差的一边记账时，目标会偏负。

如果你把 advantage 做了零均值/标准化，那么在纯 on-policy（$\pi_{old}=\pi_\theta$）时 ratios 都是 1，PPO 的 PG 项也会很自然地在 0 附近摆动，这并不奇怪。

8.1 优势标准化（whiten）会让 loss 更“接近 0”，但不等于没有梯度
实践里经常会对 advantage 做标准化（whiten）：
$$\tilde A_t=\frac{A_t-\mathbb E[A]}{\text{Std}(A)}, \quad L_{\text{pg}}=-\mathbb E[\tilde A_t\log\pi_\theta(a_t|s_t)]$$
当 $\mathbb E[\tilde A]=0$ 时，$L_{\text{pg}}$ 的数值可以写成协方差形式：
$$\mathbb{E}[\tilde{A} \log \pi] = \operatorname{Cov}(\tilde{A}, \log \pi) + \mathbb{E}[\tilde{A}] \mathbb{E}[\log \pi] = \operatorname{Cov}(\tilde{A}, \log \pi)$$
这带来一个很重要的观测口径：

loss 的正/负，更多反映的是“策略是否更偏好优势动作”的相关性，而不是“任务好坏的绝对标尺”。
看到 loss 接近 0，并不能推出 grad=0 或“没学到”，更常见的是“尺度被重新定标/中心化了”。


9.（视频 03）GRPO loss 怎么看：为什么 loss=0 不等于 grad=0
GRPO/Group-based 方法的核心训练形态是：每个 prompt 采样一组（group）输出，用组内相对分数构造 advantage（常见是中心化/标准化），再做一个 PPO-clip 风格的 surrogate + KL anchor。
一种常见的写法是：
$$\mathcal{J}_{GRPO}(\theta)= \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min \left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}\left(r_{i,t}(\theta), 1-\varepsilon, 1+\varepsilon\right)\hat{A}_{i,t} \right) - \beta D_{KL}(\pi_\theta|\pi_{ref})$$
其中 $r_{i,t}$ 就是 token-level ratio：$r_{i,t}=\exp(\log\pi_\theta-\log\pi_{\theta_{old}})$。
为什么经常会看到 “loss≈0”？

如果你把奖励/优势做了组内中心化（mean=0），那么策略项在“数值均值”意义上很容易接近 0；
但梯度取决于相关性（协方差项），并不等价于 0。

所以看 GRPO 训练日志时，更稳的做法是把 policy_loss 当作“当前更新信号的一种投影”，而把 reward/成功率 + KL/clip/entropy 等稳定性指标当作主监控项。

10. 最小可复现代码：CartPole 的 REINFORCE（对应 notebook）
下面这个实现来自 policy_gradient.ipynb 的核心代码（做了轻微排版），它能帮助你把“公式里的每一项”对应到代码里的每一步：
import gymnasium as gymimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.distributions import Categoricalimport numpy as npLEARNING_RATE = 0.01GAMMA = 0.99NUM_EPISODES = 200DEVICE = torch.device(&quot;cuda:3&quot; if torch.cuda.is_available() else &quot;cpu&quot;)class Policy(nn.Module):    def __init__(self, state_dim, action_dim):        super().__init__()        self.net = nn.Sequential(            nn.Linear(state_dim, 128),            nn.ReLU(),            nn.Linear(128, action_dim),            nn.Softmax(dim=-1),        )    def forward(self, x):        return self.net(x)class ReinforceAgent:    def __init__(self, state_dim, action_dim):        self.policy = Policy(state_dim, action_dim).to(DEVICE)        self.optim = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)        self.saved_log_probs = []        self.rewards = []    def select_action(self, state):        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)        probs = self.policy(state)        dist = Categorical(probs)        action = dist.sample()        self.saved_log_probs.append(dist.log_prob(action))        return action.item()    def finish_episode(self):        # 1) 计算 return-to-go（折扣回报）        R = 0.0        returns = []        for r in reversed(self.rewards):            R = r + GAMMA * R            returns.insert(0, R)        returns = torch.tensor(returns, device=DEVICE)        # 2) 标准化 returns（等价于一种简单 baseline/方差控制）        returns = (returns - returns.mean()) / (returns.std() + 1e-6)        # 3) 构造 PG loss：-log_prob * return（越好越提升概率）        policy_loss = []        for log_prob, Rt in zip(self.saved_log_probs, returns):            policy_loss.append(-log_prob * Rt)        loss = torch.cat(policy_loss).sum()        # 4) 一次反传更新        self.optim.zero_grad()        loss.backward()        self.optim.step()        self.rewards.clear()        self.saved_log_probs.clear()        return float(loss.item())env = gym.make(&quot;CartPole-v1&quot;)state_dim = env.observation_space.shape[0]action_dim = env.action_space.nagent = ReinforceAgent(state_dim, action_dim)episode_rewards, episode_losses = [], []for i in range(NUM_EPISODES):    state, _ = env.reset()    ep_reward = 0.0    for t in range(500):        a = agent.select_action(state)        state, r, done, _, _ = env.step(a)        agent.rewards.append(r)        ep_reward += r        if done:            break    episode_losses.append(agent.finish_episode())    episode_rewards.append(ep_reward)    if i % 50 == 0:        print(i, ep_reward, np.mean(episode_rewards[-50:]))
读这段代码时，你可以逐行对照公式：

saved_log_probs 就是 $\log \pi_\theta(a_t|s_t)$；
returns（标准化后）扮演了 $A_t$ 的角色；
loss = -log_prob * A 就是 policy gradient surrogate loss。


11. 从 CartPole 到 LLM-RL：一个 REINFORCE + KL penalty 的玩具对齐脚本
仓库里有个非常直观的脚本：deep_RL/scripts/reinforce_align.py。
它做的事情可以一句话概括：

用语言模型采样一段文本（相当于 rollout）
用某个 reward function 给这段文本打分
用 REINFORCE 形式更新模型，并加一个 对 reference model 的 KL 正则 防止漂移

代码结构上最关键的两行就是：
$$\mathcal{L} \approx -(\text{logprob}) \cdot (\text{reward}) ;+; \beta \cdot KL(\pi_\theta|\pi_{ref})$$
这也能帮助你理解为什么 LLM-RL 训练里会出现：

policy（PG）项
KL 项
以及“为什么看起来 loss 在每轮都变了”

注意：这是 toy demo，目的不是追求工程最优，而是帮助你建立“公式-代码”映射。

12. 下一步读什么：把推导接到 Agentic RL / GRPO / verl 工程
如果你是奔着 Agentic RL（比如 deep research agent）来的，我建议你按这个顺序继续看仓库：

代理目标与 on-policy/off-policy：agentic_rl/deep_RL/objective_代理loss.ipynb
PPO 从零实现（建立直觉，不一定真训练）：agentic_rl/deep_RL/ppo/ppo_from_scratch.ipynb
verl 的 objective 与 loss 聚合（你上一篇 PG loss 组件文章也讲了很多）：agentic_rl/verl/objectives/objectives_loss.ipynb、agentic_rl/verl/objectives/agg_loss.ipynb
任务侧（deep research / search agent）：agentic_rl/tasks/deepresearch.ipynb、agentic_rl/tasks/search-agent.ipynb

最后提醒一句：做 agentic RL 不一定先从“微调 LLM”开始。很多深研究任务更有效的路径是：

先把评测闭环（reward/metrics）做扎实；
再决定训练的是“LLM 本体”还是“agent 决策层”（工具调用/检索/规划策略）。

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>PPO</tag>
        <tag>RLHF</tag>
        <tag>Policy Gradient</tag>
        <tag>TRPO</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：分布视角理解 SFT 与 RL（Forward/Reverse KL、分布与奖励）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9A%E5%88%86%E5%B8%83%E8%A7%86%E8%A7%92%E7%90%86%E8%A7%A3%20SFT%20%E4%B8%8E%20RL%EF%BC%88Forward-Reverse%20KL%E3%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E5%A5%96%E5%8A%B1%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] 10 分布的视角理解 LLM 的 SFT 训练和 RL 训练，Forward/Reverse KL，分布与奖励】（BV1WvrGBGEbf）。
我会按“分布匹配（distribution matching）”的视角，把 SFT 与 RL（RLHF/RLVR/RFT/GRPO/DAPO…）放到同一个坐标系里解释清楚：

SFT 本质是 forward KL：用数据分布 $P_{\text{data}}$ 去“覆盖”模型分布 $\pi_\theta$（mode-covering）。
带 KL 正则的 RL 本质是 reverse KL：用模型分布 $\pi_\theta$ 去“追逐”一个由 reward 定义的目标分布 $P^*$（mode-seeking）。
你在工程里看到的 KL、entropy、以及“reward in loss vs in reward”，都只是这套分布视角的不同落地方式。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


PPO&#x2F;GRPO 的 loss 组件（含 KL&#x2F;entropy&#x2F;聚合）


RL4LLM 最小闭环：reward + PG&#x2F;KL 的工程细节


Reward Model：BT&#x2F;MLE 的概率建模视角


分布一致性：token-in-token-out


配套仓库（你本地已下载）里，本文主要对齐这些笔记：

概率分布视角：SFT forward KL vs RL reverse KL（含 GMM 例子）

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/sft_rl_fkl_rkl.ipynb


KL 的数值含义与单位（nats/token、ppl 与 KL 的关系、pytorch 的 forward KL 计算）

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/training_practices/kl_数值内涵.ipynb


工程里 KL “in reward vs as loss”、以及 off-policy 下估计偏差的注意点（可选，偏深）

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/training_practices/review_kl.ipynb




0. 一个统一的视角：你到底在“把哪个分布推向哪个分布”
不管你做的是：

SFT（cross entropy / next token prediction）
RLHF（PPO/GRPO/RLOO/REINFORCE++）
DPO/IPO/SPIN（偏好优化）

你都可以抽象成一句话：

给定一个输入 $x$（prompt/上下文），你在学习一个条件分布 $\pi_\theta(y\mid x)$，让它在某个“目标分布”的意义下更接近最优。

区别只在于：目标分布到底是谁。
为了少绕弯，我后面用两类分布来讨论：

数据分布（人类/合成/标注数据）：$P_{\text{data}}(y\mid x)$
奖励诱导分布（由 reward + reference 共同定义）：$P^*(y\mid x)$

其中 $P^*$ 会在 RL 小节里给出闭式形式。

1. SFT：MLE 等价于最小化 Forward KL（mode-covering）
SFT 的目标是最大化对数似然（MLE）：
$$
\max_\theta\ \mathbb{E}_{(x,y)\sim P_{\text{data}}}\big[\log \pi_\theta(y\mid x)\big].
$$
把 forward KL 写出来：
$$
\begin{aligned}
D_{\mathrm{KL}}(P_{\text{data}} | \pi_\theta)
&amp;= \mathbb{E}_{(x,y)\sim P_{\text{data}}}\big[\log P_{\text{data}}(y\mid x)-\log \pi_\theta(y\mid x)\big] \\
&amp;= \underbrace{\mathbb{E}_{P_{\text{data}}}\big[\log P_{\text{data}}\big]}_{\text{数据熵（常数）}}

\underbrace{\mathbb{E}_{P_{\text{data}}}\big[\log \pi_\theta\big]}_{\text{SFT 目标}}.
\end{aligned}
$$

因为第一项和 $\theta$ 无关，所以：

最大化 SFT 的对数似然 $\Longleftrightarrow$ 最小化 $D_{\mathrm{KL}}(P_{\text{data}}|\pi_\theta)$。

1.1 为什么说 forward KL 是“mode-covering”
forward KL 的期望是在 $P_{\text{data}}$ 上取的：

只要数据分布某个区域有概率质量（哪怕很小），模型在那块给得太低，就会被惩罚。
结果是：模型倾向于“覆盖”数据分布中出现过的各种模式（多样性更强）。

这就是为什么纯 SFT 的输出往往更“像人类平均值”：

风格多、表达方式多、语气多；
但对长尾知识/稀有模式的拟合更脆弱，容易出现“形式像但事实不稳”的问题（你可以把它理解成：为了覆盖长尾，模型不得不在低密度区域硬分配概率）。

1.2 一个工程结论：SFT 更像“把分布铺开”，RL 更像“把分布削尖”
这句在 sft_rl_fkl_rkl.ipynb 里说得很直白：

SFT 在“模仿”人类分布的平均形态（宽、平缓）
RL 在“坍缩”到奖励最高的尖峰（窄、尖锐）

你后面如果看到 RL 模型变得“更固执、更单一”，不要惊讶：这是 reverse KL 的自然产物（下一节解释）。

2. 带 KL 正则的 RL：目标函数等价于最小化 Reverse KL（mode-seeking）
在 RL4LLM 里，你最常见的目标（写成 sequence-level）大概是：
$$
J(\theta)=\mathbb{E}_{y\sim \pi_\theta(\cdot|x)}[R(x,y)]-\beta D_{\mathrm{KL}}(\pi_\theta(\cdot|x)|\pi_{\text{ref}}(\cdot|x)).
$$
其中：

$R(x,y)$ 是 reward（来自 RM/verifier/规则/指标等）
$\pi_{\text{ref}}$ 常见是 SFT 模型（anchor）
$\beta$ 控制“奖励最大化”与“保持分布不跑飞”的权衡

这玩意看起来像“奖励 - KL 正则”，但它更有用的理解方式是：

它定义了一个目标分布 $P^*$，而训练就是让 $\pi_\theta$ 去逼近 $P^*$。

2.1 奖励诱导分布 $P^*$：指数加权（Gibbs / Boltzmann）
如果你把 $P^*$ 定义成：
$$
P^*(y\mid x)=\frac{1}{Z(x)}\ \pi_{\text{ref}}(y\mid x)\ \exp\Big(\frac{R(x,y)}{\beta}\Big),
$$
其中 $Z(x)$ 是归一化常数（partition function）。
那么可以推到一个非常关键的等价关系（sft_rl_fkl_rkl.ipynb 里给了详细推导）：
$$
\max_\theta J(\theta)\ \Longleftrightarrow\ \min_\theta D_{\mathrm{KL}}(\pi_\theta | P^*)\quad (\text{Reverse KL}).
$$
这句话的含义非常大：

RL 不是“在最小化一个随便写的 loss”
RL 在做的是：用 $\pi_\theta$ 去匹配一个由 reward 指定的目标分布（并且是 reverse KL）

2.2 为什么 reverse KL 是“mode-seeking”
reverse KL 的期望是在 $\pi_\theta$ 上取的：
$$
D_{\mathrm{KL}}(\pi_\theta|P^*)=\mathbb{E}_{y\sim \pi_\theta}\Big[\log \pi_\theta(y|x)-\log P^*(y|x)\Big].
$$
这会产生一个很强的倾向：

如果某些区域奖励低、$P^*$ 很小，那么只要 $\pi_\theta$ 不去采样那块（概率趋近 0），那块就几乎不会贡献 loss。
所以策略会把概率质量集中到少数高 reward 的区域（尖峰），形成“坍缩/削尖”。

这就是为什么：

RL 后分布会变窄、熵降低（更确定、更固执）
同时也解释了 reward hacking：只要 RM/verifier 有漏洞，策略会把概率质量挤到漏洞区域，把 $P^*$ 的尖峰“造出来”

2.3 KL 约束的工程意义：限制在 RM 的可信域附近
上面 $P^*$ 里有 $\pi_{\text{ref}}$。这不是数学装饰，而是工程护栏：

RM 是在某个数据分布上训练出来的，只在那附近可靠
让策略偏离太远，RM 打分就可能失真（OOD）
用 KL 把策略锚在 ref 附近，相当于把探索限制在“RM 可信域”

这件事我在 RM 那篇里也强调过：KL 约束更多是在保护“评估系统”的有效性，而不只是保护语言质量。

3. Forward vs Reverse KL：一个经典直觉（GMM 例子）
你可以用一个“双峰分布 vs 单峰近似”的例子建立直觉：

target 分布 $P$ 是双峰混合高斯（两个 mode）
model 分布 $Q_\theta$ 是单峰高斯（只能选一个 mode 或者折中）


最小化 forward KL：$D_{\mathrm{KL}}(P|Q_\theta)$

因为期望在 $P$ 上取，$Q$ 必须在两个峰附近都给出足够概率，否则会被惩罚
结果通常是：$Q$ 会把方差拉大，去“覆盖”两个峰（mode-covering）


最小化 reverse KL：$D_{\mathrm{KL}}(Q_\theta|P)$

因为期望在 $Q$ 上取，$Q$ 只需要把概率质量放在一个峰上就能获得较小 KL
结果通常是：$Q$ 会选择其中一个峰并收缩（mode-seeking）



把它映射回 LLM：

SFT（forward KL）更倾向覆盖多样性
RL（reverse KL）更倾向追逐高 reward 的尖峰

这也是为什么“只靠 SFT 很难得到极致对齐/极致偏好”，而“只靠 RL 很容易变单一且 reward-hack”：它们本质上在优化不同方向的 KL。

4. KL 在 LLM 里怎么算：token-level、nats/token、以及 ppl 的关系
这一节的目标是：让你能读懂训练日志里的 KL 数值到底是什么量纲、为什么会随聚合方式变化。
4.1 KL 的单位：nats/token（以及它和 ppl 的指数关系）
kl_数值内涵.ipynb 里给了一个很实用的等式：
$$
D_{\mathrm{KL}}(P|Q)=H(P,Q)-H(P),
$$
以及 perplexity 的关系：
$$
\frac{\mathrm{ppl}(P,Q)}{\mathrm{ppl}(P)}=\exp\big(D_{\mathrm{KL}}(P|Q)\big).
$$
工程直觉：

你看到的 KL（如果是按 token 平均）就是“每个 token 平均多付出了多少信息量”
exp(KL) 就是 perplexity 的倍率变化

4.2 forward KL（全词表求和）vs sampled KL（只用 logprob 差）
严格的 forward KL（下一 token 分布）是对词表求和：
$$
D_{\mathrm{KL}}(P|Q)=\sum_v P(v)\,[\log P(v)-\log Q(v)].
$$
在 PyTorch 里常见写法（来自 notebook）：
log_q = log_softmax(logits, dim=-1)        # Qlog_p = log_softmax(ref_logits, dim=-1)    # Pkl = F.kl_div(log_q, log_p, reduction=\&quot;none\&quot;, log_target=True)
但在大模型 RL 训练里，很多时候不会每步都算“全词表 KL”（太贵），而是用更便宜的 sampled proxy：

在 rollout 采样到的 token 上，记录 logp_new - logp_ref
聚合成 token-level 或 sequence-level 的平均

这类 proxy 更接近 $D_{\mathrm{KL}}(\pi_\theta|\pi_{\text{ref}})$（reverse KL）的 Monte Carlo 估计。
因此你在日志里看到的 KL 需要先问清楚两件事：

是 forward 还是 reverse（期望在哪个分布上）
是全词表 KL 还是 sampled proxy（计算口径完全不同）


5. “KL in reward” vs “KL as loss”：它们不是口味差异，是估计器差异
你会看到两类常见写法：

PPO 风格：把 KL 惩罚写进每步 reward（in reward）
GRPO/DAPO 风格：把 KL 写成独立 loss（as loss）

review_kl.ipynb 里还提到了一类常见的 “k3” 形式（非负、凸、二阶匹配）：
$$
k_3(x)=\frac{\pi_{\text{ref}}(x)}{\pi_\theta(x)}-1-\log \frac{\pi_{\text{ref}}(x)}{\pi_\theta(x)}.
$$
这些写法背后的关键不是“哪个更优雅”，而是：

你在用哪个 KL 的方向（mode-seeking vs mode-covering）
你在用哪个估计器（on-policy vs off-policy、是否引入 importance ratio）
你在用哪个聚合口径（token/seq/group mean/sum）

所以我建议你把“KL 放哪”理解成：选择一个在你当前采样/并行/代价约束下可实现、且数值稳定的估计器。
（这部分展开会很深；如果你后面要细抠 GRPO/DAPO 的 KL 估计偏差，再回看 review_kl.ipynb 会更合适。）

6. 对 deep research / agentic RL 的落地启示：reward 其实是在“塑形目标分布”
当你接受了 $P^*(y|x)\propto \pi_{\text{ref}}(y|x)\exp(R/\beta)$ 这个视角之后，reward 设计就变得更清晰：

reward 不是“让模型变聪明”的魔法分数
reward 是在改变目标分布的形状：你在用 $\exp(R/\beta)$ 对参考分布做指数加权

于是你会自然地得到两个工程原则：

reward 越尖（极端偏好/离散 0-1/强 bias），$P^*$ 越尖，策略越容易坍缩
reward 越不可靠（verifier 漏洞/启发式偏差），$P^*$ 的尖峰越可能落在漏洞上 → reward hacking

因此对 deep research agent，我更建议你：

优先用可验证信号（引用可追溯、事实一致性、覆盖率、成本、时延）
把不可验证的“写作风格/逻辑性”放在次级 reward，并做分层/多目标
始终用 token-in-token-out 保证分布一致性（否则你连 “$\pi_\theta$ 产生了什么” 都说不清）


7. 小结（你只需要记住的 6 句话）

SFT 的 MLE 等价于最小化 forward KL：$D_{\mathrm{KL}}(P_{\text{data}}|\pi_\theta)$（mode-covering）。
带 KL 正则的 RL 等价于最小化 reverse KL：$D_{\mathrm{KL}}(\pi_\theta|P^*)$（mode-seeking）。
$P^*$ 的形状由 reward 决定：$P^* \propto \pi_{\text{ref}}\exp(R/\beta)$，reward 就是在塑形目标分布。
forward KL 更保多样性但更像“平均人类”；reverse KL 更会削尖分布但更容易坍缩与 reward hacking。
训练日志里的 KL 必须先问清楚口径（方向/估计器/聚合/是否 sampled）。
真正工程上最重要的是：把策略限制在评估系统（RM/verifier）可信域附近，让“优化”这件事有意义。

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>RLHF</tag>
        <tag>KL</tag>
        <tag>概率建模</tag>
        <tag>SFT</tag>
        <tag>Forward KL</tag>
        <tag>Reverse KL</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9A%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA%EF%BC%88PG%20Loss%E3%80%81TRPO%E3%80%81PPO-Clip%EF%BC%89/</url>
    <content><![CDATA[这一页是 Agentic RL 系列的导航与阅读路线图。定位是：用最少的前置，把你带到“看懂 PPO/PG loss 的公式，能对齐到代码实现，并能读懂训练日志与稳定性问题”。
配套视频与资料：

视频 01（工程组件）：BV1KFpczQEkA
视频 02（推导主线）：BV12NHJzTEoP
视频 03（PG 补充 / GRPO / 优势标准化 / On-policy）：BV18hsFzbEKJ
视频 04（REINFORCE 4 LLM / reward 设计 / PG+KL loss 细节）：BV1Ya1LB1EDM
视频 05（vLLM 推理参数 / 显存分析 / max_num_batched_tokens）：BV1QnSFBkEZU
视频 06（DeepSeekMath-v2 自我验证 / verifier + meta-verifier / 迭代验证与微调）：BV1AaSTBEEeS
视频 07（RLVR 的边界 / Base vs RL / pass@k / PPL / vLLM 评测细节）：BV1pWSvBtEAk
veRL（verl 框架 / 训练参数理解：PPO &amp; GRPO / batch size / KL &amp; entropy）：BV1DZL1zNEN2
veRL（verl 核心算法 / GRPO、RLOO、REINFORCE++ / Baseline）：BV1d4Yvz4EXA
veRL（FSDP SFT trainer / SFT vs RL / 交叉熵损失 / loss mask / LR scheduler）：BV1CkJgzAEAG
veRL（FSDP SFT trainer 补充 / teacher forcing / shift labels-logits / loss mask）：BV1eWjtzbEdP
veRL（MultiTurn Tool Use / Coding Agent SFT / Cold Start for RL）：BV19PmwB5ERd
veRL（Tokenizer 编解码非对称性 / token-in-token-out / Agent loop）：BV1b2pDzYEY2
Reward Model insights（Bradley-Terry / MLE / 深度学习）：BV1z4vkBBEgD
视频 10（分布视角：SFT vs RL / Forward-Reverse KL / 分布与奖励）：BV1WvrGBGEbf
视频 11（DPO：KL 正则 RL 推导 / 隐式 RM / 缺陷与不足）：BV1N16ZBuERA
视频 12（verl infra：AgentLoop / AgentLoopManager / hybrid 推训）：BV135zrBaEEU
视频 13（verl infra：AgentLoop 代码串讲 / multi-turn 推理 / ray trainer）：BV1MizSBJEbi
视频 14（verl AgentLoop：全流程与计算细节 / async rollout / 状态机 / tool-interaction）：BV18d6sBpEZq
配套仓库：wdkns/modern_genai_bilibili/agentic_rl


1. 两条阅读路径（按你的目标选）
如果你希望先把“公式链路”打通：


02 从 PG 到 TRPO 到 PPO-Clip（推导与代码对齐）


01 PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）


04 REINFORCE 4 LLM（Reward 设计 &#x2F; PG+KL loss 细节 &#x2F; KL 计算坑点）


09 Reward Model Insights（Bradley-Terry &#x2F; MLE &#x2F; 深度学习）


10 分布视角理解 SFT 与 RL（Forward&#x2F;Reverse KL、分布与奖励）


11 重新理解 DPO（KL 正则 RL &#x2F; 隐式 RM &#x2F; 缺陷与不足）


05 vLLM 推理参数与显存&#x2F;吞吐调优（max_num_batched_tokens）


06 DeepSeekMath-v2：Verifier + Meta-Verifier 与 Self-Verification


07 RLVR 的边界：Base vs RL、pass@k、PPL 与 vLLM 评测细节


veRL（verl）训练参数理解：PPO &amp; GRPO、Batch Size、KL &amp; Entropy


veRL 核心算法：GRPO &#x2F; RLOO &#x2F; REINFORCE++ 与 Baseline 设计


veRL：FSDP SFT Trainer（交叉熵 &#x2F; loss mask &#x2F; scheduler）


veRL：SFT Trainer 补充（Teacher Forcing &#x2F; Shift &#x2F; Loss Mask 对齐）


veRL：MultiTurn Tool Use &#x2F; Coding Agent SFT（Cold Start for RL）


12 veRL Infra：AgentLoop、AgentLoopManager 与 hybrid 推训


13 veRL Infra：AgentLoop 代码串讲（multi-turn 推理 &#x2F; Ray trainer）


14 veRL AgentLoop 全流程与计算细节（async rollout &#x2F; 状态机 &#x2F; tool-interaction）


Tokenizer 非对称性与 Token-in-Token-out（RL 训练崩溃的根因）


如果你希望先能写/改训练代码（工程优先）：


01 PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）


02 从 PG 到 TRPO 到 PPO-Clip（推导与代码对齐）


04 REINFORCE 4 LLM（Reward 设计 &#x2F; PG+KL loss 细节 &#x2F; KL 计算坑点）


09 Reward Model Insights（Bradley-Terry &#x2F; MLE &#x2F; 深度学习）


10 分布视角理解 SFT 与 RL（Forward&#x2F;Reverse KL、分布与奖励）


11 重新理解 DPO（KL 正则 RL &#x2F; 隐式 RM &#x2F; 缺陷与不足）


05 vLLM 推理参数与显存&#x2F;吞吐调优（max_num_batched_tokens）


06 DeepSeekMath-v2：Verifier + Meta-Verifier 与 Self-Verification


07 RLVR 的边界：Base vs RL、pass@k、PPL 与 vLLM 评测细节


veRL（verl）训练参数理解：PPO &amp; GRPO、Batch Size、KL &amp; Entropy


veRL 核心算法：GRPO &#x2F; RLOO &#x2F; REINFORCE++ 与 Baseline 设计


veRL：FSDP SFT Trainer（交叉熵 &#x2F; loss mask &#x2F; scheduler）


veRL：SFT Trainer 补充（Teacher Forcing &#x2F; Shift &#x2F; Loss Mask 对齐）


veRL：MultiTurn Tool Use &#x2F; Coding Agent SFT（Cold Start for RL）


12 veRL Infra：AgentLoop、AgentLoopManager 与 hybrid 推训


13 veRL Infra：AgentLoop 代码串讲（multi-turn 推理 &#x2F; Ray trainer）


14 veRL AgentLoop 全流程与计算细节（async rollout &#x2F; 状态机 &#x2F; tool-interaction）


Tokenizer 非对称性与 Token-in-Token-out（RL 训练崩溃的根因）



2. 系列内容地图（你看完应该掌握什么）
2.1 你会搞清楚的 5 个问题

PG loss 是什么，为什么它不是“监督学习意义上的 loss”，为什么更该看 reward curve。
log π 从哪来：log-trick 让“对分布求导”变成“对 logprob 求导”。
ratio r=π/π_old 从哪来：重要性采样让你能复用旧策略采样的数据做多步更新。
TRPO 在约束什么：KL trust region 的直觉与局限（实现成本高）。
PPO-clip 在做什么近似：clip 生效的 case-by-case，为什么能稳住训练，以及它的副作用（探索/概率上限）。

2.2 你会对齐到代码的关键点

r = exp(logp_new - logp_old)：PPO 公式看起来“没 log”，但实现里 log 只是被藏进 ratio 计算了。
advantages.detach() / old_logp.detach()：采样得到的信号当常数，梯度只通过当前策略回传。
clip fraction / KL / entropy：稳定性指标怎么看、怎么联动调参。
agg_loss（token/seq/group mean/sum）：聚合方式如何改变梯度尺度，从而改变你对学习率和系数的“手感”。


3. 文章列表（按主题）

推导主线（PG -&gt; TRPO -&gt; PPO-clip）


Agentic RL：从 PG 到 TRPO 到 PPO-Clip（推导与代码对齐）



工程组件（PPO-clip / Dual-clip / Entropy / KL / 聚合）


Agentic RL：PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）



RL4LLM 最小闭环（REINFORCE + reward + KL）


Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）



Reward Model / Preference Modeling（Bradley-Terry / MLE / RM 边界）


Agentic RL：Reward Model Insights（Bradley-Terry、MLE 与深度学习）



分布视角：SFT vs RL（Forward/Reverse KL、分布与奖励）


Agentic RL：分布视角理解 SFT 与 RL（Forward&#x2F;Reverse KL、分布与奖励）



偏好优化：DPO（KL 正则 RL / 隐式 RM / 缺陷与不足）


Agentic RL：重新理解 DPO（KL 正则 RL、隐式奖励模型与缺陷）



推理部署与吞吐调优（vLLM）


Agentic RL：vLLM 参数配置、显存分析与性能调优（max_num_batched_tokens）



Reasoning 的评测闭环（Verifier / Meta-Verifier / Self-Verification）


Agentic RL：DeepSeekMath-v2 自我验证（Verifier &amp; Meta-Verifier）



RLVR 的边界与评测指标（Base vs RL / pass@k / PPL / vLLM logprob 细节）


Agentic RL：RLVR 的边界（Base vs RL、pass@k、PPL 与 vLLM 评测细节）



veRL（verl）训练框架与参数理解（PPO &amp; GRPO / batch size / KL &amp; entropy / loss 聚合）


Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）



veRL（verl）核心算法与 Baseline（GRPO / RLOO / REINFORCE++）


Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计



SFT 工程（FSDP trainer / 交叉熵 / loss mask / scheduler）


Agentic RL：veRL FSDP SFT Trainer（SFT vs RL、交叉熵损失、Loss Mask、LR Scheduler）


Agentic RL：veRL FSDP SFT Trainer 补充（Teacher Forcing、Shift Labels&#x2F;Logits、Loss Mask）



Tool Use Agent（MultiTurn / Coding Agent / Cold Start for RL）


Agentic RL：veRL MultiTurn Tool Use 与 Coding Agent SFT（Cold Start for RL）



veRL Infra（AgentLoop / Async Rollout / Ray Trainer / multi-turn 推理）


Agentic RL：veRL Infra AgentLoop（AgentLoopManager、Async Rollout 与 Hybrid 推训）


Agentic RL：veRL Infra AgentLoop 代码串讲（Multi-turn 推理与 Ray Trainer）


Agentic RL：veRL AgentLoop 全流程与计算细节（Async Rollout、状态机、Tool-Interaction）



Tokenizer / Token-in-Token-out（RL 训练稳定性）


Agentic RL：Tokenizer 编解码非对称性与 Token-in-Token-out（RL 训练崩溃的根因）





4. 下一步（面向 Agentic RL / Deep Research）
如果你的最终目标是 agentic RL 用于 deep research 任务，建议你在“先看懂 PPO/PG loss”之后尽快做两件事：

明确你的“研究型任务”到底在优化什么：质量、引用覆盖、事实一致性、成本、时延，哪些能被可靠评测，哪些只能做 proxy。
决定训练对象：先训 agent 决策层（检索/工具/规划策略）还是直接训 LLM token policy（更吃数据/算力/评测闭环）。

后续如果你要把这个系列扩展到 PPO/GRPO 在 LLM-RL 框架里的工程落地（rollout、reward manager、objective、聚合与调参），我可以继续按同样方式把 verl / openrlhf 的链路拆开整理成可复现笔记。
]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>PPO</tag>
        <tag>Policy Gradient</tag>
        <tag>TRPO</tag>
      </tags>
  </entry>
  <entry>
    <title>Agentic RL：重新理解 DPO（KL 正则 RL、隐式奖励模型与缺陷）</title>
    <url>/2026/02/Agentic%20RL%EF%BC%9A%E9%87%8D%E6%96%B0%E7%90%86%E8%A7%A3%20DPO%EF%BC%88KL%20%E6%AD%A3%E5%88%99%20RL%E3%80%81%E9%9A%90%E5%BC%8F%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BC%BA%E9%99%B7%EF%BC%89/</url>
    <content><![CDATA[这篇文章对应视频：【[Agentic RL] 11 重新理解 DPO，带 KL 正则的 RL 目标函数推导，隐式的奖励模型，DPO 可能的缺陷与不足】（BV1N16ZBuERA）。
我不会把它写成“逐句视频笔记”，而是把 DPO 放回一个更稳的框架里：KL 正则的 RL 目标函数与概率建模（Bradley-Terry）。你看完应该能回答这 3 个问题：

DPO 到底在优化什么，它和 “reward - β KL” 的 RL 目标是什么关系？
为什么说 DPO 有一个“隐式的奖励模型”（implicit RM），它是什么形式？
DPO 为什么会出现一些看起来反直觉的问题（reward hacking / 过拟合 / 生成质量掉 / 长度偏置），以及你在工程里怎么防。

系列导航：


Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）


关联阅读（建议顺序）：


先把 KL-constrained RL 的分布视角打通（π* 与 P*）


BT&#x2F;MLE 的概率建模视角（你会更容易理解 DPO 的 logistic loss）


如果你最后还是要上 RL，这篇是最小闭环


配套仓库（你本地已下载）里，本文主要对齐这些笔记：

DPO 主线（含 KL-RL 目标、π*、DPO loss、缺陷讨论）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reward_model/dpo.ipynb


DPO 补充（定位：SFT 之后、RL 之前，用作 refinement）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reward_model/dpo_supp.ipynb


BT / RM 的概率建模（DPO 的“二分类”来自这里）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/reward_model/Bradley-Terry-RM.ipynb


KL-constrained RL 的分布视角（DPO 其实是这条线的一个特例）：

/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/sft_rl_fkl_rkl.ipynb




0. 先给结论：DPO 是“KL 正则 RL”的一个离线（offline）特例
你在很多地方看到的 RLHF 目标（写成 sequence-level）大概长这样：
$$
\max_{\pi}\ \mathbb{E}_{x\sim\mathcal{D}}\Big[\ \mathbb{E}_{y\sim\pi(\cdot\mid x)}\big[r(x,y)\big]\ -\ \beta\,D_{\mathrm{KL}}\big(\pi(\cdot\mid x)\ \|\ \pi_{\text{ref}}(\cdot\mid x)\big)\ \Big].
$$
这就是 dpo.ipynb 里写的 “KL-constrained reward maximization objective”。它有两个非常关键的工程含义：

你希望提高 reward（更符合偏好/更正确/更安全）。
你又不希望策略偏离 \\pi\\_&#123;\\text&#123;ref&#125;&#125; 太远（避免跑到 RM/verifier 不可靠的区域，也避免模式坍缩）。

DPO 的核心洞察是：如果你不显式训练 reward model，而是直接拿“偏好对比数据”（winner/loser）来训练 policy，那么在某些假设下，上面这个 RL 目标可以被“降维”为一个监督学习的二分类目标。
这句话要读清楚：

DPO 不是 PPO 的替代实现，它更像是：把“在线 rollout + RM 打分 + policy gradient”这条链路，用“离线偏好对比 + log-ratio 分类”替换掉。
代价是：它把很多问题“从采样/方差/不稳定”转成了“数据分布/偏好噪声/过拟合”的问题。


1. KL 正则 RL 的解析解：目标分布是 reward-tilted 的 Gibbs 分布
如果我们把上一节的目标写成最优策略 \\pi^*，那么它有一个很漂亮的解析解（这一步在 dpo.ipynb 里给出了关键式子）：
$$
\pi^*(y\mid x)=\frac{1}{Z(x)}\,\pi_{\text{ref}}(y\mid x)\,\exp\Big(\frac{r(x,y)}{\beta}\Big),
$$
其中 Z(x) 是归一化常数（partition function）。
这个式子的直觉非常直接：

参考策略 \\pi\\_&#123;\\text&#123;ref&#125;&#125; 给了你一个“可信的语言分布底座”；
reward 通过 \\exp(r/\\beta) 对这个分布做指数加权：

reward 高的区域概率被放大；
reward 低的区域概率被压低。



在 分布视角 里你可以把它读成：

你不是在“优化一个神秘 loss”，你是在让策略去拟合一个由 \\pi\\_&#123;\\text&#123;ref&#125;&#125; 和 r 共同定义的目标分布。


2. 隐式奖励模型：reward 可以由 log-ratio 反解出来
上面的解析解可以反过来写成 reward：
$$
r(x,y)=\beta\log\frac{\pi^*(y\mid x)}{\pi_{\text{ref}}(y\mid x)}+\beta\log Z(x).
$$
注意到：

\\log Z(x) 只和 x 有关，和 y 无关；
如果我们只关心“同一个 prompt 下，两个回答谁更好”，那么 \\log Z(x) 会在差分里消掉。

于是我们得到一个非常重要的结构：

reward 差分 r(x,y\\_w)-r(x,y\\_l) 本质上就是 policy vs reference 的 log-ratio 差分。

这就是所谓的 implicit reward model：你根本不需要显式训练一个 R\_\\phi(x,y)，因为在 KL 正则 RL 的最优解附近，reward 已经“编码在 log-ratio 里”了。

3. 为什么 DPO 的 loss 是 logistic regression：Bradley-Terry 一步到位
偏好数据通常长这样：对同一个 x，有一对回答：

winner：y\\_w
loser：y\\_l

Bradley-Terry（BT）建模的核心是假设：
$$
\mathbb{P}(y_w \succ y_l\mid x)=\sigma\big(r(x,y_w)-r(x,y_l)\big),
$$
其中 \\sigma(\\cdot) 是 sigmoid。
把上一节的 reward 差分用 log-ratio 替换（把 \\log Z(x) 消掉），我们可以得到 DPO 的典型目标（这也是 dpo.ipynb 里写出的那条公式）：
$$
\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})
=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}
\Big[
\log\sigma\big(
\beta\,\Delta\log\pi_\theta
-\beta\,\Delta\log\pi_{\text{ref}}
\big)
\Big],
$$
其中
$$
\Delta\log\pi_\theta
=\log\pi_\theta(y_w\mid x)-\log\pi_\theta(y_l\mid x),
\quad
\Delta\log\pi_{\text{ref}}
=\log\pi_{\text{ref}}(y_w\mid x)-\log\pi_{\text{ref}}(y_l\mid x).
$$
把它翻译成一句工程话：

你在做一个二分类：让 y\\_w 的“相对 log-ratio”大于 y\\_l；
\\beta 控制这个 margin 推得多激进；
reference 的 logprob 负责把优化约束在“可信域”附近（并且也显式进入了 margin）。


4. 从公式到代码：DPO 实现的最小骨架（LLM 版）
DPO 的实现本质上只需要两件事：

当前策略 \\pi\\_\\theta 对两条序列的 logprob（通常是 token logprob 的 sum）。
参考策略 \\pi\\_&#123;\\text&#123;ref&#125;&#125; 对两条序列的 logprob。

伪代码（关键是“同一个 prompt 的 winner/loser 要用同一个 mask 与聚合口径”）：
# x: prompt# y_w, y_l: winner/loser response tokens# model: pi_theta# ref_model: pi_ref (frozen)logp_w = sum_token_logp(model, x, y_w, mask_w)       # log pi_theta(y_w|x)logp_l = sum_token_logp(model, x, y_l, mask_l)       # log pi_theta(y_l|x)ref_logp_w = sum_token_logp(ref_model, x, y_w, mask_w)ref_logp_l = sum_token_logp(ref_model, x, y_l, mask_l)logits = beta * ((logp_w - ref_logp_w) - (logp_l - ref_logp_l))loss = -log_sigmoid(logits).mean()loss.backward()
三个实现细节我建议你显式写在日志里（否则你很难解释训练曲线）：

你用的是 sum 还是 mean（长度偏置会完全不同）。
mask 是不是严格 token-in-token-out（别把 pad/EOS 之后的 token 混进去）。
ref_logp 是不是用同一个 tokenizer / 同一个对齐方式算出来的（否则会出现“分母错位”的怪现象）。


5. DPO 的“好处”到底是什么：它把 RL 的难点换成了数据难点
和 PPO/GRPO 这种 RLHF 方式比，DPO 的优势主要是工程上的：

不需要 online rollout：不用 vLLM 跑一堆采样，不用处理采样吞吐瓶颈。
不需要显式 RM：不训练 R\_\\phi，也减少了“RM 不稳定 + KL 约束必须很强”的复杂度。
训练更像标准的 SFT：一个固定数据集上做稳定的梯度下降，调试成本低很多。

但它并不是“白捡”。它真正的隐含假设是：

你的偏好数据 \\mathcal&#123;D&#125; 足够代表“模型未来会生成的分布”，以及 BT 偏好概率确实能被某个标量 r(x,y) 解释。

一旦这个假设不成立，你会看到很多 DPO 的反直觉现象。

6. DPO 的缺陷与不足：不是“实现问题”，而是目标函数的自然后果
dpo.ipynb 的 Discussion 里点了几个我认为非常重要的问题。我用更工程化的方式把它们整理出来。
6.1 “评价能力”不等于“生成能力”：你在学打分，不一定学会决策
DPO 的梯度来自：

让 y\\_w 的相对 log-ratio 更大；
让 y\\_l 的相对 log-ratio 更小。

这更像是“学会把一对答案区分开”（ranking / scoring），而不是“在实际 generation 时做出全局最优决策”。
你可以把它类比成：

读棋谱可以让你更会评价棋，但并不保证你下棋就一定更强；
如果模型的 “evaluation” 和 “generation” 没有足够一致的表示结构，DPO 的提升会卡住，甚至出现行为退化。

6.2 Reward hacking / 过拟合：log-ratio 可以被刻意做大
这是 DPO 最容易被忽略、但最值得警惕的一点：
在 DPO loss 里，只要
$$
\frac{\pi_\theta(y_w\mid x)}{\pi_{\text{ref}}(y_w\mid x)}
$$
被推得足够大，loss 就会变小。
模型可以用一种“非常不语言模型”的方式去做大这个 ratio：把概率质量挤到 reference 认为极低概率的区域，从而制造巨大的 log-ratio。
结果可能是：

某些奇怪 token/短语被过度提权；
生成质量（流畅性/可读性）下降，但 loss 仍然很好看；
或者出现局部模式坍缩。

这不是“实现 bug”，而是 objective 的自然后果：你优化的是 log-ratio margin，而不是直接优化 “human-looking / fluent” 的 token NLL。
6.3 长度偏置（Length Bias）：sum logprob 会天然惩罚长序列
如果你用 sequence logprob 的 sum：
$$
\log\pi(y\mid x)=\sum_{t=1}^{T}\log\pi(a_t\mid s_t),
$$
那么长序列更负（更小），这会把 DPO 的 margin 结构和长度强耦合起来。
常见现象：

winner 比 loser 更长时，DPO 可能需要更大的 \\beta 才能把 margin 推过去；
你会在训练过程中看到模型开始偏向短答（或者偏向高概率的模板化答法）。

工程上常见的缓解方式是做 length-normalized logprob（mean），但这会改变目标函数，你需要在论文/对比里说清楚口径。
6.4 偏好噪声与 intransitivity：BT 的建模假设会变成上限
如果偏好数据是：

噪声大；
或者存在明显 intransitivity（A&gt;B, B&gt;C, 但 C&gt;A）；

那么 BT 形式的 logistic loss 会“变钝”（这点在 RM 那篇里已经解释过）。
你会看到：

loss 下降但行为不动；
或者为了拟合噪声，模型开始做极端 log-ratio（对应 6.2 的过拟合风险）。


7. 把 DPO 放进 Agentic RL / Deep Research：我建议你怎么用
如果你的目标是 agentic RL 用于 deep research，我建议你把 DPO 放在一个更保守的位置：

它更适合做 SFT 之后的 refinement（把“偏好”压进去，让输出更符合风格/格式/规范）。
但如果你的核心指标是 可验证正确性（引用可追溯、事实一致性、推理正确率、成本/时延），最终往往还是要回到：

verifier / rule-based reward / outcome reward；
以及更偏 on-policy 的优化（哪怕是 GRPO/RLOO 这类不训练 critic 的算法）。



你可以把 DPO 当成“更稳定的对齐步骤”，但不要把它当成“能替代整个 RL 闭环”的魔法。

8. 小结

DPO 的数学骨架来自 KL 正则 RL：reward - β KL。
解析解 \\pi^* \\propto \\pi\\_&#123;ref&#125;\\exp(r/\\beta) 给出了一个关键映射：reward 可以由 log-ratio 反解（隐式 RM）。
BT 偏好建模把它降维成监督学习：DPO loss 就是对 log-ratio margin 做 logistic regression。
DPO 的风险也来自同一个结构：log-ratio 可以被极端放大（reward hacking/过拟合），并且与长度偏置/数据噪声强耦合。
对 agentic RL/deep research：我更建议把 DPO 用作 refinement，而把“可验证指标”的提升交给 verifier + on-policy 优化闭环。

]]></content>
      <categories>
        <category>算法解析</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>Agentic RL</tag>
        <tag>强化学习</tag>
        <tag>RLHF</tag>
        <tag>KL</tag>
        <tag>Bradley-Terry</tag>
        <tag>概率建模</tag>
        <tag>DPO</tag>
        <tag>Preference Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT详解 - 双向编码器表示模型精读</title>
    <url>/2025/11/BERT/</url>
    <content><![CDATA[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
引言
BERT (Bidirectional Encoder Representations from Transformers) 是Google在2018年提出的革命性自然语言处理模型，它通过在无标注文本上进行预训练，学习深层的双向语言表示，在下游任务上取得了突破性的成果。
BERT的核心创新在于双向上下文编码，与之前的ELMo（浅层双向）和GPT（单向）不同，BERT使用Transformer编码器同时利用上下文信息，彻底改变了NLP领域的预训练范式。
背景知识
预训练语言模型的发展
在BERT之前，主流的预训练方法存在以下局限性：

单向语言模型（如GPT）：只能从左到右或从右到左进行编码，无法同时利用双向上下文
浅层双向模型（如ELMo）：虽然考虑了双向信息，但只是简单拼接左右向表示，而非深度双向，网络架构比较老，使用的RNN

为什么需要双向编码？
语言的理解往往需要同时考虑前后文信息。例如：

“银行” 这个词在 “我去银行存钱” 和 “河边的银行” 中含义完全不同
只有同时看到前后文，才能准确理解词义

论文核心思想
主要贡献

双向预训练：通过Masked Language Model (MLM) 实现真正的双向编码
统一的预训练框架：一个模型可以适应多种下游任务
迁移学习的成功：证明了大规模预训练+微调的有效性

核心架构
BERT基于Transformer的编码器部分，主要包含：
BERT = Transformer Encoder × N层

BERT-Base: 12层Transformer，768维隐藏层，12个注意力头，110M参数
BERT-Large: 24层Transformer，1024维隐藏层，16个注意力头，340M参数

预训练方法
1. Masked Language Model (MLM)
目标：预测被掩码的词汇
方法：

随机选择15%的token进行掩码
其中：

80%替换为 [MASK]
10%替换为随机token
10%保持不变



为什么不完全用[MASK]？

预训练时用[MASK]，但微调时没有[MASK]，会造成不匹配
通过随机替换，模型学会在缺少明确信号时也能预测

数学表示：
P(w_i | w_&#123;context&#125;) = softmax(W_h_i)
其中，h_i是第i个token的上下文表示
2. Next Sentence Prediction (NSP)
目标：判断两个句子是否是连续的
方法：

输入格式：[CLS] 句子A [SEP] 句子B [SEP]
50%的概率是连续句子（IsNext）
50%的概率是随机句子（NotNext）

为什么需要NSP？

许多下游任务（如问答、自然语言推理）需要理解句子间关系
MLM主要学习词级表示，NSP帮助学习句子级表示

输入表示
BERT的输入由三个embedding相加：
Input = Token Embedding + Segment Embedding + Position Embedding
Token Embedding

使用WordPiece分词
特殊token：

[CLS]：分类任务的表示
[SEP]：句子分隔符
[MASK]：掩码token
[UNK]：未知词



Segment Embedding

区分句子A和句子B
E_A = 0，E_B = 1

Position Embedding

学习的位置编码（而非Transformer的固定位置编码）
最大序列长度：512

模型架构详解
Transformer Encoder层
每个Transformer Encoder包含：


多头自注意力机制 (Multi-Head Self-Attention)
Attention(Q, K, V) = softmax(QK^T / √d_k)VMultiHead = Concat(head_1, ..., head_h)W^O


前馈神经网络 (Feed-Forward Network)
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2


残差连接和层归一化
output = LayerNorm(x + Sublayer(x))


BERT的输出

最后一层的输出：每个token的上下文表示
[CLS] token的输出：用于分类任务的句子级表示

微调策略
下游任务适配
BERT可以通过简单的修改适应各种任务：


单句分类（如情感分析）
[CLS] 句子 [SEP] → 分类层


句子对分类（如自然语言推理）
[CLS] 句子A [SEP] 句子B [SEP] → 分类层


问答任务（如SQuAD）
[CLS] 问题 [SEP] 段落 [SEP] → 起始位置 + 结束位置


序列标注（如命名实体识别）
[CLS] token1 token2 ... [SEP] → 每个token的标签


微调技巧

学习率：预训练的学习率通常较小（如2e-5）
Batch size：16或32通常效果较好
Epochs：2-4个epoch通常足够
学习率调度：线性衰减或warmup

实验结果
GLUE基准测试
BERT在11个NLP任务上取得了state-of-the-art的结果：

MNLI: 84.6% (4.6%提升)
QQP: 71.2% F1 (4.2%提升)
QNLI: 90.5% (5.1%提升)
SST-2: 93.5% (2.0%提升)
CoLA: 52.1% (5.6%提升)

SQuAD v1.1

F1: 93.2%
EM: 87.4%

消融实验

MLM的影响：移除MLM导致显著性能下降
NSP的影响：对某些任务有帮助，但不是必需的
模型大小的影响：更大的模型带来更好的性能
训练步数的影响：更多训练步数持续提升性能

代码实现
使用Hugging Face Transformers
from transformers import BertTokenizer, BertModelimport torch# 加载预训练模型和分词器tokenizer = BertTokenizer.from_pretrained(&#x27;bert-base-uncased&#x27;)model = BertModel.from_pretrained(&#x27;bert-base-uncased&#x27;)# 输入文本text = &quot;Hello, how are you?&quot;inputs = tokenizer(text, return_tensors=&#x27;pt&#x27;)# 获取模型输出with torch.no_grad():    outputs = model(**inputs)# outputs包含：# - last_hidden_state: 最后一层的隐藏状态# - pooler_output: [CLS] token的池化输出print(outputs.last_hidden_state.shape)  # [batch_size, seq_len, hidden_size]print(outputs.pooler_output.shape)      # [batch_size, hidden_size]
文本分类示例
from transformers import BertForSequenceClassificationfrom transformers import Trainer, TrainingArguments# 加载分类模型model = BertForSequenceClassification.from_pretrained(    &#x27;bert-base-uncased&#x27;,    num_labels=2  # 二分类)# 准备数据train_texts = [&quot;I love this movie&quot;, &quot;This movie is terrible&quot;]train_labels = [1, 0]# 训练参数training_args = TrainingArguments(    output_dir=&#x27;./results&#x27;,    num_train_epochs=3,    per_device_train_batch_size=16,    learning_rate=2e-5,)# 训练trainer = Trainer(    model=model,    args=training_args,    train_dataset=train_dataset,)trainer.train()
自定义BERT模型（简化版）
import torchimport torch.nn as nnfrom torch.nn import TransformerEncoder, TransformerEncoderLayerclass SimpleBERT(nn.Module):    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=12):        super().__init__()        self.embedding = nn.Embedding(vocab_size, d_model)        self.pos_encoding = nn.Parameter(torch.randn(512, d_model))                encoder_layers = TransformerEncoderLayer(            d_model=d_model,            nhead=nhead,            dim_feedforward=3072,            dropout=0.1        )        self.transformer = TransformerEncoder(encoder_layers, num_layers)            def forward(self, x, mask=None):        # x: [batch_size, seq_len]        x = self.embedding(x) + self.pos_encoding[:x.size(1)]        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model]        output = self.transformer(x, src_key_padding_mask=mask)        return output.transpose(0, 1)  # [batch_size, seq_len, d_model]
技术细节与优化
注意力机制计算
def scaled_dot_product_attention(Q, K, V, mask=None):    &quot;&quot;&quot;    Q: [batch_size, num_heads, seq_len, d_k]    K: [batch_size, num_heads, seq_len, d_k]    V: [batch_size, num_heads, seq_len, d_v]    &quot;&quot;&quot;    d_k = Q.size(-1)    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)        if mask is not None:        scores = scores.masked_fill(mask == 0, -1e9)        attention_weights = torch.softmax(scores, dim=-1)    output = torch.matmul(attention_weights, V)    return output, attention_weights
预训练损失函数
def compute_bert_loss(mlm_logits, nsp_logits, mlm_labels, nsp_labels, mlm_mask):    # MLM损失    mlm_loss = nn.CrossEntropyLoss(ignore_index=-100)(        mlm_logits.view(-1, mlm_logits.size(-1)),        mlm_labels.view(-1)    )        # NSP损失    nsp_loss = nn.CrossEntropyLoss()(        nsp_logits,        nsp_labels    )        # 总损失    total_loss = mlm_loss + nsp_loss    return total_loss, mlm_loss, nsp_loss
BERT的优缺点
优点

强大的表示能力：双向编码捕获丰富的上下文信息
通用性强：一个模型适应多种任务
迁移学习效果好：预训练+微调范式非常有效
开源可用：提供了多种规模的预训练模型

缺点

计算成本高：参数量大，推理速度慢
最大长度限制：只能处理512个token
预训练任务局限：MLM和NSP可能不是最优的预训练目标
单向生成困难：由于双向特性，不适合生成任务

BERT的后续发展
改进方向


模型压缩：

DistilBERT：知识蒸馏减小模型
ALBERT：参数共享降低参数量



效率提升：

ELECTRA：更高效的预训练任务
RoBERTa：去除NSP，优化训练策略



长文本处理：

Longformer：处理更长序列
BigBird：稀疏注意力机制



多语言扩展：

mBERT：多语言BERT
XLM：跨语言预训练



总结
BERT通过双向Transformer编码器和创新的预训练任务（MLM + NSP），为NLP领域带来了革命性的变化。其核心思想是：

双向编码：同时利用前后文信息
预训练+微调：大规模无监督预训练 + 任务特定微调
统一架构：一个模型适应多种下游任务

BERT的成功证明了大规模预训练语言模型的有效性，为后续的GPT、T5等模型奠定了基础。
参考文献


Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.


Vaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30.


Radford, A., et al. (2018). Improving language understanding by generative pre-training.


Howard, J., &amp; Ruder, S. (2018). Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146.


思考题

为什么BERT使用15%的mask比例？这个比例是否可以调整？
BERT的MLM任务中，为什么要用80% [MASK]、10% 随机、10% 不变的方式？
NSP任务对BERT的性能有多大影响？为什么RoBERTa去掉了NSP？
BERT为什么不适合生成任务？如果要用于生成，应该怎么改进？
如何理解BERT的双向编码？它与ELMo的双向有什么本质区别？

思考题答案
1. 为什么BERT使用15%的mask比例？这个比例是否可以调整？
15% mask比例的原因：

平衡学习效率与难度：如果mask比例太低（如5%），模型看到的有效训练信号太少，学习效率低；如果太高（如50%），输入信息损失过多，模型难以学习有效的语言表示。
经验最优值：15%是经过实验验证的平衡点，既能提供足够的训练信号，又不会过度破坏句子的完整性。
计算效率：只预测15%的token，相比预测所有token，计算成本更低。

是否可以调整？
可以调整，但需要权衡：

更低的mask比例（5-10%）：训练更稳定，但可能需要更多训练步数
更高的mask比例（20-30%）：可能学到更强的表示，但训练难度增加，可能影响性能

实验发现：

RoBERTa的实验表明，15%仍然是一个较好的选择
对于特定领域或任务，可能需要微调这个比例

2. BERT的MLM任务中，为什么要用80% [MASK]、10% 随机、10% 不变的方式？
这是BERT设计中的一个关键创新，主要解决预训练与微调的不匹配问题：
问题背景：

预训练时：模型看到大量[MASK] token
微调时：模型看不到[MASK] token，只有真实词汇
如果只使用[MASK]，模型会过度依赖这个特殊token，导致微调时性能下降

三种策略的作用：


80% [MASK]：

主要学习目标：让模型学会从上下文预测被掩盖的词
这是MLM的核心任务



10% 随机替换：

引入噪声：让模型学会在&quot;错误&quot;信息下也能正确预测
提高鲁棒性：模拟真实场景中可能出现的错误或噪声
防止过拟合：避免模型只记住[MASK]的模式



10% 保持不变：

关键设计：让模型在微调时能够利用真实词汇的信息
缓解不匹配：预训练时也看到真实词汇，与微调场景更一致
保持语言理解：确保模型不仅学习预测，还学习理解真实词汇



数学直觉：
如果只使用[MASK]：  P(预测|看到[MASK]) ≠ P(预测|看到真实词)  ❌ 不匹配使用混合策略：  P(预测|看到[MASK]) ≈ P(预测|看到真实词)  ✅ 更匹配
实验验证：

如果100%使用[MASK]，在微调任务上性能会下降约2-3%
混合策略使得预训练和微调更加一致

3. NSP任务对BERT的性能有多大影响？为什么RoBERTa去掉了NSP？
NSP的影响分析：
正面影响：

对需要理解句子关系的任务有帮助：如自然语言推理（MNLI）、问答（QNLI）
帮助模型学习句子级表示，而不仅仅是词级表示

负面影响：

任务过于简单：模型可能只关注句子边界信息，而非深层语义关系
训练信号弱：相比MLM，NSP提供的学习信号较弱
可能引入噪声：随机选择的负样本可能包含一些有用的信息，但被标记为&quot;不相关&quot;

RoBERTa去掉NSP的原因：


实验发现NSP效果有限：

在某些任务上，去掉NSP反而性能更好
NSP带来的提升主要来自更长的训练，而非任务本身



NSP任务设计有问题：

负样本（随机句子对）可能包含有用的信息
模型可能学到&quot;只要不是连续句子就是负样本&quot;的简单模式



MLM已经足够强大：

MLM本身就能学习到句子间的关系
通过跨句子的上下文，模型自然能理解句子关系



简化训练流程：

去掉NSP后，训练更简单，只需要MLM任务
可以专注于优化MLM的训练策略



实验结果：

RoBERTa去掉NSP后，在大多数任务上性能持平或略有提升
证明了NSP并非必需，MLM已经足够学习句子级表示

4. BERT为什么不适合生成任务？如果要用于生成，应该怎么改进？
BERT不适合生成任务的原因：


双向编码的本质：

BERT在编码时同时看到整个序列（包括&quot;未来&quot;的信息）
生成任务需要自回归（从左到右），不能看到未来信息
这违反了生成任务的基本要求



架构限制：

BERT只有编码器，没有解码器
无法进行自回归生成



预训练目标不匹配：

MLM是&quot;填空&quot;任务，不是&quot;续写&quot;任务
模型没有学习到生成下一个token的能力



改进方案：


使用BERT作为编码器 + 独立解码器：
BERT Encoder + Transformer Decoder

编码器：用BERT理解输入
解码器：用Transformer解码器进行自回归生成
应用：机器翻译、摘要生成



单向BERT变体：

训练时使用因果掩码（只能看到左侧信息）
类似GPT，但可以保留BERT的其他优势



序列到序列BERT（BART）：

使用BERT的编码器 + GPT的解码器
预训练任务：文本去噪（删除、打乱、填充等）
既保留BERT的理解能力，又具备生成能力



T5（Text-to-Text Transfer Transformer）：

统一的编码器-解码器架构
所有任务都转换为文本生成任务
更强的生成能力



实际应用：

BART：专门为生成任务设计的BERT变体
T5：统一的文本到文本模型
GPT系列：更适合纯生成任务

5. 如何理解BERT的双向编码？它与ELMo的双向有什么本质区别？
BERT的双向编码：


同时编码：

在单次前向传播中，每个token都能同时看到左侧和右侧的所有信息
通过自注意力机制，所有位置的信息同时参与计算



深度双向：

多层Transformer编码器，每一层都是双向的
信息在多层间双向流动和融合



数学表示：
h_i^l = Transformer_Encoder([h_1^&#123;l-1&#125;, ..., h_n^&#123;l-1&#125;])
其中，h_i^l 同时依赖于所有位置的 h_j^{l-1}


ELMo的双向：


分别编码：

使用两个独立的LSTM：前向LSTM和后向LSTM
前向LSTM：从左到右编码，h_i^forward 只看到左侧信息
后向LSTM：从右到左编码，h_i^backward 只看到右侧信息



浅层拼接：

最终表示 = [h_i^forward; h_i^backward]
只是简单拼接，没有深度融合



数学表示：
h_i^forward = LSTM_forward(x_1, ..., x_i)h_i^backward = LSTM_backward(x_i, ..., x_n)h_i = [h_i^forward; h_i^backward]


本质区别：



特性
ELMo
BERT




编码方式
两个独立的单向编码器
一个双向编码器


信息融合
浅层拼接
深度融合（多层）


同时性
分别处理，后拼接
同时处理所有位置


架构
RNN（LSTM）
Transformer


表示质量
词级表示为主
上下文相关表示


计算效率
顺序计算，较慢
并行计算，较快



关键差异示例：
假设要理解句子 “The bank is closed” 中的 “bank”：


ELMo：

前向LSTM看到 “The bank”，可能理解为&quot;银行&quot;
后向LSTM看到 “bank is closed”，也可能理解为&quot;银行&quot;
拼接后得到表示，但两个方向的信息是独立计算的



BERT：

自注意力机制同时考虑 “The”、“bank”、“is”、“closed”
所有位置的信息在每一层都相互影响
最终表示融合了完整的上下文信息



为什么BERT的双向更好？

信息融合更充分：不是简单拼接，而是深度交互
并行计算：Transformer可以并行处理，效率更高
长距离依赖：自注意力机制能更好地捕捉长距离依赖
表示能力更强：多层双向编码产生更丰富的表示

总结：

ELMo是&quot;伪双向&quot;：两个单向模型的拼接
BERT是&quot;真双向&quot;：真正的双向同时编码和融合

]]></content>
      <categories>
        <category>论文精读</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Transformer</tag>
        <tag>自然语言处理</tag>
        <tag>论文精读</tag>
      </tags>
  </entry>
  <entry>
    <title>CSRMS</title>
    <url>/2025/04/CSRMS/</url>
    <content><![CDATA[CSRMS
用于视觉表征学习的类级结构化关系建模与平滑 (MM2023)
论文地址：https://ercdm.sdu.edu.cn/__local/7/AC/70/7E4948C4761839F62E3958CE772_043AE854_2B459A.pdf
代码地址：https://github.com/czt117/CSRMS
个人理解这个像是一个知识总结的过程。首先通过特征提取获得特征图，这个过程可以类比我从书本上学习知识的过程，提取出有用的知识，然后通过聚类算法对特征图进行分簇，就相当于把学到的知识进行总结的过程，但是总会有一些比较相近的知识容易被搞混，这个就是类间相似性和类内多样性，再着重对这一块进行处理，使得对知识的掌握更加透彻。
名词解释：
课程构建（Curriculum Construction）：是一种渐进式学习方法，通过根据任务或样本的难度或复杂性来设计学习顺序，让模型从简单的内容逐步过渡到复杂的内容，从而更高效地掌握知识。它通过评估样本难度、调整学习权重并动态控制训练过程，帮助模型更高效地学习，最终提高收敛速度和泛化能力。
实例级别的图像（Instance-level images）：指的是针对单个具体图像样本的研究和处理。每一张图像被视为一个独立的实例，拥有其独特的特征和属性，而不是仅仅代表某个类别或群体的共性。与类别级别（Class-level）的研究不同，实例级别更关注图像的个体差异和细节。
对比学习（Contrastive Learning）：是一种机器学习技术，特别在无监督学习和自监督学习中应用广泛。它的核心思想是通过对比不同样本之间的相似性和差异性，让模型自动学习数据的特征表示
研究目的
缓解类内多样性和类间相似性问题

整体架构
CSRMS 框架的三个核心模块：类级关系建模、类感知图采样 和 关系图引导的表示学习
在离线学习阶段，根据表征视觉分布挖掘类级关系，构建数据集级关系图；在训练阶段，根据类级关系构建多样化类感知采样策略，结合课程构建，获得Batch级别关系子图；在类级关系的引导下，利用特征平滑、类原型对齐和类级约束显式正则化视觉表征，有效缓解视觉表征分布复杂性。

离线学习阶段
类级关系建模
类级关系定义
(a) 类内视觉模式的多样性（Intra-Class Diversity of Visual Patterns）：同一类别的样本分布在不同主导簇，说明类内样本视觉差异大。
$$
R_{ID} = \{ (I_a, I_b) \mid y_a = y_b, P_{C_a} \neq P_{C_b} \}
$$
(b) 视觉模式的类间相似性 （Inter-Class Similarity of Visual Patterns）：不同类别的样本分布在同一主导簇，说明类间样本视觉上相似。
$$
R_{IS} = \{(I_a, I_b) \mid y_a \neq y_b, P_{C_a} = P_{C_b} \}
$$
© 视觉模式的混类簇 (Mix-class Cluster of Visual Patterns)：不同类别的样本分布在同一混类簇，说明聚类结果复杂，缺乏主导类别。
$$
R_{MC} = \{(I_a, I_b) \mid y_a \neq y_b, M_{C_a} = M_{C_b} \}
$$
主导簇（Dominant Clusters）：指聚类中主要由某个类别样本构成的簇，通常通过聚类算法基于特征分布确定。
混类簇（Mix-class Clusters）：指聚类中没有明显主导类别的簇，包含多个类别的样本，增加了建模难度。
ART聚类
自适应聚类算法 ART：能够根据数据分布动态调整簇数量，适合处理视觉特征空间中的复杂分布。对视觉特征进行聚类，得到视觉模式。$ C={c1,…,cJ}$
$$
C = ART(M_v(I))
$$
得到三个级别：类级别、模式级别、实例级别。然后构建关系图 $G_t$ 。这是一个多层图结构，实例级别连接到模式级别（通过聚类），模式级别连接到类级别（通过标签），这种结构有助于捕捉样本间的复杂关系。
训练阶段
类感知图采样
正样本采样
使用主导模式采样器$ S_{dp} $，从类别 $ y_i $ 主导的最大簇 $c_j $ 中选择与 $ I_i $ 距离最大的前 $ n $个正样本。为每个批次样本选择正样本集合 $ \Omega_{posi} $，以解决类内多样性。
选择距离最远的正样本，增加类内样本的多样性，增强表征学习的鲁棒性。
$$
\Omega_{posi} = S_{dp}(I_i, \varphi(I_i, I_{i_j}), n)
$$
负样本采样
视觉相似模式采样器 $ S_{ap} $，从类别 $ y_i $ 主导的最大簇 $c_i $ 中选择与 $ I_i $ 距离最大的前 $m$个正样本。为每个批次样本选择负样本集合$\Omega_{nega} $，以解决类间相似性和混类簇。
选择视觉相似的负样本，增强类间区分度，缓解类间相似性问题。
$$
\Omega_{nega} = S_{ap}(I_i, \varphi(I_i, I_{i_j}), m)
$$
课程学习
基于易难估计，为批次样本分配难度级别。
$$
f(I_i) = \begin{cases}
\Omega_{\text{easy}} &amp; \text{if } \frac{N_k^j}{N_k} &gt; \rho_1 \\
\Omega_{\text{medium}} &amp; \text{if } \frac{N_k^j}{N_k} &lt; \rho_1 \\
\Omega_{\text{hard}} &amp; \text{if } \frac{N_k^h}{N_k} &gt; \rho_1 \text{ and } j \neq h
\end{cases}
$$
调整学习权重
使用“衰减方法”，设置惩罚系数，并通过参数调节。
$$
α_i⋅λ_e+(1−α_i)⋅(α_f⋅λ_m+(1−α_f)⋅λ_h)=1
$$
初期：$\alpha_i $、$ \alpha_f $接近 1，优先学习简单样本。
后期：当损失收敛（例如损失 &lt; 0.01 且连续两轮损失差 &lt; 0.0001）时，逐步减小 $ \alpha_i $、$ \alpha_f $，引入更多困难样本。
构建批次子图
$G_b$ 是 $ G_t $的一个子图，针对每个训练批次（batch）生成，包含批次中的样本及其正负样本对，用于后续表征学习。过采样策略，从 $ G_t $中选择与批次样本相关的子图结构，保留类级别、模式级别和实例级别的关系，同时引入正样本和负样本以增强表征学习。
关系图引导的表示学习
簇感知表征平滑
图形平滑
使用图卷积网络（GCN）$ \mathcal{G}(\cdot) $，基于子图 $ G_b $ 和正样本 $ \Omega_{posi} $，聚合同一类别图像的信息。
$$
F_g=\mathcal{G}(\hat{A},I_i,Ω_{posi})=softmax(\hat{A}⋅ReLU(\hat{A}⋅X⋅W^{(1)})⋅W^{(0)})
$$
簇级特征平滑
将表征 $ F_g $ 与簇原型  $ w_{cu} $ 对齐，其中$w_{cu} $是类别 $ y_i $主导的最大簇 $c_j $的原型。
$$
F_u=α_u⊙F_g+β_u⊙w_{cu}
$$
类级分布正则化
类级表征对齐
表征 $ F_u$ 与类原型 $ p_{cu} $ 对齐，其中 $ p_{cu} $ 是类别 $ y_i $ 在所有主导簇中的表征聚合。
$$
F_a=α_a⊙F_u+β_a⊙p_{cu}
$$
负样本约束
构建负样本损失$\mathcal L_{neg}$，推远不同类别表征：
$$
\mathcal L_{neg} = \sum_{i=1}^{N} -\log \left( \mu_{i} \frac{\theta}{\sum_{q=1}^{m}|M_v(I_i) - M_v(\Omega_{nega}^q)|_2 + \theta} \right)
$$
类间约束
构建类间分散损失$\mathcal L_{inter}$，进一步推远不同类别表征：
$$
\mathcal L_{inter} = \sum_{i,j} { i = j } |0| + { i \ne j } -\log \left( \mu\frac{\theta}{|F_v^i - F_v^j|_2 + \theta} \right)
$$
分类器训练
使用分类器 $ C(\cdot) $预测类别：
$$
F_a →P
$$
使用交叉熵损失 $\mathcal L_{ce}$优化：
$$
\mathcal L_{ce} = \frac{1}{N} \sum_{i} \mathcal L_i = -\frac{1}{N} \sum_{i}\sum_{c=1}^{C} y_{ic} \log(P_{ic})
$$
]]></content>
      <categories>
        <category>论文精读</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>视觉表征</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 两个变量 - 2025.11.13</title>
    <url>/2025/11/LeetCode%E5%88%B7%E9%A2%98%E6%97%A5%E5%BF%97%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[今日概览

日期：&#123;2025-11-13&#125;
题目数量：&#123;共 2 题&#125;
难度分布：简单 2 
主要收获：自己的方法就是屎山，灵神的方法高端又通透
心情/状态：太久没刷了，已经把基本的语法忘记了，以后尝试用python刷题，学习一些比较好用的函数


题目列表与详解
1. Two Sum


题号 / 链接：#1 / 题目链接


难度：简单


题型标签：哈希表，数组


题目描述（简要）：

就是查找一下哪两个数相加等于target，返回下标。



思路分析

两个方法，不同的时间复杂度

方法一：暴力写法。
复杂度

时间：O(*n*2)
空间：O(1)

class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        for i, x in enumerate(nums):              for j in range(i + 1, len(nums)):                  if x + nums[j] == target:                     return [i, j]  
​	这里主要是enumerate( )函数，忘记是干啥用的了，enumerate( ) 是 Python 提供的内置函数，用来在遍历可迭代对象（如列表、元组、字符串等）时，同时获得元素的索引和值。它的常见用法如下：
words = [&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;]for index, item in enumerate(words):    print(index, item)
输出结果：
0 apple1 banana2 cherry
方法二：哈希。
class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        seen = &#123;&#125;        for idx, num in enumerate(nums):            complement = target - num            if complement in seen:                return [seen[complement], idx]            seen[num] = idx        return []
复杂度

时间：O(n)
空间：O(n)

2025.11.17 二刷
出去开会几天，把enumerate函数忘记了
class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        s=&#123;&#125;        for i in range(len(nums)):            if target-nums[i] in s:                return [i,s[target-nums[i]]]            s[nums[i]]=i
反思与总结

可以按照题目要求找符合两数相加的值，也可以换一种思路直接去查找符合要求的值，枚举其中一个变量，把它当成常量看待，从而转化成「一个变量」的问题


2. 好数对的数目

题号 / 链接：#2 / 题目链接
难度：简单
题型标签：哈希表，数组
题目描述（简要）：

就是统计每个数出现了多少次，然后用排列组合算出来相加。



思路分析
我的垃圾写法，后续有可以进行改进，学习的新函数，还是贴了一下。
复杂度
class Solution:    def numIdenticalPairs(self, nums: List[int]) -&gt; int:        n=[0 for i in range(101)]        m=0        for index,num in enumerate(nums):            n[num]+=1        for i in range(101):            t=0            while n[i]&gt;=1:                n[i]=n[i]-1                t=t+n[i]            m+=t        return m
如果换一个数据范围可能就不行了，更通用的写法可以使用 collections.Counter 或 defaultdict(int) 自动统计。还有可以直接使用排列组合公式cnt * (cnt - 1) // 2 就能得到该值的所有好数对数量。
改进方法一：使用 Counter
from typing import Listfrom collections import Counterclass Solution:    def numIdenticalPairs(self, nums: List[int]) -&gt; int:        counter = Counter(nums)        ans = 0        for cnt in counter.values():            ans += cnt * (cnt - 1) // 2        return ans
改进方法二：一边遍历一边累加
from typing import Listfrom collections import defaultdictclass Solution:    def numIdenticalPairs(self, nums: List[int]) -&gt; int:        freq = defaultdict(int)        ans = 0        for num in nums:            ans += freq[num]      # 之前出现了 freq[num] 个 num，与当前 num 都能构成好数对            freq[num] += 1        return ans
复杂度

时间：O(n)
空间：O(n)

反思与总结

我的方法毫无通用性可言

]]></content>
      <categories>
        <category>算法刷题</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>刷题记录</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>MAE详解 - Masked Autoencoders精读</title>
    <url>/2025/11/MAE/</url>
    <content><![CDATA[Masked Autoencoders Are Scalable Vision Learners
引言
MAE (Masked Autoencoders) 由He Kaiming团队在2021年提出，为视觉自监督学习带来了新的范式。论文标题“Masked Autoencoders Are Scalable Vision Learners”凸显了其两大特性：一是基于掩码的自重构任务；二是能在大规模数据和模型上稳定扩展。和SimCLR、MoCo等对比学习方法相比，MAE丢弃了昂贵的负样本构造环节，通过简单的遮挡-重建目标即可学习高质量的视觉特征。
在图像理解任务中，过去的自监督方法往往依赖对比学习或生成式建模。MAE将NLP中成熟的Masked Language Modeling理念迁移到视觉领域，将图片切分为patch token，然后随机遮挡大部分token，让模型仅凭剩余少量可见token推断出被遮挡的像素，从而学到上下文结构。
背景知识
自监督视觉预训练的演进

预文本任务 (Pretext Task)：如旋转预测、拼图恢复等，但任务与下游语义差距较大。
对比学习时代：SimCLR、MoCo、BYOL通过实例判别与数据增强获得强特征，但需要大batch或额外队列。
生成式方法回潮：iGPT、VQ-VAE尝试像素重建，但计算成本高、收敛慢。
Vision Transformer普及：ViT把图像转成patch序列，为视觉领域引入“token”概念，使得掩码预测成为可能。

MAE正是在ViT基础上发展的生成式自监督方法。
自动编码器 vs. 对比学习



特性
自动编码器
对比学习




目标
自重构输入
拉近正样本、推远负样本


数据增强依赖
低
高


训练稳定性
较稳定
依赖大batch/动量队列


计算需求
可低（MAE只编码可见token）
往往高


表征性质
偏向局部+全局
偏向判别特征



MAE融合了自动编码器的补全思想与Transformer的全局建模能力。
论文核心思想
主要贡献

高掩码率：高达75%~90%的随机遮挡仍能有效预训练，极大降低计算量。
轻量解码器：编码器专注表示学习，解码器仅用于预训练阶段重建。
非对称架构：编码器处理少量可见token，解码器负责重建全部token，简化了训练。
可扩展性：在ImageNet上与有监督预训练相当甚至更优，对下游检测、分割任务具备竞争力。

整体流程
输入图像 → Patch划分 → 随机掩码 → 编码器处理可见patch → 嵌入掩码token → 轻量解码器重建像素 → 计算重建损失
模型架构
Patch Embedding 与 ViT兼容性

使用ViT相同的patchify方式：将图像划分为16×16的patch，拼接成序列。
对每个patch做线性投影得到D维token，并加上位置编码。
由于MAE后续要还原像素，额外保存patch形状信息用于unpatchify。

随机掩码策略

均匀随机：从所有patch中随机采样保留25%，掩码75%。
可见token排序：保留patch在顺序上也会随机打乱，增加任务难度。
原因：视觉信息冗余高，遮挡大部分区域仍能推断结构，训练效率也随掩码率提高而提高。

编码器（Encoder）

直接使用标准ViT Backbone（如ViT-B/16）。
输入仅限可见token，大幅减少自注意力计算量。
由于只处理25%的token，训练速度提高约3~4倍。

解码器（Decoder）

结构比编码器浅（如8层Transformer），隐藏维度更小（如512）。
输入由“编码器输出的可见token + 掩码token嵌入”组成。
仅在预训练阶段存在，下游微调时丢弃，避免增加推理成本。

重建目标

对所有patch输出预测像素，常用均方误差(MSE)。
论文采用归一化的pixel值（对每个patch做均值方差归一），提升稳定性。
也可以替换为DCT系数、特征空间等，扩展空间很大。

训练策略
预训练设置



配置
常用取值




数据集
ImageNet-1K无标签


掩码率
75%


Patch size
16


训练时长
400 epoch


优化器
AdamW (lr=1.5e-4, weight decay=0.05)


学习率策略
Cosine + warmup


数据增强
仅RandomResizeCrop + 随机水平翻转



微调策略

使用预训练好的编码器权重初始化ViT Backbone。
替换为任务特定头（分类、检测、分割）。
学习率通常更小（如5e-4），训练epoch也减少（如100 epoch）。
对分类任务，还会加入Mixup、CutMix等常规增强。

Linear Probe &amp; KNN

冻结编码器，仅训练线性分类头，可评估特征线性可分性。
5-NN评估也常用来衡量无监督表示质量。

实验结果
ImageNet分类



模型
预训练方式
Top-1 Acc




ViT-B/16 (监督)
有标签
81.8%


ViT-B/16 (MAE)
400 epoch
83.7%


ViT-L/16 (MAE)
400 epoch
85.9%



MAE在有限标注下游训练（如少量epoch）同样保持优势。
下游任务

目标检测 (COCO)：与有监督预训练持平甚至略优。
语义分割 (ADE20K)：微调后比监督预训练高1~2 mIoU。
鲁棒性：对遮挡和噪声具有更强适应性。

消融实验

掩码率：75%最佳，过低浪费计算，过高训练不稳定。
解码器深度：浅层即可，过深收益有限。
重建目标：直接像素即可，无需复杂loss。
位置编码：无需额外改动，使用ViT默认编码即可。

表征分析

MAE倾向关注全局结构，对局部纹理不过拟合。
可视化显示模型能推断出缺失的主体轮廓，说明捕捉到高层语义。
线性探针结果表明特征具备良好的线性可分性。

代码实现要点
基础组件
import torchimport torch.nn as nnclass PatchEmbed(nn.Module):    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):        super().__init__()        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)    def forward(self, x):        # x: [B, C, H, W]        x = self.proj(x)                               # [B, embed_dim, H/patch, W/patch]        x = x.flatten(2).transpose(1, 2)               # [B, N, embed_dim]        return x
随机掩码函数
def random_masking(x, mask_ratio=0.75):    B, N, _ = x.shape    len_keep = int(N * (1 - mask_ratio))    noise = torch.rand(B, N, device=x.device)    ids_shuffle = torch.argsort(noise, dim=1)    ids_restore = torch.argsort(ids_shuffle, dim=1)    ids_keep = ids_shuffle[:, :len_keep]    x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, x.size(-1)))    return x_masked, ids_restore, ids_keep
训练循环示例
def mae_forward(model, imgs, mask_ratio=0.75):    tokens = model.patch_embed(imgs)    tokens = tokens + model.pos_embed[:, 1:, :]    tokens, ids_restore, _ = random_masking(tokens, mask_ratio)    latent = model.encoder(tokens)    # prepare decoder input    mask_tokens = model.mask_token.repeat(latent.size(0), ids_restore.size(1) - latent.size(1), 1)    latent_full = torch.cat([latent, mask_tokens], dim=1)    latent_full = torch.gather(latent_full, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, latent.size(2)))    rec = model.decoder(latent_full)    target = model.patchify(imgs)    loss = ((rec - target) ** 2).mean()    return loss
PyTorch Lightning/Timm 的现成实现

timm.models.mae 提供官方实现，可直接加载mae_vit_base_patch16等模型。
Hugging Face Transformers也提供Mask2Former等派生实现。

工程实践建议

Mixed Precision：配合高掩码率，训练非常高效。
数据增强：预训练阶段增强较轻，避免破坏像素重建难度；微调阶段再加重。
学习率调试：预训练用较大学习率，微调用较小的学习率。
梯度累计：若显存受限，可结合梯度累计保持批量大小。

与其他方法的比较



模型
训练范式
计算需求
表征特点




MAE
掩码重建
低 (编码少量token)
全局语义强


BEiT
Token重建
中 (需tokenizer)
依赖dVAE词表


SimMIM
像素重建
中等
无解码器分离


MaskFeat
HOG特征重建
中
更注重低级特征



MAE以简单高效著称，成为后续大量工作（如MaskFeat、iBOT）的基础。
优缺点总结
优点

训练高效：编码器只处理可见token，显著降低计算。
鲁棒性强：对遮挡与噪声有更好表现。
迁移能力好：分类、检测、分割任务均有优势。
实现简单：无须复杂的数据增强或对比对。

缺点

重建目标限制：重建像素可能关注低级细节，对高层语义关注不足。
遮挡策略固定：随机掩码未利用场景先验，对结构化遮挡可能欠佳。
不适合生成：目标是补全而非生成高质量图像。
超参数敏感：掩码率、解码器宽度等需要调优。

相关与后续工作

MAE v2：引入多尺度特征、更强的数据增强。
SimMIM / MaskFeat：探索不同重建目标（直接像素、HOG等）。
MaskCLIP：结合跨模态信息，用文本引导掩码学习。
Masked Siamese Networks：将掩码与对比学习结合。
VideoMAE：扩展到视频，随机掩码时空patch。

总结
MAE通过高掩码率的自重构任务，在不依赖负样本对的情况下实现了高质量的视觉自监督学习。其关键在于：

非对称架构：编码器轻量输入，解码器仅用于训练。
高掩码率：降低计算同时保持学习难度。
贴近NLP的设计：借鉴MLM理念，实现跨模态迁移。

MAE证明了生成式自监督在视觉任务中的可行性，为后续的Mask-based预训练方法奠定了基础。
参考文献

He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked autoencoders are scalable vision learners. arXiv:2111.06377.
Dosovitskiy, A., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale.
Bao, H., Dong, L., &amp; Wei, F. (2021). BEiT: BERT Pre-Training of Image Transformers.
Xie, Z., et al. (2022). SimMIM: A Simple Framework for Masked Image Modeling.

思考题

为什么MAE可以使用75%的高掩码率？是否存在场景需要降低掩码率？
解码器仅用于预训练阶段，是否意味着我们可以用更复杂的解码器来提升效果？
MAE的像素重建目标会不会让模型学到过多低级特征？如果是，该如何改进？
与对比学习相比，MAE缺少显式的判别约束，如何弥补这一点？
如果要把MAE扩展到视频或多模态任务，需要额外注意哪些设计？

思考题答案
1. 为什么MAE可以使用75%的高掩码率？是否需要调整？

图像存在大量冗余，局部区域通常可由上下文推断。
Transformer具备全局建模能力，即便仅看到25%的patch也能捕捉结构。
高掩码率减少计算成本；在纹理细节极其丰富的任务（如医学影像）可以适当调低（如50%）以保证信息量。

2. 解码器能否更复杂？

理论上可以，但实验表明解码器过深收益有限，且增加训练成本。
解码器主要提供学习信号，过强的解码器会掩盖编码器能力。
若要改进，不如让解码器重建更高层次表征（如语义分割mask）。

3. 像素重建是否导致偏向低级细节？

像素Loss会促使模型拟合纹理，但高掩码率迫使模型理解结构。
可通过重建特征空间（如DINO特征）、频域信息或多任务损失提升语义理解。

4. 如何引入判别约束？

在MAE基础上叠加对比学习头（如MAE+MoCo）。
结合监督信号（半监督设置）或知识蒸馏。
在线性探针阶段引入额外的判别任务。

5. 拓展到视频或多模态的注意点

视频：需要处理时序维度，可随机掩码时空patch，并采用时序位置编码。
多模态（图文）：需联合掩码不同模态，并设计跨模态对齐目标，如重建文本描述。
计算资源：视频/多模态数据更大，掩码率、模型尺寸需兼顾效率。

]]></content>
      <categories>
        <category>论文精读</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>MAE</tag>
        <tag>自监督学习</tag>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MoE</title>
    <url>/2025/04/MOE/</url>
    <content><![CDATA[Mixtures of Experts
《Adaptive Mixture of Local Experts》

论文链接：https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf
1991年，由 Hinton和 Jordan提出，这是最早的MoE架构。
核心思想：通过多个独立专家网络处理输入数据不同子集，并由门控网络动态选择专家。每个专家接受相同的输入数据，但通过门控网络的动态分配，专家会专注于处理输入空间的特定区域。
基础架构
如图，一个由专家网络和门控网络组成的系统。每个专家是一个前馈网络，所有专家接收相同的输入，并具有相同数量的输出。门控网络也是一个前馈网络，通常接收与专家网络相同的输入。它的输出是归一化的 $ p_j = \exp(r_j) / \sum_i \exp(r_i) $，其中 $ r_j $是门控网络输出单元 $j$ 接收的总加权输入。选择器（selector）类似于一个多输入单输出的随机开关；开关选择来自专家 $ j $ 的输出的概率为 $p_j$ 。每个专家通常只会被分配到可能输入向量空间的一个小区域内。
系统由多个专家网络和一个门控网络组成。每个专家是一个前馈网络，处理特定子任务；门控网络根据输入决定每个专家的混合比例（概率）。

通过重新定义误差函数，鼓励专家竞争而非协作，确保每个专家专注于特定子任务。传统误差函数（如均方误差）会导致专家协作，增加耦合；论文提出优比损失（基于高斯混合模型的负对数概率），使专家独立学习，减少干扰。改进后的误差函数使门控网络倾向于选择最适合的专家，加快收敛。
性能比较
元音辨别：区分多说话者元音区分（识别元音 [i], [I], [a], [A]）。
数据集：来自 75 个说话者的共振峰数据（Peterson 和 Barney, 1952），前 50 个用于训练，后 25 个用于测试。

模型能自动分解任务，专注于不同类别对（如 [i]/[I] 和 [a]/[A]），仅 2-3 个专家在最终混合中活跃。
混合专家模型达到误差标准明显快于反向传播网络，平均只需要大约一半的周期数。混合模型的学习时间也随着专家数量的增加而很好地扩展。混合专家模型具有较小但统计上显著的平均周期数优势。
总结
这篇论文针对的问题是在不同场合执行不同任务会产生干扰，导致训练速度慢和泛化性能差。MoE 的核心思想是让专家专注于适合的子任务。这意味着某些专家的利用率较低（论文中提到最终只有 2-3 个专家活跃），后续的论文大多都是在解决这个问题。
论文展示了 MoE 模型的任务分解过程：初始阶段，门控网络给所有专家分配相等的混合比例，导致每个专家处理的案例数量大致相等，决策线趋向于处理所有案例的平均最优解。随着训练进行，竞争机制使专家分化，专注于特定子任务，从而形成更符合数据分布的最优决策面。这种机制显著减少了干扰，训练速度比反向传播网络快约 50%，体现了 MoE 架构在训练时间上的优势。
《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》
超大型神经网络：稀疏门控混合专家层
论文链接：https://arxiv.org/abs/1701.06538
2017年，合作作者中还有Hinton和 Jordan，在LSTM层之间应用MoE卷积。仅以微小的计算效率损失就取得了超过 1000 倍的模型容量提升，并在公共语言建模和翻译数据集上显著提升了最先进的结果。
上一篇强调的是减小时间成本，这一篇是减小计算成本。

基础架构
稀疏门控专家混合层（MoE）
MoE 由很多专家组成，每个专家相当于一个前馈神经网络。以及一个可训练的门控网络，该网络选择专家的稀疏组合来处理每个输入。网络的所有部分都通过反向传播进行联合训练。

设 $ G(x) $和 $ E_i(x) $ 分别为给定输入 $ x $ 的门控网络输出和第 $ i $ 个专家网络的输出。MoE 模块的输出 $ y $ 可表示为：
$$
y = \sum_{i=1}^{n} G(x)_i E_i(x)
$$

均衡负载
门控网络总是倾向于收敛到一种状态，在这种状态下，它总是对同一批专家产生较大的权重，使得被偏好的专家训练得更快，更容易被门控网络选中。希望在训练和推理过程中，专家的重要性相等，称之为负载平衡 。某种程度上，这是为了防止对同一个专家过度拟合。
Keep Top-K
Noisy Top-K Gating
添加了两个组件：稀疏性和噪音。在应用 Softmax 函数之前，添加可调的高斯噪声，然后仅保留前 $ k $ 个值，将其余值设为 $-\infty$（这会导致对应的门控值为 0）。每部分的噪声量由可训练权重矩阵 $ W_{\text{noise}} $ 控制。通过简单的反向传播训练门控网络。稀疏性用于节省计算，噪声项有助于负载均衡。

Noise
使用Noisy Top-K Gating方法改进MoE层，引入可训练的 Gaussian 噪声防止总是选择相同的专家。

$$
H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i)
$$
Sparse
除了想要激活的前 k 名专家之外，其他所有专家的权重都将设置为 $-\infty$ 。

$$
\text{KeepTopK}(v, k)_i =
\begin{cases}
v_i & \text{if } v_i \text{ is in the top } k \text{ elements of } v, \\\
-\infty & \text{otherwise}.
\end{cases}
$$
通过将这些权重设置为 $-\infty$ ，SoftMax 在这些权重上的输出将产生概率 0 ：

$$
G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))
$$
KeepTopK 策略将每个 token 路由给几个选定的专家。

Auxiliary Loss
为了在训练过程中获得更均匀的专家分布，辅助损失（也称为负载平衡损失 ）被添加到网络的常规损失中。它增加了一个约束，迫使专家具有同等重要性。
该辅助损失的第一个组成部分是将整个批次中每个专家的门控值相加。

$$
\text{Importance}(X) = \sum_{x \in X} G(x) 
$$
这为我们提供了每个专家的重要性分数 ，该分数表示无论输入如何，选择特定专家的可能性。可以用它来计算变异系数 （ CV ），它告诉我们专家之间重要性得分的差异有多大。利用这个 CV 分数，我们可以在训练期间更新辅助损失 ，以尽可能降低 CV 分数（ 从而给予每个专家同等的重要性 ）。

如果重要性分数存在很大差异，则 CV 会很高，相反，如果所有专家的重要性得分都相似，那么 CV 就会较低

$$
L_{\text{importance}}(X) = w_{\text{importance}} \cdot CV(\text{Importance}(X))^2
$$
最后，将辅助损失作为单独的损失添加，以便在训练期间进行优化。
虽然这种损失函数可以确保平衡的重要性，但专家仍然可能收到数量非常不同的样本。为了解决这一问题，论文引入了 $ L_{\text{load}} $ 损失，专门用于平衡专家接收的样本数量（即负载均衡），与 $ L_{\text{importance}} $ 损失（平衡门控权重总和）配合使用。
Load-Balancing Loss
专家接收的样本数量是一个离散值，无法用于反向传播。所以这里定义了一个平滑估计器 $Load(X)$，用于估计每个专家在输入批次 $X$ 中分配到的示例数量，通过概率计算来近似样本分配。平滑性使得可以通过估计器反向传播梯度。这是门控函数中噪声项的目的。
对于一个输入批次 $ X $，第 $ i $ 个专家的负载定义为：
$$ 
{Load}(X)_i = \sum_{x \in X} P(x, i)
$$ 
其中 $ P(x, i) $ 是给定输入 $ x $ 时第 $ i $ 个专家被选中的概率，它描述了第 $ i$ 个专家的“带噪声得分”大于某个阈值的概率。论文通过噪声 Top-K 门控的特性计算 $ P(x, i) $：
$$ 
P(x, i) = \Pr\left( (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i) > k{th\_excluding}(H(x), k, i) \right)
$$ 
$ (x \cdot W_g)_i $：

$x $ 是输入向量（例如来自上一层的 LSTM 输出）。
$ W_g $ 是门控网络的可训练权重矩阵。
$ (x \cdot W_g)_i $ 是第 $ i $ 个专家的原始得分（未添加噪声），表示门控网络对第 $ i $ 个专家的“偏好”。

$ StandardNormal()⋅Softplus((x⋅Wnoise)i) $：

$ \text{StandardNormal}() $：表示从均值为 0、标准差为 1 的标准正态分布中采样一个随机数。
$ (x \cdot W_{\text{noise}})_i $：通过另一个可训练权重矩阵$ W_{\text{noise}} $ 计算的噪声控制项。
$ \text{Softplus}((x \cdot W_{\text{noise}})_i) $：这部分计算噪声的标准差。它通过将输入 $x$ 与另一个可训练的权重矩阵 $W_{noise}$ 相乘，然后应用 Softplus 函数 $ \text{Softplus}(z) = \log(1 + e^z) $ 来确保标准差为正值。这个标准差是可调节的，并且依赖于输入 $x$ 。
这部分是加到原始得分上的高斯噪声，表示一个高斯噪声项，均值为 0，方差由 $ \text{Softplus}((x \cdot W_{\text{noise}})_i) $决定。噪声的引入有助于负载均衡，避免门控网络总是选择固定的专家。

$$ 
H(x)_ i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i
$$ 
$H(x)_i$代表了第$ i $个专家的最终“带噪声得分”。
$k\text{th_excluding}(H(x), k, i)$: 这是决定专家 i 是否被选中的阈值。它的含义是：在向量 $H(x)$（包含了所有专家的带噪声得分）中，排除掉第 i 个专家自身的分数后，找到剩下 n−1 个分数中第 k 大的分数 。
$Pr(⋯&gt;…): $ 公式计算  $ H(x)_ i $（重新采样噪声后）大于  $k\text{th_excluding}(H(x), k, i)$ 的概率。在噪声 Top-K 门控中，第 $𝑖$ 个专家被选中当且仅当 $ H(x)_ i$ 是 $𝐻 ( 𝑥 )$ 中前 $𝑘$ 大的值。
$$ 
P(x, i) = \Phi\left( \frac{(x \cdot W_g)_i - k{th\_ excluding}(H(x), k, i)}{\text{Softplus}((x \cdot W_{\text{noise}})_i)} \right)
$$ 
Φ 表示标准正态分布的累积分布函数（CDF）。利用正态分布的 CDF 给出了计算这个概率的具体数学表达式，方便进行计算和反向传播（因为 Φ 是可微的）。这整个机制是为了在选择专家时引入随机性（有助于负载均衡 ）并估算每个专家被选中的概率，进而定义$L_{load}$损失。
$$ 
L_{\text{load}}(X) = w_{\text{load}} \cdot CV(\text{Load}(X))^2
$$ 
初始负载不平衡：为了避免内存溢出错误，需要在近似相等的专家负载状态下初始化网络（因为软约束需要一些时间才能发挥作用）。为了实现这一点，将矩阵 $W_g$ 和 $W_{noise}$ 初始化为全零，这样就不会产生信号，而只有一些噪声。
Hierarchical Mixture-of-Experts
层次 MoE 是一种分层结构的 MoE，如果专家数量非常庞大，可以使用两层分层MoE来降低分支因子。在一个分层MoE中，一个主门控网络选择一个稀疏加权组合的“专家”，每个“专家”本身就是一个具有自己门控网络的二级混合专家。
第一级（主门控网络）：主门控网络 $ G_{\text{primary}} $ 负责选择一组“专家组”（groups of experts）。
第二级（次级门控网络）：每个专家组内有一个次级门控网络 $G_i $，负责在该组内选择具体的专家。
专家网络：最终的专家网络 $E_{i,j} $，其中 $ i $ 表示组索引，$j $ 表示组内的专家索引。
$$ 
y_H = \sum_{i=1}^{a} \sum_{j=1}^{b} G_{\text{primary}}(x)_i \cdot G_i(x)_j \cdot E_{i,j}(x)
$$ 
$Gprimary(x)i$：主门控网络对第 $ i $ 个组的权重。
$G_i(x)_j $：第 $ i $ 个组的次级门控网络对组内第 $j $ 个专家的权重。
$ E_{i,j}(x) $：第 $ i $ 个组中第 $j $ 个专家的输出。
对专家利用率的衡量指标将更改为以下内容：
$$ 
\text{Importance}_H(X)_{i,j} = \sum_{x \in X} G_{\text{primary}}(x)_i \cdot G_i(x)_j
$$ 
$$ 
\text{Load}_H(X)_{i,j} = \frac{\text{Load}_{\text{primary}}(X)_i \cdot \text{Load}_i(X^{(i)})_j}{|X^{(i)}|}
$$ 
$Load_{primary}$   和 $Load_i$ 分别表示主门控网络和 $i^{th}$ 次级门控网络的加载函数。 $ X^{(i)}$表示 X 中满足 $ G_{primary}(x)_i &gt; 0 $ 的子集。
《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》
基于条件计算和自动分片的巨型模型扩展。
论文链接：https://arxiv.org/abs/2006.16668

创新点
Expert Capacity
专家容量
不平衡不仅存在于所选专家中，还存在于发送给专家的token分配中。如果输入 token 不成比例地发送给一个专家而不是另一个专家，那么也可能导致训练不足。

解决这个问题的一个方法是限制每个专家可以处理的 token 数量，即专家容量 。当专家达到容量上限时，产生的 token 将被发送给下一个专家：

如果两个专家都已达到其容量上限，则 token 将不会被任何专家处理，而是被发送到下一层。这称为 token 溢出 。这些token的表示通过残差连接传递到下一层。

Auxiliary loss
门控函数不应总是选择相同的一小部分专家，因为这会导致一小部分专家容量溢出，而其余专家则被闲置。定义了一个辅助损失项 ℓaux 来强制执行此约束。 它被添加到模型的总损失函数中 L = ℓnll + k ∗ ℓaux，其中 k 是一个常数乘数。
Random routing
随机路由机制
在 top-2 设计中，始终选择表现最优的专家，但第二选择的专家则根据其权重以一定概率被选中。
《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Eﬃcient Sparsity》
论文链接：https://arxiv.org/abs/2101.03961

简化了MoE路由算法，并设计了直观的改进模型，从而降低了通信和计算成本。提出的训练技术减轻了不稳定性，并且证明了可以使用较低精度（bfloat16）格式首次训练大型稀疏模型。将 Switch 层添加到 Transformer 的自注意力层中。
Switch Routing
（1）仅将token路由到单个专家，因此减少了路由器计算。
（2）由于每个token仅被路由到单个专家，因此每个专家的批次大小（专家容量）可以至少减半。
（3）路由实现得到简化，并且通信成本降低。
《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》
现有MoE架构问题
（1）知识混合性：现有的MoE实践通常采用数量有限的专家（例如，8个或16个），因此分配给特定专家的token可能涵盖各种知识。因此，指定的专家将倾向于在其参数中组合截然不同的知识类型，而这些知识很难同时利用。
（2）知识冗余：分配给不同专家的token可能需要共同的知识。因此，多个专家可能会收敛于在其各自的参数中获取共享知识，从而导致专家参数的冗余。
这些问题共同阻碍了现有MoE实践中的专家专业化，使其无法达到MoE模型的理论性能上限。
我个人感觉第一个问题和第一篇论文提出的目的应该是一致的，一个模型在不同场合执行不同任务会产生干扰，然后这里的专家又出现了这个问题，使得专家的专业化程度较低。
解决问题的主要策略
（1）细粒度专家分割：在保持参数数量不变的同时，我们通过分割 FFN 中间隐藏维度将专家分割成更细的粒度。相应地，在保持恒定计算成本的同时，我们也激活更多细粒度的专家，以实现激活专家的更灵活和适应性更强的组合。细粒度的专家分割使得不同的知识能够被更精细地分解，并被更精确地学习到不同的专家中，其中每个专家将保持更高的专业化水平。此外，激活专家组合的灵活性增加也有助于更准确和有针对性的知识获取。
（2）共享专家隔离：我们隔离某些专家作为始终激活的共享专家，旨在捕获和巩固不同上下文中的通用知识。通过将通用知识压缩到这些共享专家中，其他路由专家之间的冗余将被减轻。这可以提高参数效率，并确保每个路由专家通过专注于独特的方面来保持专业性。
DeepSeekMoE 中的这些架构创新为训练参数高效的 MoE 语言模型提供了机会，其中每个专家都高度专业化。

DeepSeekMoE 的示意图。子图 (a) 展示了具有传统 top-2 路由策略的 MoE 层。子图 (b) 说明了细粒度专家分割策略。子图 © 展示了共享专家隔离策略的集成，构成了完整的 DeepSeekMoE 架构。值得注意的是，在这三种架构中，专家参数的数量和计算成本保持不变。

DeepSeekMoE 架构
细粒度专家分割
通过将FFN中间隐藏维度降低到原始大小的𝑚分之一，将每个专家FFN分割成𝑚个更小的专家。由于每个专家变得更小，作为回应，还将激活专家的数量增加到𝑚倍，以保持相同的计算成本。通过细粒度的专家分割，MoE层的输出可以表示为：
$$ 
\mathbf{h}_t^l = \sum_{i=1}^{mN} (g_{i,t} \cdot \text{FFN}_i(\mathbf{u}_t^l)) + \mathbf{u}_t^l
$$ 
$$
g_{i,t} = \begin{cases}
s_{i,t}, &amp; s_{i,t} \in \text{Topk}({s_{j,t}} \mid 1 \leq j \leq mN), mK), \
0, &amp; \text{otherwise}
\end{cases}
$$
$$
s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^{l^T} \mathbf{e}_i^l)
$$
其中，专家参数的总数等于 𝑁 乘以标准 FFN 中的参数数量，mN 表示细粒度专家的总数。采用细粒度专家分割策略后，非零门控的数量也将增加到 mK。
组合灵活性的激增增强了实现更准确和更有针对性的知识获取的潜力。
共享专家隔离
采用传统的路由策略，分配给不同专家的令牌可能需要一些共同的知识或信息。因此，多个专家可能会趋同于在其各自的参数中获取共享知识，从而导致专家参数的冗余。然而，如果存在专门用于捕获和整合不同上下文中的共同知识的共享专家，则可以减轻其他路由专家之间的参数冗余。这种冗余的减轻将有助于构建一个参数效率更高、专家更专业的模型。
隔离𝐾𝑠个专家作为共享专家。无论路由器模块如何，每个token都将被确定性地分配给这些共享专家。为了保持恒定的计算成本，其他路由专家中激活的专家数量将减少𝐾𝑠。通过集成共享专家隔离策略，完整DeepSeekMoE架构中的MoE层可以表述如下：
$$ 
\mathbf{h}_t^l = \sum_{i=1}^{K_s} \text{FFN}_i(\mathbf{u}_t^l) + \sum_{i=K_s+1}^{mN} (g_{i,t} \cdot \text{FFN}_i(\mathbf{u}_t^l)) + \mathbf{u}_t^l
$$ 
$$
g_{i,t} = \begin{cases}
s_{i,t}, &amp; s_{i,t} \in \text{Topk}({s_{j,t} \mid K_s + 1 \leq j \leq mN }), mK - K_s), \
0, &amp; \text{otherwise}
\end{cases}
$$
$$
s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^{l^T} \mathbf{e}_i)
$$
最后，在 DeepSeekMoE 中，共享专家的数量为 𝐾𝑠，路由专家的总数为 𝑚𝑁 − 𝐾𝑠，非零门的数量为 𝑚𝐾 − 𝐾𝑠。
负载均衡问题
自动学习的路由策略可能会遇到负载不均衡的问题，这表现出两个显著的缺陷。首先，存在路由崩溃的风险，即模型总是只选择少数几个专家，导致其他专家无法得到充分的训练。其次，如果专家分布在多个设备上，负载不均衡会加剧计算瓶颈。
Expert-Level Balance Loss
专家级平衡损失。为了降低路由崩溃的风险，平衡损失的计算如下：
$$ 
\mathcal{L}_{\text{ExpBal}} = \alpha_1 \sum_{i=1}^{N'} f_i p_i
$$ 
$$
f_i = \frac{N’}{K’ T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ selects Expert } i)
$$
$$
p_i = \frac{1}{T} \sum_{t=1}^{T} s_{i,t}
$$
其中𝛼1是一个被称为专家级别平衡因子的超参数，为了简洁起见，𝑁′等于(𝑚𝑁 − 𝐾𝑠)，𝐾′等于(𝑚𝐾 − 𝐾𝑠)。1(·)表示指示函数。
Device-Level Balance Loss
将所有路由的专家划分为 𝐷 组 {E1, E2, . . ., E𝐷}，并将每组部署在单个设备上，则设备级别平衡损失的计算方式如下：
$$ 
\mathcal{L}_{\text{DevBal}} = \alpha_2 \sum_{i=1}^{D} f'_i p'_i
$$ 
$$ 
f'_i = \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i} f_j
$$ 
$$ 
p'_i = \sum_{j \in \mathcal{E}_i} p_j
$$ 
其中𝛼2是一个被称为设备级别平衡因子的超参数。设置一个较小的专家级别平衡因子以降低路由崩溃的风险，同时设置一个较大的设备级别平衡因子以促进设备间的均衡计算。
]]></content>
      <categories>
        <category>论文精读</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>混合专家系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Mamba详解 - 选择性状态空间模型精读</title>
    <url>/2025/11/Mamba/</url>
    <content><![CDATA[Mamba: Linear-Time Sequence Modeling with Selective SSMs
论文地址：https://arxiv.org/pdf/2312.00752
代码地址：https://github.com/state-spaces/mamba
引言
Mamba 是一种基于状态空间模型（State Space Model, SSM）的高效序列建模框架，旨在在保持强表达能力的同时，将计算与内存复杂度降至与序列长度线性相关。与Transformer的二次复杂度相比，Mamba在超长序列、低时延和内存受限场景中具有显著优势。
Mamba的核心在于“选择性扫描（Selective Scan）”与“输入依赖的状态转移”，通过对经典S4（Structured State Space Sequence Model）的工程化与理论改进，实现端到端可训练、GPU友好、且具有SOTA性能的线性时间序列模型。
背景知识：状态空间模型（SSM）与S4
连续与离散SSM
连续时间SSM：
$$
\dot x(t) = A x(t) + B u(t), \quad y(t) = C x(t) + D u(t)
$$
离散化后：
$$
x_{k+1} = \bar A x_k + \bar B u_k, \quad y_k = C x_k + D u_k
$$
其中 $x$ 为隐状态，$u$ 为输入，$y$ 为输出。
S4的关键思想

通过特定结构（HiPPO 等）构造稳定的 $A$ 矩阵，捕捉长程依赖；
使用高效卷积实现序列到序列的映射（状态演化可转换为一维卷积核）；
优点：理论稳定、长依赖强；
局限：实现复杂、某些硬件/场景下吞吐有限，且与输入的自适应性不足。

Mamba的核心创新


选择性扫描（Selective Scan）：

令状态转移与输入耦合，通过门控/选择性机制动态调节信息流；
避免固定核卷积的刚性，增强对非稳态、稀疏事件的响应能力。



输入条件化的SSM参数化：

将 $B, C$ 等参数设为输入依赖（conditioning on input），提升表达力；
结合高效实现，使其仍保持线性复杂度。



GPU友好的实现与块并行：

通过分块扫描（block scan）与前缀-后缀合并，实现可并行化的线性时间推理；
内存访问模式优化，显著提升吞吐。



端到端可训练的稳定性：

对状态矩阵参数施加稳定性约束（如谱半径控制、隐式参数化），保证数值稳定。



模型架构
典型的 Mamba Block 包含：

输入投影与门控（选择性）
选择性SSM扫描（线性复杂度）
前馈网络（MLP/GEGLU）
残差连接与归一化

流程示意：
X → InputProj → Selective SSM (scan) → OutputProj → MLP → Residual/Norm
相较Transformer：不使用全局自注意力，而以“可学习的一维核 + 选择性门控”的形式进行token混合；相较S4：对输入进行条件化，提升适配性与表达力。
选择性扫描（Selective Scan）要点

将序列划分为若干块（blocks），每块内部执行线性递推扫描；
记录块末端的状态作为“前缀状态”，在合并阶段向后续块传递；
通过选择性/门控，抑制无用状态更新，突出关键位置的信息流；
与并行卷积不同，选择性扫描可随输入动态调整有效感受野。

复杂度与性能

时间复杂度：$O(N)$ 随序列长度线性；
空间复杂度：$O(N)$（可通过流水线/检查点进一步降内存峰值）；
对超长序列（&gt;4K、&gt;16K tokens）训练与推理更加高效；
在语言建模、语音、时间序列等任务上与或优于同级别Transformer。

训练与实践

优化器：AdamW；学习率余弦退火；warmup数千步；
正则：Dropout/Stochastic Depth 按深度线性增长；
初始化：对状态矩阵采用稳定化参数化（如对角+低秩）；
长序列技巧：梯度检查点、混合精度、分块扫描大小调优。

与Transformer/Conv的比较



方向
Transformer
ConvNet
Mamba（SSM）




复杂度
O(N^2)
O(N)
O(N)


长距离依赖
强（显式全连接）
弱-中
强（稳定递推核）


并行性
训练强/推理弱（自回归）
强
强（块并行扫描）


归纳偏置
弱
强（局部）
中（递推+选择性）


适用场景
通用
局部模式
超长序列/低时延



代码片段（概念化示例）
import torchimport torch.nn as nnclass SelectiveSSM(nn.Module):    def __init__(self, d_model, state_size):        super().__init__()        self.d_model = d_model        self.state_size = state_size        # 输入依赖的门控/参数化（示意）        self.in_proj = nn.Linear(d_model, 3 * state_size)        self.out_proj = nn.Linear(state_size, d_model)    def forward(self, x):        # x: [B, N, D]        B, N, D = x.shape        a, b, g = self.in_proj(x).chunk(3, dim=-1)  # 输入条件化参数（示意）        state = torch.zeros(B, self.state_size, device=x.device)        outputs = []        for t in range(N):  # 实际实现会使用块并行，这里仅示意            state = torch.tanh(a[:, t, :] * state + b[:, t, :])            state = g[:, t, :].sigmoid() * state  # 选择性门控            outputs.append(state)        y = torch.stack(outputs, dim=1)  # [B, N, S]        return self.out_proj(y)class MambaBlock(nn.Module):    def __init__(self, d_model, state_size, mlp_ratio=4.0, p=0.1):        super().__init__()        self.norm1 = nn.LayerNorm(d_model)        self.ssm = SelectiveSSM(d_model, state_size)        self.drop1 = nn.Dropout(p)        self.norm2 = nn.LayerNorm(d_model)        hidden = int(d_model * mlp_ratio)        self.mlp = nn.Sequential(            nn.Linear(d_model, hidden), nn.GELU(), nn.Dropout(p),            nn.Linear(hidden, d_model), nn.Dropout(p)        )    def forward(self, x):        x = x + self.drop1(self.ssm(self.norm1(x)))        x = x + self.mlp(self.norm2(x))        return x

注：上例为思想演示，真实Mamba使用更优化的扫描与参数化，且为高效CUDA/Flash实现；请参考官方实现。

实验要点与经验

序列长度：尽量使用长序列预训练以发挥线性复杂度优势；
批量与块大小：根据显存调优块扫描长度，保持吞吐与稳定；
任务迁移：语言→语音/时间序列时，适当调整状态规模与门控强度；
可解释性：通过可视化门控开启位置与核响应，分析模型关注点。

优缺点
优点

线性复杂度，适合超长序列与端侧低时延应用；
选择性门控使得对稀疏/非稳态事件更敏感；
GPU友好实现，训练/推理吞吐高；
理论上可与注意力/卷积并行或互补，形成混合结构。

缺点

对全局两两交互的显式建模不如注意力直观；
超参数（状态规模、门控强度、离散化方式）对稳定性敏感；
在某些视觉任务（如密集预测）上需额外结构适配（见MambaOut）。

相关与后续工作

S4/S5：结构化SSM与其后续改进；
MambaOut：面向视觉分类的实用化改造；
Hyena/RetNet/WSI：其他线性/次线性序列混合范式；
混合架构：SSM + Attention/Conv的多路并行。

参考文献


Gu, A., &amp; Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752. https://arxiv.org/pdf/2312.00752


Gu, A., Goel, K., &amp; Ré, C. (2022). Efficiently Modeling Long Sequences with Structured State Spaces. ICML 2022.


Sun, Y., et al. (2023). RetNet: Retentive Network: A Successor to Transformer for Large Language Models. NeurIPS 2023.


Poli, M., et al. (2023). Hyena Hierarchy: Towards Larger Convolutional Language Models. ICML 2023.


思考题

为什么选择性扫描能在保持线性复杂度的同时提升表达力？
如何在保持稳定性的同时引入更强的输入条件化（例如多尺度门控）？
在语言建模中，Mamba与Transformer是否适合分工合作？如何设计混合路由？
针对超长上下文（&gt;64K tokens），Mamba的块并行与跨块信息传递应如何权衡？

思考题答案

选择性扫描通过输入依赖的门控让状态更新“稀疏化/聚焦化”，避免固定核对非稳态信号的欠拟合，同时保持线性递推形式；因此表达力↑、复杂度仍为O(N)。
采用分层门控（通道/时间/块级多粒度）、稳定化参数化（对角+低秩）、以及正则（门控L1/温度限制）可提升条件化同时维持稳定；
可用“前段SSM提取远距记忆 + 中段注意力做细粒度交互”的混合体，或MoE式路由将易于局部建模的片段交给Conv/SSM，需在延迟与精度之间折中；
提高块大小、增加跨块汇总通道（summary tokens）、或在块边界引入轻量全局模块（如稀疏注意力）以减小信息切换损失；同时用检查点与流水线降低内存峰值。

]]></content>
      <categories>
        <category>论文精读</category>
        <category>序列建模</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>Mamba</tag>
        <tag>SSM</tag>
        <tag>序列建模</tag>
      </tags>
  </entry>
  <entry>
    <title>MambaOut</title>
    <url>/2025/09/MambaOut/</url>
    <content><![CDATA[MambaOut
MambaOut: Do We Really Need Mamba for Vision?  (CVPR 2025)
论文地址：https://arxiv.org/pdf/2405.07992
代码地址：https://github.com/yuweihao/MambaOut
概述
Mamba最初为解决注意力的二次复杂度提出（线性时空复杂度、长序列友好），但在视觉分类等任务上往往不如卷积或Transformer稳定。MambaOut系统性分析了&quot;何时以及为何Mamba在视觉上不占优&quot;，并提出一种在视觉分类上更实用的简化设计：保留有益的序列建模特性，同时移除对视觉任务不友好的模块（如SSM核心token混合器），通过堆叠精简的Mamba块，获得在ImageNet上的强竞争力。
研究动机深度分析
Mamba在NLP中的成功要素：

因果自回归特性：文本生成天然需要单向建模
超长序列处理：文档级任务需要处理数万token
时序依赖建模：语言具有明确的前后文关系
状态演化机制：SSM能有效编码历史信息

为什么这些优势在视觉中不明显？

双向语义理解：图像理解需要全局信息，不适合单向扫描
序列长度有限：即使是高分辨率图像，patch数量通常在千级别
空间结构敏感：图像的2D空间结构被强制展平会损失信息
局部性优先：视觉特征的局部性比全局依赖更重要

核心洞察
本文的核心洞察在于：视觉任务的成功不需要复杂的序列建模机制。通过系统的消融实验，作者发现：

移除SSM后，模型性能不降反升
简单的token混合（如深度可分离卷积）效果更好
训练稳定性显著改善，不再需要复杂的初始化策略

主要结论与贡献

视觉分类不具备&quot;长序列+自回归&quot;的天然优势场景，原生Mamba的强项难以发挥。
去除SSM等不利于图像表征的混合器，保留高效的序列建模后，分类性能显著提升。
提出MambaOut：在相似FLOPs下，ImageNet上优于现有视觉Mamba变体；训练更稳定、收敛更容易。

关键发现与洞察
1. SSM在视觉中的问题
通过系统的消融实验，论文发现了几个关键问题：

单向扫描不适合2D图像：图像的空间关系是双向的，不像文本的因果关系
状态演化增加复杂性：SSM的状态更新机制在视觉任务中带来额外计算负担，收益有限
选择性机制效果有限：Mamba的选择性扫描在图像patch序列上没有明显优势

2. 简化带来的改进
移除SSM组件后的改进：

训练更稳定：梯度流更简单，不再有复杂的状态更新
收敛更快：150 epochs vs 200 epochs达到相同性能
性能更好：在ImageNet上提升1-2%准确率

3. 关键设计选择
MambaOut保留的关键组件：

Gated MLP：提供必要的非线性变换
层归一化：稳定训练过程
残差连接：保证梯度流通
简单的token混合：使用深度可分离卷积替代SSM

实验设计的巧妙之处
渐进式消融策略：

首先移除选择性扫描 → 性能略有提升
然后移除状态空间模型 → 性能继续提升
最后用简单混合器替代 → 达到最佳性能

这种渐进式方法清晰地展示了每个组件的贡献（或负贡献）。
公平比较的设置：

保持参数量相近（误差&lt;5%）
保持FLOPs相近（误差&lt;10%）
使用相同的训练配置（optimizer、schedule、augmentation）

这确保了性能提升来自架构改进，而非其他因素。
方法与架构
基本思想

图像分类核心在于“空间结构与语义的高效建模”，无需严格自回归；
简化Mamba的状态空间混合，将其作为高效token混合器使用；
通过多层堆叠，利用线性复杂度优势扩展模型深度与序列长度。

模块结构（示意）

Patch Embedding：将图像划分为patch并投影为token序列；
MambaOut Block：

轻量token混合（保留线性高效特性）
前馈网络（FFN）
残差连接与归一化


分类头：全局池化/CLS + 线性分类器

相较ViT，MambaOut在token交互上更偏线性序列建模，复杂度更低；相较原生Mamba，MambaOut去掉不利视觉任务的SSM，强调稳定性与判别性。
与相关方法对比

ViT（注意力）

优点：全局建模强、表现稳定
缺点：注意力复杂度二次、长序列成本高


原生Mamba（SSM）

优点：线性复杂度，长序列/自回归强
缺点：在图像分类上表现不稳定/欠鲁棒


MambaOut（本文）

优点：保留线性高效的优点，去除不利视觉模块，分类更强
缺点：对密集预测（检测/分割）的泛化仍需更多验证



实验结果
ImageNet-1K分类性能
主要结果对比（ImageNet-1K validation set）：



Model
Params
FLOPs
Top-1 Acc




DeiT-S
22M
4.6G
79.8%


Swin-T
29M
4.5G
81.3%


Vim-S
26M
5.1G
80.5%


VMamba-S
26M
5.1G
81.5%


MambaOut-S
24M
4.5G
82.0%


DeiT-B
86M
17.5G
81.8%


Swin-B
88M
15.4G
83.3%


Vim-B
98M
18.2G
81.8%


VMamba-B
89M
18.0G
82.8%


MambaOut-B
85M
15.8G
83.6%



关键发现：

性能优势：MambaOut在各个模型规模上均超越视觉Mamba变体
计算效率：在相似或更少的FLOPs下实现更高精度
参数效率：参数量略少但性能更优

训练稳定性分析
收敛速度对比：

MambaOut-S：150 epochs即可达到80%准确率
Vim-S：需要200 epochs才能达到相同性能
训练损失下降更平滑，无明显震荡

超参数鲁棒性：

学习率范围：MambaOut在1e-4到5e-3范围内都能稳定训练
Warmup需求：仅需5 epochs warmup（vs. Vim需要20 epochs）
数据增强：对各种增强策略（RandAugment、Mixup、CutMix）响应良好

消融实验详细分析
SSM组件消融：



Component
Top-1 Acc
Training Time




Full Mamba
79.8%
1.0x


w/o selective scan
80.5%
0.85x


w/o state space
81.2%
0.75x


MambaOut (all removed)
82.0%
0.70x



Token混合器对比：



Mixer Type
Top-1 Acc
FLOPs




SSM
79.8%
4.8G


Self-Attention
81.0%
5.2G


Depthwise Conv
81.5%
4.3G


Gated Conv
82.0%
4.5G



消融与分析

去除SSM的影响：显著提升稳定性与最终精度；
深度/宽度扩展：线性复杂度便于堆叠更多层，收益更稳定；
数据增强/正则：与ViT常规配置兼容（Mixup/CutMix/Label Smoothing等）。

工程实践建议

预处理：标准的ImageNet增强流程足够（RandAug/RandomResizedCrop/HFlip）。
优化器：AdamW + Cosine LR + Warmup；
批大小：和ViT训练设置一致即可，梯度累积可平衡显存；
初始化：可复用ViT/Conv stem进行更稳定的早期训练。

代码片段（概念化示例）
import torchimport torch.nn as nnclass MambaOutBlock(nn.Module):    def __init__(self, dim, mlp_ratio=4.0, p=0.0):        super().__init__()        self.norm1 = nn.LayerNorm(dim)        # 轻量token混合：可替换为线性时序混合或深度可分离卷积等高效混合器        self.token_mixer = nn.Sequential(            nn.Conv1d(dim, dim, kernel_size=3, padding=1, groups=dim),            nn.Conv1d(dim, dim, kernel_size=1)        )        self.drop1 = nn.Dropout(p)        self.norm2 = nn.LayerNorm(dim)        hidden = int(dim * mlp_ratio)        self.mlp = nn.Sequential(            nn.Linear(dim, hidden), nn.GELU(), nn.Dropout(p),            nn.Linear(hidden, dim), nn.Dropout(p)        )    def forward(self, x):        # x: [B, N, C]        y = self.norm1(x)        y = y.transpose(1, 2)              # [B, C, N]        y = self.token_mixer(y).transpose(1, 2)        x = x + self.drop1(y)        x = x + self.mlp(self.norm2(x))        return x

注：上面用深度可分离卷积近似“线性复杂度的token混合”思想，便于理解；实际论文实现请以官方代码为准。

优缺点
优点

训练与收敛更稳定，相比视觉Mamba变体更易用；
线性复杂度便于扩展深度与序列长度；
与ViT训练技巧兼容，工程门槛低。

缺点

对检测、分割等密集预测任务的系统性评估仍有限；
理论解释仍在发展中，对何种视觉任务最适合有待进一步量化。

总结
MambaOut的核心在于：针对视觉分类的需求，对Mamba做“有保留的简化”。它保留线性复杂度与高效序列建模优势，去除对视觉不友好的SSM，最终在ImageNet上超过其他视觉Mamba模型，并接近/超越同级ViT。对希望探索Transformer之外高效结构的研究者与工程师而言，MambaOut提供了一条可行、稳健的路线。
参考

MambaOut: Do We Really Need Mamba for Vision? (CVPR 2025)
Mamba: Linear-Time Sequence Modeling with Selective SSMs (2023)
ViT: An Image is Worth 16x16 Words (ICLR 2021)

思考题

为什么移除SSM会提升分类稳定性？是否可以保留部分SSM结构以兼顾泛化？
线性复杂度混合在极高分辨率或长序列输入下的收益与瓶颈是什么？
在检测/分割等任务上，MambaOut需要哪些针对性的修改？
与ViT的多头注意力相比，MambaOut的token混合在哪些场景更具优势？

思考题答案
1. 为什么移除SSM会提升分类稳定性？是否可以保留部分SSM结构以兼顾泛化？

分类不需要严格自回归，SSM的选择性状态更新在图像场景中可能引入不必要的时序归纳偏置，导致优化目标与任务不匹配；
移除后，token混合更“判别化”且梯度传播路径更稳定；
可尝试“部分保留”：仅在深层或低分辨率阶段加入简化SSM，或以门控/残差旁路的形式弱耦合，避免主干表征被SSM主导。

2. 线性复杂度混合在极高分辨率或长序列输入下的收益与瓶颈是什么？

收益：复杂度O(N)便于扩展到超长序列/高分辨率，显存和速度更友好；
瓶颈：缺乏显式全局交互的建模能力，远程依赖捕获可能弱于注意力；
实践：可叠加稀疏/分层全局模块（如跨块聚合、低频通道注意力）补足长距关系。

3. 在检测/分割等任务上，MambaOut需要哪些针对性的修改？

多尺度特征：引入FPN/HRNet式多尺度分支，适配密集预测；
局部-全局结合：在高分辨率阶段加入局部卷积/窗口注意力以提升细节；
解码头适配：与常见检测/分割头（Mask R-CNN、UPerNet、Deeplab）对齐接口与特征尺度；
训练策略：更强的数据增强（大尺度抖动、copy-paste）与正负样本平衡。

4. 与ViT的多头注意力相比，MambaOut的token混合在哪些场景更具优势？

超长序列与资源受限训练：线性复杂度在显存与时间上更占优；
分类/检索等判别任务：无需强全局两两交互即可取得良好表现；
端侧/实时场景：更低的计算开销带来更稳定的延迟与能耗。

]]></content>
      <categories>
        <category>论文精读</category>
        <category>序列建模</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>Mamba</tag>
      </tags>
  </entry>
  <entry>
    <title>MoCo</title>
    <url>/2025/04/MoCo/</url>
    <content><![CDATA[MoCo
Momentum Contrast for Unsupervised Visual Representation Learning  (cvpr2020)
论文地址：https://arxiv.org/pdf/1911.05722
代码地址：https://github.com/facebookresearch/moco
概述

MoCo 将对比学习看作是一个字典查找任务 ：一个编码后的查询（query）应该与其匹配的键（正样本）相似，而与其他所有的键（负样本）不相似 。
对比学习的核心思想是训练一个编码器，使其能够区分相似（正样本）和不相似（负样本）的样本 。
传统方法 VS MoCo

端到端（End-to-end）方法（SimCLR，Inva Spread）：将当前 mini-batch 内的样本作为字典 。这种方法的优点是字典中的键编码是一致的（由同一个编码器生成），但缺点是字典的大小受限于 mini-batch 的大小，而 mini-batch 大小又受限于 GPU 内存 。过大的 mini-batch 也会带来优化难题 。
Memory Bank 方法（Inst Disc）：Memory Bank包含数据集所有数据的特征表示，从Memory Bank中采样数据不需要进行反向传播，所以能支持比较大的字典，然而一个样本的特征表示只在它出现时才在Memory Bank更新，所以一个epoch只会更新一次，但模型在训练过程中不断迭代，这个特征就会“过时”，因此具有更少的一致性，而且它的更新只是进行特征表示的更新，不涉及encoder。
MoCo跟Inst Disc是非常相似的，比如它用队列取代了原来的memory bank作为一个额外的数据结构去存储负样本，用动量编码器去取代了原来loss里的约束项，这样就可以动量的更新编码器，而不是动量的去更新特征，从而能得到更好的结果。其整体的出发点以及一些实现的细节（比如backbone和lr、batch_size，dim、τ等等超参数都是一样的）和Inst Disc都是非常类似的，所以可以说MoCo是Inst Disc的改进工作。
创新点
MoCo 通过两个创新点解决了这一问题：基于队列的动态字典和动量编码器，给无监督的对比学习构造了一个又大又一致的字典。
基于队列的动态字典
MoCo 使用一个队列来存储图像表征（称为键），作为动态字典。通过这种“先进先出”的更新方式，队列始终保持固定的大小，并且包含的是相对较新的样本表示，有助于维持字典的一致性。队列可以容纳 65,536 个键，为对比学习提供了丰富的负样本集合。
f_k.params = f_q.params # 初始化for x in loader: # 输入一个图像序列x，包含N张图，没有标签    x_q = aug(x) # 用于查询的图（数据增强得到）    x_k = aug(x) # 模板图（数据增强得到），自监督就体现在这里，只有图x和x的数据增强才被归为一类    q = f_q.forward(x_q) # 提取查询特征，输出NxC    k = f_k.forward(x_k) # 提取模板特征，输出NxC    # 不使用梯度更新f_k的参数，这是因为文章假设用于提取模板的表示应该是稳定的，不应立即更新    k = k.detach()     # 这里bmm是分批矩阵乘法    l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # 输出Nx1，也就是自己与自己的增强图的特征的匹配度    l_neg = mm(q.view(N,C), queue.view(C,K)) # 输出Nxk，自己与上一批次所有图的匹配度（全不匹配）    logits = cat([l_pos, l_neg], dim=1) # 输出Nx(1+k)    labels = zeros(N)    # NCE损失函数，就是为了保证自己与自己衍生的匹配度输出越大越好，否则越小越好    loss = CrossEntropyLoss(logits/t, labels)     loss.backward()    update(f_q.params) # f_q使用梯度立即更新    # 由于假设模板特征的表示方法是稳定的，因此它更新得更慢，这里使用动量法更新，相当于做了个滤波。    f_k.params = m*f_k.params+(1-m)*f_q.params     enqueue(queue, k) # 为了生成反例，所以引入了队列    dequeue(queue)
动量编码器
MoCo 使用两个编码器：查询编码器（query encoder）和键编码器（key encoder）。查询编码器通过反向传播正常更新，而键编码器是查询编码器的缓慢更新版本，其参数通过动量机制更新。具体来说，键编码器的参数是其之前参数与查询编码器参数的加权平均，动量系数（momentum coefficient）通常设为 0.999，使得键编码器变化非常缓慢。这种稳定性保证了队列中键的表征不会因编码器快速变化而失去一致性。
$$
\theta_k \leftarrow m \theta_k + (1-m) \theta_q
$$
query的编码器和key的编码器既可以是相同的（模型的架构一样，参数完全共享，比如Inva Spread），或者说它们的参数是部分共享的，也可以是彻底不一样的两个网络（CMC，多视角多编码器）。
Pretext Task（代理任务）
论文中主要采用了一个简单的实例判别任务（Instance Discrimination）：如果一个查询和一个键来源于同一张图片的不同随机增强视图（例如，不同的裁剪、颜色抖动等），则它们构成一个正样本对；否则构成负样本对 。编码器（如 ResNet ）将这些视图编码为特征向量，然后通过 InfoNCE 损失函数进行优化 。
损失函数
在对比学习中，最常用的目标函数是 InfoNCE 损失（Noise-Contrastive Estimation 的信息理论扩展）。InfoNCE 损失基于互信息最大化的思想，鼓励模型区分正样本对和负样本对。
$$
\mathcal L_{q} = - \log \frac{\exp(q \cdot k_{+} / \tau)}{\sum_{i=0}^{K} \exp(q \cdot k_{i} / \tau)}
$$
$ q $ 是查询的编码表示。
$ k_{+} $ 是正样本键的编码表示。
$ k_i $ 是字典中所有的键（包括一个正样本 $ k_{+} $ 和 K 个负样本）。
$ \tau $ 是超参数，用来控制分布的形状 ，去掉后整个式子其实就是交叉熵损失函数。
$ q \cdot k $ 表示点积相似度。
NCE loss（noise contrastive estimation ）：将超级多分类转为二分类——数据类别data sample和噪声类别noisy sample。这样解决了类别多的问题。
]]></content>
      <categories>
        <category>论文精读</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>对比学习</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识--STL</title>
    <url>/2025/02/STL/</url>
    <content><![CDATA[标准模板库（STL）
STL 即标准模板库（Standard Template Library），是 C++  标准库的一部分，里面包含了一些模板化的通用的数据结构和算法。由于其模板化的特点，它能够兼容自定义的数据类型，避免大量的造轮子工作。NOI 和  ICPC 赛事都支持 STL 库的使用，因此合理利用 STL 可以避免编写无用算法，并且充分利用编译器对模板库优化提高效率。
STL容器

迭代器
​	在 STL 中，迭代器（Iterator）用来访问和检查 STL 容器中元素的对象，它的行为模式和指针类似，但是它封装了一些有效性检查，并且提供了统一的访问格式。
​	迭代器听起来比较晦涩，其实迭代器本身可以看作一个数据指针。迭代器主要支持两个运算符：自增 (++) 和解引用（单目 * 运算符），其中自增用来移动迭代器，解引用可以获取或修改它指向的元素。
vector&lt;int&gt; data(10);//下面两个 for 循环的效果是一样的for (int i = 0; i &lt; data.size(); i++)  cout &lt;&lt; data[i] &lt;&lt; endl;  // 使用下标访问元素for (vector&lt;int&gt;::iterator iter = data.begin(); iter != data.end(); iter++)  cout &lt;&lt; *iter &lt;&lt; endl;  // 使用迭代器访问元素// 在C++11后可以使用 auto iter = data.begin() 来简化上述代码
​	STL 容器 一般支持从一端或两端开始的访问，以及对 const 修饰符 的支持。例如容器的 begin() 函数可以获得指向容器第一个元素的迭代器，rbegin() 函数可以获得指向容器最后一个元素的反向迭代器，cbegin() 函数可以获得指向容器第一个元素的 const 迭代器，end() 函数可以获得指向容器尾端（「尾端」并不是最后一个元素，可以看作是最后一个元素的后继；「尾端」的前驱是容器里的最后一个元素，其本身不指向任何一个元素）的迭代器。
序列式容器

向量(vector) 后端可高效增加元素的顺序表。
双端队列(deque) 双端都可高效增加元素的顺序表。
列表(list) 可以沿双向遍历的链表。
单向列表(forward_list) 只能沿一个方向遍历的链表。

vector
std::vector 是 STL 提供的 内存连续的、可变长度 的数组（亦称列表）数据结构。能够提供线性复杂度的插入和删除，以及常数复杂度的随机访问。
vector定义
#include&lt;vector&gt;                       //头文件vector&lt;typename&gt; name;                 //相当于长度可以变化的一维数组vector&lt;vector&lt;typename&gt; &gt; name;        //相当于长度可以变化的一维数组vector&lt;typename&gt; Arrayname[arraySize]; //一维长度固定为arraySize，Arrayname[0]~Arrayname[arraySize-1]中每一个都是一个vector容器//typename可以是任何基本类型，例如int,double,char,结构体等，也可以是STL标准容器，例如vector,set,queue等
// 1. 创建空vector; 常数复杂度vector&lt;int&gt; v0;// 1+. 这句代码可以使得向vector中插入前3个元素时，保证常数时间复杂度v0.reserve(3);// 2. 创建一个初始空间为3的vector，其元素的默认值是0; 线性复杂度vector&lt;int&gt; v1(3);// 3. 创建一个初始空间为3的vector，其元素的默认值是2; 线性复杂度vector&lt;int&gt; v2(3, 2);// 4. 创建一个初始空间为3的vector，其元素的默认值是1，// 并且使用v2的空间配置器; 线性复杂度vector&lt;int&gt; v3(3, 1, v2.get_allocator());// 5. 创建一个v2的拷贝vector v4， 其内容元素和v2一样; 线性复杂度vector&lt;int&gt; v4(v2);// 6. 创建一个v4的拷贝vector v5，其内容是&#123;v4[1], v4[2]&#125;; 线性复杂度vector&lt;int&gt; v5(v4.begin() + 1, v4.begin() + 3);// 7. 移动v2到新创建的vector v6，不发生拷贝; 常数复杂度; 需要 C++11vector&lt;int&gt; v6(std::move(v2));  // 或者 v6 = std::move(v2);
vector容器内元素的访问
//1.通过下标访问vector&lt;int&gt; m;//直接访问m[index],index从0到m.size()-1//2.通过迭代器访问vector&lt;int&gt;::iterator it;//迭代器就像STL容器的“指针”，可以用*it访问vector中的元素//m[i]和*(m.begin()+i)等价
#include&lt;stdio.h&gt;#include&lt;vector&gt;using namespace std;int main()&#123;    vector&lt;int&gt; vi;    for(int i=1;i&lt;=5;i++) vi.push_back(i);    //1    vector&lt;int&gt;::iterator it = vi.begin();    for(int i=0;i&lt;5;i++)    &#123;        printf(&quot;%d&quot;,*(it+i)); //输出 1 2 3 4 5    &#125;    //2    for(vector&lt;int&gt;::iterator it = vi.begin();it!=vi.end();it++)    &#123;        printf(&quot;%d&quot;,*it);    //输出 1 2 3 4 5    &#125;    return 0;&#125;
vector常用函数


push_back() 在末尾插入一个元素，均摊复杂度为 常数，最坏为线性复杂度。


pop_back() 删除末尾元素，常数复杂度。


front()返回vector的第一个元素，等价于*a.begin() 和 a[0]。


back()返回vector的最后一个元素，等价于*==a.end() 和 a[a.size() – 1]。


size() 返回容器长度（元素数量），即 std::distance(v.begin(), v.end())。


v.data() 返回指向数组第一个元素的指针。


insert() 支持在某个迭代器位置插入元素、可以插入多个。insert(it,x)在it处插入x。


clear() 清除所有元素。


erase() 删除某个迭代器或者区间的元素，返回最后被删除的迭代器。


erase(it)删除迭代器为it处的元素，erase(first，last)即删除[first，last)内所有元素。
deque
std::deque（双端队列）支持在头部和尾部进行高效插入删除的序列容器，与vector相比：

头尾插入/删除时间复杂度为O(1)
支持随机访问（通过下标）
存储空间分块管理，迭代器比vector复杂
中间插入删除效率较低

deque定义
#include &lt;deque&gt;std::deque&lt;int&gt; dq1;              // 空双端队列std::deque&lt;char&gt; dq2(5, &#x27;A&#x27;);     // 包含5个&#x27;A&#x27;std::deque&lt;int&gt; dq3 = &#123;1,2,3&#125;;    // 列表初始化（C++11）
deque容器内元素的访问



方式
示例
说明




下标访问
dq[0]
无越界检查


at() 方法
dq.at(0)
有越界检查，越界抛出异常


首元素
dq.front()
等价于 dq[0]


末尾元素
dq.back()
等价于 dq[dq.size()-1]



deque常用函数



函数
功能说明




push_front(x)
在头部插入元素x


pop_front()
删除头部元素


push_back(x)
在尾部插入元素x


pop_back()
删除尾部元素


insert(pos, x)
在迭代器pos前插入元素x


erase(pos)
删除迭代器pos指向的元素


resize(n)
调整容器大小为n


shrink_to_fit()
请求移除未使用的容量（C++11）



特殊说明
// 头尾操作示例std::deque&lt;int&gt; dq;dq.push_front(2);    // 头部插入：dq = [2]dq.push_back(3);     // 尾部插入：dq = [2,3]dq.pop_front();      // 删除头部：dq = [3]// 随机访问示例dq[0] = 5;          // 修改第一个元素
综合对比



特性\容器
vector
deque




头插效率
O(n)
O(1)


尾插效率
均摊O(1)
O(1)


中间插入
O(n)
O(n)


内存布局
单块连续内存
多块连续内存


迭代器失效
容易失效
部分操作不失效


缓存友好性
高
较低



关联式容器

集合(set) 用以有序地存储 互异 元素的容器，其实现是由节点组成的红黑树。
多重集合(multiset) 用以有序地存储元素的容器。允许存在相等的元素。

​	头文件set，主要包括set和multiset两个容器，分别是“有序集合”和“有序多重集合”，即前者的元素不能重复，而后者可以包含若干个相等的元素。set和multiset的内部实现是一棵红黑树，它们支持的函数基本相同。
set
set 是关联容器，含有键值类型对象的已排序集，搜索、移除和插入拥有对数复杂度。set 内部通常采用 红黑树 实现。平衡二叉树 的特性使得 set 非常适合处理需要同时兼顾查找、插入与删除的情况。
set定义
#include&lt;set&gt;set&lt;typename&gt; name;    //set数组的定义和vector相同set&lt;typename&gt; Arrayname[arraySize];
set容器内元素的访问
set只能通过迭代器访问
set和multiset的迭代器称为“双向访问迭代器”，不支持“随机访问”，支持星号(*)解除引用，仅支持”++”和–“两个与算术相关的操作。
若把it++，则it会指向“下一个”元素。这里的“下一个”元素是指在元素从小到大排序的结果中，排在it下一名的元素。同理，若把it–，则it将会指向排在“上一个”的元素。
set&lt;typename&gt;::iterator it;   //这样就得到了迭代器，可以通过*it来访问set
#include &lt;stdio.h&gt;#include &lt;set&gt;using namespace std;int main()&#123;    set&lt;int&gt; st;    st.insert(1);    st.insert(5);    st.insert(3);    st.insert(3);    for(set&lt;int&gt;::iterator it = st.begin();it != st.end();it++)    &#123;        printf(&quot;%d&quot;,*it);    &#125;    return 0;&#125;//输出结果：1 3 5
set常用函数


insert(x) 当容器中没有等价元素的时候，将元素 x 插入到 set 中。


find(x) 在 set 内存在键为 x 的元素时会返回该元素的迭代器，否则返回 end()。


erase(x) 删除值为 x 的 所有 元素，返回删除元素的个数。


​      erase(pos) 删除迭代器为 pos 的元素，要求迭代器必须合法。
​      erase(first,last) 删除迭代器在 [first,last) 范围内的所有元素。


clear() 清空 set。


count(x) 返回 set 内键为 x 的元素数量。


lower_bound(x) 返回指向首个不小于给定键的元素的迭代器。如果不存在这样的元素，返回 end()。


upper_bound(x) 返回指向首个大于给定键的元素的迭代器。如果不存在这样的元素，返回 end()。


empty() 返回容器是否为空。


size() 返回容器内元素个数。


// 现存可用的元素set&lt;int&gt; available;// 需要大于等于的值int x;// 查找最小的大于等于x的元素set&lt;int&gt;::iterator it = available.lower_bound(x);if (it == available.end()) &#123;  // 不存在这样的元素，则进行相应操作……&#125; else &#123;  // 找到了这样的元素，将其从现存可用元素中移除  available.erase(it);  // 进行相应操作……&#125;

映射(map) 由 {键，值} 对组成的集合，以某种比较键大小关系的谓词进行排列。
多重映射(multimap) 由 {键，值} 对组成的多重集合，亦即允许键有相等情况的映射。

map
map 是有序键值对容器，它的元素的键是唯一的。搜索、移除和插入操作拥有对数复杂度。map 通常实现为 红黑树。
map定义
map&lt;key,value&gt; mp;//字符串映射到整型，必须使用string而不能用char数组map&lt;string,int&gt; mp;//将set容器映射到字符串map&lt;set&lt;int&gt;, string&gt; mp;
map容器内元素的访问
//1.通过下标访问map&lt;char,int&gt; mp;//直接使用mp[&#x27;c&#x27;]来访问对应的整数，map中的键是唯一的。#include &lt;stdio.h&gt;#include &lt;map&gt;using namespace std;int main()&#123;    map&lt;char,int&gt; mp;    mp[&#x27;c&#x27;]=20;    mp[&#x27;c&#x27;]=30;    printf(&quot;%d\n&quot;,mp[&#x27;c&#x27;]);  //输出30    return 0;&#125;//2.通过迭代器访问map&lt;key,value&gt;::iterator it;//map可以使用it-&gt;first来访问键，使用it-&gt;second来访问值。#include &lt;stdio.h&gt;#include &lt;map&gt;using namespace std;int main()&#123;    map&lt;char,int&gt; mp;    mp[&#x27;m&#x27;]=20;    mp[&#x27;r&#x27;]=30;    mp[&#x27;a&#x27;]=40;    for(map&lt;char,int&gt;::iterator it = mp.begin();it!=mp.end();it++)    &#123;        printf(&quot;%c %d\n&quot;,it-&gt;first,it-&gt;second);    &#125;            return 0;&#125;// 输出：map会以键从小到大的顺序自动排序// a 40// m 20// r 30
map常用函数


通过向 map 中插入一个类型为 pair&lt;Key, T&gt; 的值可以达到插入元素的目的，例如 mp.insert(pair&lt;string,int&gt;(&quot;Alan&quot;,100));；


find(x): 若容器内存在键为 x 的元素，会返回该元素的迭代器；否则返回 end()。


erase(key) 函数会删除键为 key 的 所有 元素。返回值为删除元素的数量。


​       erase(pos): 删除迭代器为 pos 的元素，要求迭代器必须合法。
​       erase(first,last): 删除迭代器在 [first,last) 范围内的所有元素。


clear() 函数会清空整个容器。


count(x): 返回容器内键为 x 的元素数量。复杂度为 （关于容器大小对数复杂度，加上匹配个数)。


lower_bound(x): 返回指向首个不小于给定键的元素的迭代器。


upper_bound(x): 返回指向首个大于给定键的元素的迭代器。若容器内所有元素均小于或等于给定键，返回 end()。


empty(): 返回容器是否为空。


size(): 返回容器内元素个数。


无序（关联式）容器


无序（多重）集合(unordered_set/unordered_multiset)C++11，与 set/multiset 的区别在于元素无序，只关心「元素是否存在」，使用哈希实现。


无序（多重）映射(unordered_map/unordered_multimap)C++11，与 map/multimap 的区别在于键 (key) 无序，只关心 “键与值的对应关系”，使用哈希实现。


容器适配器
容器适配器其实并不是容器。它们不具有容器的某些特点（如：有迭代器、有 clear() 函数……）。

「适配器是使一种事物的行为类似于另外一种事物行为的一种机制」，适配器对容器进行包装，使其表现出另外一种行为。


栈(stack) 后进先出 (LIFO) 的容器，默认是对双端队列（deque）的包装。
队列(queue) 先进先出 (FIFO) 的容器，默认是对双端队列（deque）的包装。
优先队列(priority_queue) 元素的次序是由作用于所存储的值对上的某种谓词决定的的一种队列，默认是对向量（vector）的包装。

stack（栈）
STL 栈(std::stack) 是一种后进先出 (Last In, First Out) 的容器适配器，仅支持查询或删除最后一个加入的元素（栈顶元素），不支持随机访问，且为了保证数据的严格有序性，不支持迭代器。用来模拟实现一些递归。
stack定义
#include&lt;stack&gt;std::stack&lt;TypeName&gt; name;  // 使用默认底层容器 deque，数据类型为 TypeNamestd::stack&lt;TypeName, Container&gt; name;  // 使用 Container 作为底层容器std::stack&lt;TypeName&gt; s2(s1);        // 将 s1 复制一份用于构造 s2
stack容器内元素的访问
由于栈本身就是一种后进先出的数据结构，在STL的stack中只能通过top()来访问栈顶元素。
stack常用函数


top() 访问栈顶元素（如果栈为空，此处会出错）


push(x) 向栈中插入元素 x


pop() 删除栈顶元素


size() 查询容器中的元素数量


empty() 询问容器是否为空


queue（队列）
STL 队列(std::queue) 是一种先进先出 (First In, First Out) 的容器适配器，仅支持查询或删除第一个加入的元素（队首元素），不支持随机访问，且为了保证数据的严格有序性，不支持迭代器。
queue定义
#include&lt;queue&gt;std::queue&lt;TypeName&gt; name;  // 使用默认底层容器 deque，数据类型为 TypeNamestd::queue&lt;TypeName, Container&gt; name;  // 使用 Container 作为底层容器std::queue&lt;TypeName&gt; q2(q1);  // 将 s1 复制一份用于构造 q2
queue容器内元素的访问
由于队列本身就是一种先进先出的数据结构，在STL的queue中只能通过front()来访问队首元素，通过back()访问队尾元素。
queue常用函数


front() 访问队首元素（如果队列为空，此处会出错）


back() 访问队尾元素


push(x) 向队列中插入元素 x


pop() 删除队首元素


size() 查询容器中的元素数量


empty() 询问容器是否为空


priority_queue（优先队列）
优先队列 std::priority_queue 是一种 堆，一般为 二叉堆。队首元素一定是当前队列中优先级最高的那一个。
priority_queue定义
#include&lt;queue&gt;priority_queue&lt;TypeName&gt; name;             // 数据类型为 TypeNamepriority_queue&lt;TypeName, Container&gt; name;  // 使用 Container 作为底层容器priority_queue&lt;TypeName, Container, Compare&gt; name;// 使用 Container 作为底层容器，使用 Compare 作为比较类型
priority_queue容器内元素的访问
和队列不一样的是，优先队列没有front()和back()，只能通过top()访问队首元素(堆顶元素)。
priority_queue常用函数


top() 访问堆顶元素（此时优先队列不能为空）


pop() 删除堆顶元素（此时优先队列不能为空）


push(x) 插入元素，并对底层容器排序


size() 查询容器中的元素数量


empty() 询问容器是否为空


STL算法
STL 提供了大约 100 个实现算法的模版函数，基本都包含在 &lt;algorithm&gt; 之中，还有一部分包含在 &lt;numeric&gt; 和 &lt;functional&gt;。
algorithm
&lt;algorithm&gt; 头文件提供了大量通用算法，适用于多种容器。所有算法均通过迭代器操作，不对容器进行直接修改（除非明确说明）。
常用算法列表



算法
功能说明




sort(beg, end, cmp)
对区间[beg,end)排序，cmp为可选比较函数（默认升序）


reverse(beg, end)
反转指定区间的元素顺序


max(a, b) / min(a, b)
返回两个值的较大/较小值（C++11支持初始化列表：max(&#123;1,2,3&#125;)）


swap(a, b)
交换两个变量的值


find(beg, end, val)
在区间内查找值，返回首个匹配的迭代器，未找到返回end


count(beg, end, val)
统计区间内指定值出现的次数


fill(beg, end, val)
用指定值填充区间


copy(src_beg, src_end, dest_beg)
复制源区间到目标位置


unique(beg, end)
去除相邻重复元素，返回去重后的新结尾迭代器（通常先排序后使用）


lower_bound(beg, end, val)
在有序区间中找第一个不小于val的元素位置


upper_bound(beg, end, val)
在有序区间中找第一个大于val的元素位置


binary_search(beg, end, val)
检查有序区间中是否存在指定值



典型使用示例
// 自定义排序std::vector&lt;int&gt; vec = &#123;3,1,4,2&#125;;std::sort(vec.begin(), vec.end(), [](int a, int b)&#123; return a &gt; b; &#125;); // 降序排列// 去重操作std::sort(vec.begin(), vec.end());auto last = std::unique(vec.begin(), vec.end());vec.erase(last, vec.end());// 查找元素auto it = std::find(vec.begin(), vec.end(), 3);if (it != vec.end()) std::cout &lt;&lt; &quot;Found: &quot; &lt;&lt; *it;
pair
std::pair 是一个模板类，用于将两个值组合成一个单元。常用于需要返回两个值的场景，或作为map容器的元素类型。通过灵活使用 pair，可以轻松应对 需要将关联数据捆绑存储、处理 的场景。
pair定义
#include &lt;utility&gt;//可以在定义时直接完成 pair 的初始化。pair&lt;int, double&gt; p0(1, 2.0);//也可以使用先定义，后赋值的方法完成 pair 的初始化。pair&lt;int, double&gt; p1;   // 默认构造：int=0, double=0.0p1.first = 1;p1.second = 2.0;//还可以使用 std::make_pair 函数。该函数接受两个变量，并返回由这两个变量组成的 pair。pair&lt;int, double&gt; p2 = make_pair(1, 2.0);//在 C++11 以及之后的版本中，make_pair 可以配合 auto 使用，以避免显式声明数据类型。auto p3 = make_pair(3.14, &quot;PI&quot;);          // 自动推导类型（C++11）
pair元素访问



成员变量
说明




first
访问第一个元素


second
访问第二个元素



常用操作



操作
说明




比较运算符（==, !=, &lt;等）
按字典序比较：先比较first，相等时再比较second


swap
使用 swap 函数交换 pair 的值。


赋值
将 pair 的值赋给另一个类型一致的 pair。p0 = p1;



string
std::string 是 C++ 标准库提供的字符串类，用于存储和操作字符序列。它在内存中以连续块存储字符，支持高效的随机访问和动态调整大小。
string定义
#include &lt;string&gt;std::string str1;                // 空字符串std::string str2 = &quot;Hello&quot;;      // 用C风格字符串初始化std::string str3(5, &#x27;A&#x27;);        // 创建含5个&#x27;A&#x27;的字符串，输出&quot;AAAAA&quot;std::string str4(str2, 1, 3);    // 从str2下标1开始取3个字符，输出&quot;ell&quot;
string容器内元素的访问
//1. 通过下标访问str[index]//2. 通过迭代器访问string::iterator it;//可以通过*it访问string里的每一位for(string::iterator it= str.begin();it!=str.end();it++)&#123;    printf(&quot;%c&quot;,*it);&#125;//string和vector一样，支持直接对迭代器进行加减某个字，str.begin()+3
string常用函数



函数
功能说明




append(str)
在字符串末尾追加内容


push_back(c)
追加单个字符


pop_back()
删除最后一个字符（C++11）


insert(pos, str)
在指定位置插入字符串


erase(pos, len)
从pos开始删除len个字符


replace(pos, len, str)
替换从pos开始的len个字符为str


find(str, pos)
从pos开始查找子串，返回首次出现的位置，未找到返回string::npos


compare(str)
比较字符串（返回0表示相等，负数表示小于，正数表示大于）


c_str()
返回C风格字符串（const char*）


clear()
清空字符串内容


resize(n, c)
调整字符串长度为n，多出部分用字符c填充


capacity()
返回当前分配的存储容量


reserve(n)
预分配至少能存储n个字符的内存空间



]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>set</tag>
        <tag>map</tag>
        <tag>queue</tag>
        <tag>stack</tag>
        <tag>deque</tag>
        <tag>STL</tag>
        <tag>vector</tag>
        <tag>string</tag>
        <tag>algorithm</tag>
        <tag>priority_queue</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/2025/04/Transformer/</url>
    <content><![CDATA[Attention Is All You Need
个人理解 transformer 编码器是把人能理解的东西转化成计算机能理解的东西。对比与论文写作的这个过程来说，位置编码就是作者写这篇论文的顺序，反复打磨论文这个过程就对应着这个n个编码器，第一遍的初稿相当于第一个编码器，可能效果不尽人意。把人能理解的东西编码成论文。到读者来说就是解码的过程，每一次读论文就是一次解码的过程，你必须多次解码才能对这个论文理解的更加透彻，还要时刻注意mask操作，写作时要时刻注意读者理解到什么地步，读者的阅读是按顺序进行的。q就是你感兴趣的地方，k就是论文中的关键点。
Transformer 是一种基于注意力机制（Attention Mechanism）的深度学习模型，摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN），完全依赖注意力机制处理序列数据。
RNN处理长序列时容易出现梯度消失或梯度爆炸的问题，健达来说就是就像一个记性不好的老人，处理长内容时，前面的内容记不住，后面又容易混淆。
CNN虽然在提取局部特征上表现出色，但对长距离依赖关系的捕捉能力欠佳，就好比只盯着眼前的局部风景，而忽略了远方的整体美景。
Transformer则巧妙地摒弃了RNN的顺序处理方式和CNN的局部处理局限，引入了自注意力机制，这就像是给大模型装上了一个“全局扫描雷达”，能够同时关注输入序列中的各个位置，极大地提升了对长序列的处理能力，完美解决了上述两个难题。
注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向，这是与 CNN 等模型的关键区别。


]]></content>
      <categories>
        <category>论文精读</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识--哈希表</title>
    <url>/2025/02/%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
    <content><![CDATA[哈希表
哈希表概述
​	哈希表：又称散列表，一种以关键码的值「key-value」而直接进行访问的数据结构。任意的键值 key  都唯一对应到内存中的某个位置。只需要输入查找的键值，就可以快速地找到其对应的  value。
​	哈希函数：根据键值计算索引的函数就叫做哈希函数。
​	**冲突：**不同的关键码映射到同一散列位置。key1!=key2，但是H(key1)=H(key2)。
​	**同义词：**具有相同函数值的多个关键字。
​	All in all:  将元素通过一个函数转换为整数，使得该整数可以尽量唯一地代表这个元素 。
​	**需要解决的问题：**1. 哈希函数的构造。   2. 冲突解决的方法。
哈希函数构造方法
​	哈希函数应当易于计算，并且尽量使计算出来的索引均匀分布，以避免冲突。
直接定址法
**概述：**直接取关键字的某个线性函数值为哈希函数。
**哈希函数：**H(key) = key 或 H(key) = a*key + b   ( a和b为常数 )
**特点：**计算简单，不会产生冲突，适合关键字分布连续的情况（若不连续，则存储空间浪费很多，空间效率低）。
除留余数法
**概述：**指把key除以一个数mod得到的余数作为hash值的方法。当mod是一个质数时，H(key)能尽可能均匀覆盖每一个数。所以取mod为不大于表长Tsize但接近或等于表长的质数，即mod&lt;=Tsize且为质数。
**哈希函数：**H(key) = key % mod
**特点：**比较常用，关键在mod的选择，如何使得每个关键字通过该函数转换后等概率映射到散列空间的任一地址。
平方取中法
**概述：**指取key的平方的中间若干位作为hash值的方法，不常用。
**特点：**适合于关键字的每位取值都不够均匀或均小于散列地址所需位数。
处理冲突的方法
开放定址法（开地址法）
**基本思想：**有冲突时就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将数据元素存入。Hi = (H(key) + di) % Tsize (di为增量序列)   计算新的哈希值。
线性探查法
​	di = 0, 1, 2, … , Tsize-1
​	发生冲突时，顺序表查看表中下一个元素，直到有空闲单元。会出现聚集现象，降低查询效率。
平方探查法
​	di = 0² , +1² ,  -1² , +2² , -2² , … , +k² , -k²
​	不会出现聚集现象，不能探测所有单元，但至少能探测一半
伪随机探测法
​	di = 伪随机数序列
链地址法（拉链法）
**基本思想：**和上边方法不同，链地址法不计算新的哈希值，而是把相同散列地址的记录链成一单链表。m个散列地址就设m个单链表，然后用一个数组将m个单链表的表头指针存储起来，形成一个动态的结构。
**优点：**1. 非同义词不会冲突，无&quot;聚集&quot;现象。
​	   2. 链表上结点空间动态申请，更适合于表长不确定的情况（经常插入删除）。

哈希的查找及性能分析
查找过程
​	检测由散列函数形成的地址上是否有记录，若无记录则失败； 若有记录比较关键字值，若相等则查找成功，否则散列函数更新增量值，重复执行。
性能分析
​	在列表查找中，使用最广泛的二分查找算法，复杂度为O(log2n)，但其始终只能用于有序列表。普通无序列表只能采用遍历查找，复杂度为O(n)。而拥有较为理想的哈希函数实现的哈希表，对其任意元素的查找速度始终为常数级，即O(1)。  链地址法优于开放定址法，除留余数法作散列函数优于其它类型函数。
**装填因子：**∂=表中记录数/散列表长度。平均查找长度直接依赖于装填因子大小。也就是说，装填因子大小会直接影响到查找效率。装填因子越大，发生冲突的可能性越大。

​	查找效率三个因素影响：哈希函数、装填因子、处理冲突方法。
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>数组</tag>
        <tag>链表</tag>
        <tag>哈希表</tag>
        <tag>查找</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识--回溯算法</title>
    <url>/2025/03/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[回溯算法
​	回溯算法 是一种通过试错的方式寻找问题解的算法。它尝试逐步构建解决方案，如果在构建过程中的某一步发现当前的构建方案不可行，它会回溯（即取消最近一步的选择），然后尝试其他的可能性。这个过程就像是在迷宫中探索路径，当遇到死路时就退回上一个岔路口，选择另一条路继续前进。
核心思想

试探性选择:  在每一步，都面临多个选择，回溯算法会先选择其中一个进行尝试。
逐步构建:  它一步一步地构建可能的解。
可行性判断 (约束条件): 在每一步构建后，都会检查当前的部分解是否满足问题的约束条件。
回溯 (撤销选择): 如果发现当前部分解不可行，则撤销上一步的选择，回到之前的状态，并尝试其他的选择。
终止条件: 当找到一个完整的可行解，或者当所有可能的选择都尝试完毕且没有找到解时，算法终止。

基本步骤


定义问题的解空间： 确定问题的解可能存在的所有可能组合。例如，对于一个排列问题，解空间就是所有可能的排列。


确定约束条件：  定义问题解必须满足的条件。这些条件用于判断当前构建的部分解是否有效。


选择搜索策略 (通常是深度优先搜索 DFS)：  回溯算法通常使用深度优先搜索的方式来遍历解空间。


设计递归函数 (或迭代方式模拟递归)：
递归函数是实现回溯算法的关键。该函数通常包含以下几个部分：

基本情况 (终止条件)：  判断是否找到了一个完整的解，或者是否已经遍历完所有可能性但没有找到解。
选择步骤：  在当前状态下，有哪些选择可以进行？
扩展状态：  对于每个选择，更新状态，并递归调用自身进入下一层搜索。
回溯步骤：  在递归调用返回后，需要撤销当前的选择，恢复到之前的状态，以便尝试其他的选择。



剪枝优化 (可选但重要)：  在搜索过程中，如果发现某个部分解已经不可能导致最终解，可以提前结束对该分支的搜索，以提高效率。这称为剪枝。


应用示例
回溯算法可以应用于解决许多经典问题，包括但不限于：

组合问题：

组合总和 (Combination Sum):  在一个数字集合中找到所有和为目标值的组合。
子集 (Subsets):  找出给定集合的所有子集。
电话号码的字母组合 (Letter Combinations of a Phone Number):  给定数字字符串，返回所有可能的字母组合。


排列问题：

全排列 (Permutations):  生成给定集合的所有排列。
下一个排列 (Next Permutation):  找到给定排列的下一个字典序排列。


图论问题：

N 皇后问题 (N-Queens Problem):  在 NxN 的棋盘上放置 N 个皇后，使其互不攻击。
数独 (Sudoku Solver):  解决数独谜题。
迷宫寻路 (Maze Solving):  找到迷宫的路径。
图的着色问题 (Graph Coloring):  给图的顶点着色，使得相邻顶点颜色不同。
旅行商问题 (Traveling Salesman Problem - TSP) (近似解或小规模问题)：  虽然TSP通常使用更优化的算法，但在小规模情况下，回溯可以找到解。


其他问题：

0-1 背包问题 (0-1 Knapsack Problem)：  虽然动态规划更常用，但回溯也可以解决。
正则表达式匹配 (Regular Expression Matching) (某些情况)：  复杂的正则表达式匹配问题可以使用回溯解决。
解方程 (Solving Equations) (某些类型)：  例如，约束满足问题 (Constraint Satisfaction Problems - CSPs)。



回溯算法模板
回溯法其实就是暴力查找，回溯法解决的问题都可以抽象为树形结构（N叉树）。
void backtracking(参数) &#123;    if (终止条件) &#123;        存放结果;        return;    &#125;    for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) &#123;        处理节点;        backtracking(路径，选择列表); // 递归        回溯，撤销处理结果    &#125;&#125;
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络</title>
    <url>/2025/09/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[图神经网络（GNN）全面指：从基础到高级应用
引言
在数据爆炸的时代，传统深度学习模型如CNN和RNN在处理结构化数据（如图像和序列）上取得了巨大成功，但现实世界中的许多数据都具有图结构（Graph Structure），例如社交网络、分子结构、知识图谱、交通网络等。这些数据是非欧几里德的（Non-Euclidean），节点之间存在复杂的拓扑关系，无法直接用网格或序列表示。这就是图神经网络（Graph Neural Networks, GNN）登场的原因。
GNN 通过模拟节点间的消息传递机制，捕捉图的局部和全局结构，实现对图数据的表示学习。它已在推荐系统、药物发现、蛋白质折叠预测等领域大放异彩。本文将从基础概念入手，逐步深入GNN的核心原理、经典模型、实现技巧和实际应用，帮助你全面掌握这一技术。无论你是初学者还是有经验的从业者，这篇指南都能提供实用价值。
图数据基础
图的定义与表示
图 $ G = (V, E) $ 由节点集 $ V $（Vertices）和边集 $ E $（Edges）组成：

节点（Nodes）：实体，如用户、原子。
边（Edges）：关系，如友谊、化学键。
类型：

无向图：边无方向（e.g., 社交网络）。
有向图：边有方向（e.g., 网页链接）。
加权图：边有权重 $ w_{uv} $（e.g., 相似度分数）。
异构图：节点/边有多种类型（e.g., 知识图谱）。



图的常见表示方法：

邻接矩阵（Adjacency Matrix） $ A \in \mathbb{R}^{|V| \times |V|} $：$ A_{uv} = 1 $ 如果存在边 $ (u, v) $，否则为0。适合小图，但空间复杂度 $ O(|V|^2) $。
边列表（Edge List）：稀疏表示，如 $[ (u_1, v_1), (u_2, v_2), \dots ] $，适合大图。
度矩阵（Degree Matrix） $ D $，对角线 $ D_{ii} = \sum_j A_{ij} $（节点i的度）。
拉普拉斯矩阵（Laplacian Matrix） $ L = D - A $，用于谱分析：它是半正定的，特征分解 $ L = U \Lambda U^T $，其中 $ U $ 是傅里叶基。

图任务类型
GNN 针对不同粒度的数据设计：

节点级任务：节点分类（e.g., 预测论文类别）、节点回归（e.g., 预测节点影响力）。
边级任务：链接预测（e.g., 推荐朋友）。
图级任务：图分类（e.g., 判断分子是否毒性）、图回归（e.g., 预测分子能量）。

数据集示例：

节点分类：Cora（论文引用网络，2708节点，5429边）。
图分类：MUTAG（188个分子图）。

GNN 核心原理
GNN 的本质是**消息传递神经网络（Message Passing Neural Network, MPNN）**框架，由Scarselli等人在2009年提出。它通过多层迭代，让每个节点从邻居聚合信息，逐步捕捉多跳（multi-hop）依赖。
数学基础：图信号处理
图傅里叶变换：
给定图拉普拉斯矩阵 $L = D - A$ 的特征分解 $L = U\Lambda U^T$，其中 $U$ 是特征向量矩阵，$\Lambda$ 是特征值对角矩阵。
图信号 $x \in \mathbb{R}^{|V|}$ 的傅里叶变换定义为：
$$\hat{x} = U^T x$$
逆变换为：
$$x = U\hat{x}$$
图卷积的谱定义：
图上的卷积操作定义为：
$$g_\theta * x = U((U^T g_\theta) \odot (U^T x)) = Ug_\theta(\Lambda)U^T x$$
其中 $g_\theta(\Lambda)$ 是谱域的滤波器，$\odot$ 是元素级乘积。
从谱域到空间域：
计算特征分解的复杂度为 $O(|V|^3)$，不可扩展。通过多项式近似（如Chebyshev多项式）可以避免显式特征分解：
$$g_\theta(\Lambda) \approx \sum_{k=0}^{K} \theta_k T_k(\tilde{\Lambda})$$
其中 $T_k$ 是Chebyshev多项式，$\tilde{\Lambda} = \frac{2}{\lambda_{max}}\Lambda - I$。
消息传递机制
假设初始节点特征 $ h_v^{(0)} = x_v $（节点v的输入特征）。 在第 $ k $ 层：

消息生成（Message Generation）：对于每条边 $ (u, v) $，生成消息 $ m_{uv}^{(k)} = f(h_u^{(k-1)}, h_v^{(k-1)}, e_{uv}) $，其中 $ f $ 是可学习函数（如MLP），$ e_{uv} $ 是边特征。
聚合（Aggregation）：节点v聚合邻居消息 $ \tilde{h}v^{(k)} = \text{AGGREGATE}({ m{uv}^{(k)} : u \in \mathcal{N}(v) }) $。

常见AGG：Sum（求和）、Mean（平均）、Max（最大）、Attention（注意力）。


更新（Update）：$ h_v^{(k)} = \text{UPDATE}(\tilde{h}_v^{(k)}, h_v^{(k-1)}) $，UPDATE 如GRU、MLP + ReLU。
读出（Readout）（仅图级任务）：全局池化 $ \hat{y} = \text{READOUT}({ h_v^{(K)} : v \in V }) $，如mean pooling或sum。

数学上，整个过程可并行化，使用稀疏矩阵运算。关键假设：同质性（Homophily），相连节点相似。
谱方法 vs 空间方法

谱方法（Spectral GNN）：基于图信号处理（Graph Signal Processing）。图卷积定义为 $ g_\theta * x = U g_\theta(\Lambda) U^T x $，其中 $ g_\theta(\lambda) $ 是滤波器（e.g., 多项式）。优点：理论基础强；缺点：计算 $ U $ 成本高（O(|V|^3)）。

示例：ChebNet（2017），用Chebyshev多项式近似滤波器，K阶多项式只需O(K)参数。


空间方法（Spatial GNN）：直接在节点邻域操作，更高效、可扩展。主流模型如GCN、GAT均属此类。

经典GNN模型详解
Graph Convolutional Network (GCN)
Kipf &amp; Welling (2017) 的开创性工作，将CNN推广到图。
详细数学推导
从谱卷积到GCN：


起点：谱卷积
$$g_\theta * x = Ug_\theta(\Lambda)U^T x$$


一阶Chebyshev近似（$K=1$）
$$g_\theta(\Lambda) \approx \theta_0 + \theta_1 \Lambda$$
代入得：
$$g_\theta * x \approx \theta_0 x + \theta_1 L x$$


参数简化
假设 $\theta = \theta_0 = -\theta_1$，得：
$$g_\theta * x \approx \theta(I - L)x = \theta(I - D + A)x$$
由于 $L = D - A$，可以重写为：
$$g_\theta * x \approx \theta(2I - L)x$$


重归一化技巧
使用 $\tilde{A} = A + I$（添加自环）和对应的度矩阵 $\tilde{D}$：
$$H^{(l+1)} = \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$$


直观理解：

$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$ 是对称归一化的邻接矩阵
每个节点的特征是其邻居特征的加权平均
权重由节点度数决定，防止度数大的节点主导

GCN的实际意义与应用
为什么GCN如此重要？


计算效率的突破

避免了特征分解的O(N³)复杂度
稀疏矩阵运算，复杂度降至O(E)，E是边数
可以处理百万节点规模的图



表达能力与简洁性的平衡

一阶近似虽简单但足够有效
每层只有一个权重矩阵W，参数量少
实践证明：2-3层GCN往往就能达到好效果



理论与实践的统一

有坚实的谱图理论基础
实现简单，易于集成到深度学习框架
可以自然地与其他神经网络模块结合



GCN的典型应用场景



应用领域
具体任务
为什么适用GCN




社交网络
用户分类、社区发现
利用社交关系传播信息


推荐系统
物品推荐、用户画像
建模用户-物品交互图


知识图谱
实体分类、关系预测
利用知识结构


分子化学
分子属性预测
原子作为节点，化学键作为边


交通网络
流量预测
道路连接的空间依赖



实践中的关键技巧


层数选择

通常2-3层最优，过深会过平滑
使用残差连接可以训练更深的网络
JKNet：跳跃连接聚合所有层的输出



归一化变体

对称归一化：$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$（最常用）
随机游走归一化：$\tilde{D}^{-1}\tilde{A}$
行归一化：保持特征尺度



加速技巧

预计算归一化邻接矩阵
使用稀疏矩阵运算库
批处理多个图时使用块对角矩阵



缺点与解决方案：

过平滑问题：深层GCN会导致所有节点表示趋同

解决：残差连接、跳跃连接、DropEdge


固定邻域聚合：不能自适应调整邻居重要性

解决：GAT引入注意力机制


扩展性受限：需要完整邻接矩阵

解决：GraphSAGE的采样策略



Graph Attention Network (GAT)
Veličković等 (2018) 引入注意力机制，提升表达力。
注意力机制详解
核心思想：不同邻居对中心节点的重要性不同，应该学习自适应的聚合权重。
注意力系数计算：


特征变换：
$$ 
\mathbf{z}_v = \mathbf{W}\mathbf{h}_v
$$ 


注意力得分：
$$ 
e_{vu} = \text{LeakyReLU}(\mathbf{a}^T[\mathbf{z}_v \parallel \mathbf{z}_u])
$$    $$
其中 $\parallel$ 表示拼接操作，$\mathbf{a} \in \mathbb{R}^{2F’}$ 是可学习的注意力向量。


归一化（softmax）：
$$ 
\alpha_{vu} = \frac{\exp(e_{vu})}{\sum_{k \in \mathcal{N}(v) \cup \{v\}} \exp(e_{vk})}
$$    $$


特征聚合：
$$ 
\mathbf{h}_v' = \sigma\left(\sum_{u \in \mathcal{N}(v) \cup \{v\}} \alpha_{vu} \mathbf{z}_u\right)
$$    $$


多头注意力机制
GAT使用多头注意力来稳定学习过程：
$$ 
\mathbf{h}_v' = \mathop{\Big\Vert}_{k=1}^K \sigma\left(\sum_{u \in \mathcal{N}(v) \cup \{v\}} \alpha_{vu}^k \mathbf{W}^k\mathbf{h}_u\right)
$$ 
其中 $\Big\Vert$ 表示拼接，$K$ 是注意力头的数量。
最后一层使用平均而非拼接：
$$ 
\mathbf{h}_v' = \sigma\left(\frac{1}{K}\sum_{k=1}^K \sum_{u \in \mathcal{N}(v) \cup \{v\}} \alpha_{vu}^k \mathbf{W}^k\mathbf{h}_u\right)
$$ 
GAT vs GCN



特性
GCN
GAT




聚合权重
固定（基于度）
可学习


计算复杂度
$O(EF)$
$O(EF^2)$


表达能力
受限于拓扑
更灵活


可解释性
低
高（注意力可视化）



GAT的实践指南
何时选择GAT而非GCN？


异构性较强的图

节点的邻居重要性差异大
存在噪声边或无关连接
需要学习复杂的关系模式



需要可解释性

注意力权重可以可视化
帮助理解模型决策依据
发现重要的连接模式



动态图或时序图

边的重要性随时间变化
需要自适应调整聚合策略



GAT的调参技巧



超参数
典型值
调参建议




注意力头数
4-8
太少欠拟合，太多过拟合


隐藏维度
64-256
与图规模成正比


Dropout率
0.5-0.6
GAT容易过拟合，需要较大dropout


LeakyReLU负斜率
0.2
通常不需要调整


层数
2-3
深层GAT同样有过平滑问题



注意力机制的计算细节
步骤1：线性变换h&#x27;_i = W·h_i  (将d维特征映射到F&#x27;维)步骤2：计算注意力系数e_ij = LeakyReLU(a^T[h&#x27;_i || h&#x27;_j])其中 || 表示拼接，a是2F&#x27;维的可学习向量步骤3：归一化（softmax）α_ij = exp(e_ij) / Σ_k∈N(i) exp(e_ik)步骤4：加权聚合h&#x27;&#x27;_i = σ(Σ_j∈N(i) α_ij·h&#x27;_j)
常见问题与解决


注意力权重趋于均匀

原因：特征区分度不够
解决：增加特征维度，使用更深的变换



训练不稳定

原因：注意力机制增加了优化难度
解决：使用warmup，降低初始学习率



内存消耗大

原因：需要存储所有边的注意力权重
解决：使用稀疏注意力或采样



GraphSAGE
Hamilton等 (2017) 针对大图设计，支持归纳学习。

采样聚合：随机采样固定大小的k-hop邻居，避免全图计算。
聚合函数：

Mean：$ \text{AGG} = \text{mean}({ \mathbf{h}_u : u \in \text{sample}(\mathcal{N}(v)) }) $。
Pool：每个邻居用MLP池化后sum。
LSTM：顺序处理邻居消息。


更新：$ \mathbf{h}_v^{(k)} = \sigma( \mathbf{W}^{(k)} \cdot \text{CONCAT}( \mathbf{h}_v^{(k-1)}, \text{AGG} ) ) $。
优点：可扩展到百万节点图；缺点：采样引入方差。
应用：Pinterest的引脚推荐。

Graph Isomorphism Network (GIN)
Xu等 (2019) 提升图级表示的区分力。
公式：
$$ 
\mathbf{h}_v^{(k)} = \text{MLP}^{(k)} \left( (1 + \epsilon^{(k)}) \mathbf{h}_v^{(k-1)} + \sum_{u \in \mathcal{N}(v)} \mathbf{h}_u^{(k-1)} \right)
$$ 

$\epsilon^{(k)}$ 可学习，控制自环权重。

读出：
$$ 
\mathbf{h}_G = \text{MLP}\left( \sum_v \mathbf{h}_v^{(K)} \right)
$$ 

理论：等价于Weisfeiler-Lehman (WL) 图同构测试，能区分更多非同构图。
优点：SOTA于图分类基准（如OGB）。

实现与实践
框架选择

PyTorch Geometric (PyG)：基于PyTorch，易用。安装：pip install torch-geometric
Deep Graph Library (DGL)：支持PyTorch/TensorFlow/MXNet，适合异构图
Spektral：Keras接口

代码实现
GCN完整实现（PyTorch Geometric）
import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch_geometric.nn import GCNConvfrom torch_geometric.datasets import Coraclass GCN(nn.Module):    &quot;&quot;&quot;图卷积网络实现&quot;&quot;&quot;    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):        super(GCN, self).__init__()        self.conv1 = GCNConv(input_dim, hidden_dim)        self.conv2 = GCNConv(hidden_dim, output_dim)        self.dropout = dropout        def forward(self, x, edge_index):        # 第一层GCN + ReLU + Dropout        x = self.conv1(x, edge_index)        x = F.relu(x)        x = F.dropout(x, p=self.dropout, training=self.training)                # 第二层GCN        x = self.conv2(x, edge_index)        return F.log_softmax(x, dim=1)# 训练函数def train_gcn(model, data, optimizer, epochs=200):    model.train()    for epoch in range(epochs):        optimizer.zero_grad()        out = model(data.x, data.edge_index)        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])        loss.backward()        optimizer.step()                if epoch % 50 == 0:            model.eval()            pred = model(data.x, data.edge_index).argmax(dim=1)            acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()            print(f&#x27;Epoch &#123;epoch&#125;, Loss: &#123;loss:.4f&#125;, Test Acc: &#123;acc:.4f&#125;&#x27;)            model.train()# 使用示例dataset = Cora(root=&#x27;/tmp/Cora&#x27;)data = dataset[0]model = GCN(dataset.num_features, 16, dataset.num_classes)optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)train_gcn(model, data, optimizer)
GAT实现
from torch_geometric.nn import GATConvclass GAT(nn.Module):    &quot;&quot;&quot;图注意力网络实现&quot;&quot;&quot;    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout=0.6):        super(GAT, self).__init__()        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, dropout=dropout)        self.dropout = dropout        def forward(self, x, edge_index):        x = F.dropout(x, p=self.dropout, training=self.training)        x = self.conv1(x, edge_index)        x = F.elu(x)        x = F.dropout(x, p=self.dropout, training=self.training)        x = self.conv2(x, edge_index)        return F.log_softmax(x, dim=1)
GraphSAGE实现
from torch_geometric.nn import SAGEConvclass GraphSAGE(nn.Module):    &quot;&quot;&quot;GraphSAGE实现&quot;&quot;&quot;    def __init__(self, input_dim, hidden_dim, output_dim, aggregator=&#x27;mean&#x27;):        super(GraphSAGE, self).__init__()        self.conv1 = SAGEConv(input_dim, hidden_dim, aggregator=aggregator)        self.conv2 = SAGEConv(hidden_dim, output_dim, aggregator=aggregator)        def forward(self, x, edge_index):        x = self.conv1(x, edge_index)        x = F.relu(x)        x = F.dropout(x, training=self.training)        x = self.conv2(x, edge_index)        return F.log_softmax(x, dim=1)
应用案例
1. 社交网络节点分类
# 社交网络影响力预测def social_network_influence_prediction(graph_data):    &quot;&quot;&quot;    预测社交网络中用户的影响力        节点特征：用户画像（年龄、兴趣、活跃度等）    边：社交关系（关注、互动）    任务：预测用户影响力评分    &quot;&quot;&quot;    model = GAT(        input_dim=graph_data.num_features,        hidden_dim=64,        output_dim=1,  # 回归任务        heads=4    )    # 训练过程略...    return model
2. 分子属性预测
# 药物毒性预测def molecular_toxicity_prediction(molecular_graphs):    &quot;&quot;&quot;    预测分子的毒性        节点：原子（特征：原子类型、度、电荷等）    边：化学键（特征：键类型、键长等）    任务：二分类（有毒/无毒）    &quot;&quot;&quot;    model = GIN(        input_dim=atomic_features_dim,        hidden_dim=128,        output_dim=2  # 二分类    )    # 图级任务，需要全局池化    return model
3. 推荐系统
# 基于GNN的推荐系统class GNNRecommender(nn.Module):    &quot;&quot;&quot;用户-商品二部图推荐&quot;&quot;&quot;    def __init__(self, num_users, num_items, embedding_dim=64):        super().__init__()        self.user_embedding = nn.Embedding(num_users, embedding_dim)        self.item_embedding = nn.Embedding(num_items, embedding_dim)        self.conv1 = GCNConv(embedding_dim, embedding_dim)        self.conv2 = GCNConv(embedding_dim, embedding_dim)        def forward(self, user_item_edges):        # 获取嵌入        user_emb = self.user_embedding.weight        item_emb = self.item_embedding.weight        x = torch.cat([user_emb, item_emb], dim=0)                # 图卷积        x = self.conv1(x, user_item_edges)        x = F.relu(x)        x = self.conv2(x, user_item_edges)                # 分离用户和商品嵌入        user_emb_final = x[:num_users]        item_emb_final = x[num_users:]                return user_emb_final, item_emb_final
优化技巧
1. 过平滑问题及解决方案
class ResGCN(nn.Module):    &quot;&quot;&quot;带残差连接的GCN，缓解过平滑&quot;&quot;&quot;    def __init__(self, input_dim, hidden_dim, num_layers=4):        super().__init__()        self.convs = nn.ModuleList()        self.bns = nn.ModuleList()                self.convs.append(GCNConv(input_dim, hidden_dim))        self.bns.append(nn.BatchNorm1d(hidden_dim))                for _ in range(num_layers - 1):            self.convs.append(GCNConv(hidden_dim, hidden_dim))            self.bns.append(nn.BatchNorm1d(hidden_dim))        def forward(self, x, edge_index):        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):            identity = x if i &gt; 0 else None            x = conv(x, edge_index)            x = bn(x)            x = F.relu(x)            if identity is not None:                x = x + identity  # 残差连接        return x
2. 可扩展性优化
class MiniBatchGraphSAGE:    &quot;&quot;&quot;小批量训练GraphSAGE&quot;&quot;&quot;    def __init__(self, model, num_samples=[10, 10]):        self.model = model        self.num_samples = num_samples  # 每层采样邻居数        def sample_neighbors(self, node_ids, edge_index, num_samples):        &quot;&quot;&quot;邻居采样&quot;&quot;&quot;        # 实现采样逻辑        sampled_edges = []        for node_id in node_ids:            neighbors = edge_index[1][edge_index[0] == node_id]            if len(neighbors) &gt; num_samples:                neighbors = neighbors[torch.randperm(len(neighbors))[:num_samples]]            sampled_edges.append(neighbors)        return sampled_edges
3. 注意力机制优化
class ImprovedGAT(nn.Module):    &quot;&quot;&quot;改进的GAT：添加边特征和多尺度注意力&quot;&quot;&quot;    def __init__(self, node_dim, edge_dim, hidden_dim):        super().__init__()        self.node_transform = nn.Linear(node_dim, hidden_dim)        self.edge_transform = nn.Linear(edge_dim, hidden_dim)        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)        def forward(self, x, edge_index, edge_attr):        # 节点特征变换        h = self.node_transform(x)                # 边特征融入注意力计算        edge_h = self.edge_transform(edge_attr)                # 多头注意力        attn_output, _ = self.attention(h, h, h)        return attn_output
未来方向
1. 动态图神经网络

处理时序变化的图结构
应用：社交网络演化、交通流预测

2. 图生成模型

GraphVAE、GraphGAN
应用：分子生成、网络设计

3. 可解释性

注意力可视化
子图重要性分析

4. 大规模图处理

分布式GNN训练
图采样和压缩技术

总结
图神经网络作为处理非欧几里德数据的强大工具，已经在多个领域展现出巨大潜力。从基础的GCN到复杂的GAT、GraphSAGE，每种模型都有其独特优势和适用场景。
核心要点：

消息传递是GNN的核心机制
聚合函数的选择影响表达能力
过平滑是深层GNN的主要挑战
可扩展性是实际应用的关键

随着研究的深入，GNN正在向更高效、更可解释、更通用的方向发展。掌握GNN不仅需要理解理论，更需要大量实践。希望本文能为你的GNN学习之旅提供帮助。
参考文献

Kipf, T. N., &amp; Welling, M. (2017). Semi-supervised classification with graph convolutional networks. ICLR 2017.
Veličković, P., et al. (2018). Graph Attention Networks. ICLR 2018.
Hamilton, W. L., et al. (2017). Inductive representation learning on large graphs. NeurIPS 2017.
Xu, K., et al. (2019). How powerful are graph neural networks? ICLR 2019.
Wu, Z., et al. (2020). A comprehensive survey on graph neural networks. IEEE TNNLS.

]]></content>
      <categories>
        <category>论文精读</category>
        <category>图神经网络</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>GNN</tag>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识--图论</title>
    <url>/2025/03/%E5%9B%BE%E8%AE%BA/</url>
    <content><![CDATA[基础知识–图论
图论是数学的一个分支，以图为研究对象。  图论中的“图”是一种抽象的数学结构，由顶点（点）和连接顶点的边（线）构成，用于描述事物之间特定的关系。

核心思想:  用点代表事物，用连接两点的线表示事物之间的关系。

图的基本概念


图 (Graph)
一个图 G 由两个基本集合构成：

顶点集 (Vertex Set) V(G):  图中所有顶点的集合。 顶点通常用点、小圆圈或标签表示。
边集 (Edge Set) E(G):  图中所有边的集合。 边连接一对顶点，表示顶点之间的关系。 边通常用线段、曲线或箭头表示。

我们可以用数学符号表示一个图：  G = (V, E)


顶点 (Vertex / Node / Point)

图的基本组成部分，代表研究对象或实体。
示例:

在社交网络中，顶点可以代表 人。
在交通网络中，顶点可以代表 城市。
在电路图中，顶点可以代表 电子元件。





边 (Edge / Arc / Line)

连接两个顶点的线，表示顶点之间存在的关系。
示例:

在社交网络中，边可以表示两个人之间的 朋友关系。
在交通网络中，边可以表示两个城市之间的 道路。
在电路图中，边可以代表 导线。





关联 (Incidence)

如果一条边 e 连接了顶点 u 和 v，则称边 e 与顶点 u 和 v 关联。
简单来说，就是边“连接到”哪些顶点。



邻接 (Adjacency)

顶点邻接: 如果两个顶点 u 和 v 被一条边直接连接，则称顶点 u 和 v 是邻接的。
边邻接: 如果两条边 共享 一个公共顶点，则称这两条边是邻接的。



度 (Degree)


一个顶点的度是指与该顶点关联的边的数量。  度反映了顶点与其他顶点的连接数量。


有向图的度:  对于有向图，度分为两种：

入度 (In-degree):  指向该顶点的边的数量 (有多少边进入该顶点)。
出度 (Out-degree):  从该顶点出发的边的数量 (有多少边从该顶点出发)。





路径 (Path)

图中从一个顶点到另一个顶点，依次经过的顶点和边的序列。
路径中通常不重复经过顶点，除非是环路。



环路/回路 (Cycle)

一种特殊的路径，起点和终点是同一个顶点。
环路表示从一个点出发，可以回到原点的路径。



简单路径 (Simple Path)

路径中所有顶点都不同的路径。
简单路径避免了在路径中重复访问同一个顶点。



简单环路/简单回路 (Simple Cycle)

环路中，除了起点和终点相同外，所有其他顶点都不同的环路。
简单环路是最基本的环路形式，避免了在环路中重复访问除起点/终点之外的顶点。



图的类型
有向图 vs 无向图


无向图: 边 没有方向，表示连接的两个顶点是互通的。 通常用  (v, w)  表示顶点 v 和 w 之间的无向边。


有向图: 边 有方向，表示连接的两个顶点之间是单向关系。  通常用  &lt;v, w&gt;  表示从顶点 v 指向顶点 w 的有向边。


有权图 vs 无权图


有权图: 图的每条边都被赋予一个 权重 (weight)，通常是一个数字。 权重可以代表 距离、成本、时间 等实际含义。


无权图: 图的边 没有权重。 可以理解为所有边的权重都相同（例如权重为 1）。


连通图 vs 非连通图


连通图 (针对无向图):  图中 任意两个顶点之间都存在路径。  这意味着从图中任何一个顶点出发，都可以到达其他任何顶点。


非连通图 (针对无向图):  图中 至少存在一对顶点之间没有路径。  非连通图由多个互相独立的“部分”组成。


连通分量 (Connected Component):  非连通图可以被划分成多个 连通的子图，每个这样的连通子图被称为一个 连通分量。 连通图自身只有一个连通分量，就是它本身。


强连通 vs 单连通

强连通 (Strongly Connected):  有向图 中，任意两个顶点之间都存在互相可达的路径。  即从顶点 u 可以到达 v，同时从顶点 v 也可以到达 u。
单连通 (Unilaterally Connected): 有向图 中，任意两个顶点之间都存在至少一个方向可达的路径 （无需互相可达）。  即对于任意两个顶点 u 和 v，要么从 u 可以到达 v，要么从 v 可以到达 u，或者两者都成立。

图的表示方法
邻接矩阵表示法
邻接矩阵是一个二维数组，用于表示顶点之间的连接关系：

对于无权图，矩阵元素 A[i][j] = 1 表示顶点 i 和 j 之间有边相连，A[i][j]= 0 表示没有边相连
对于有权图，矩阵元素 A[i][j]表示顶点 i 和 j 之间边的权值，通常用一个特殊值（如∞或0）表示不存在的边

优点：

实现简单，便于理解
查询两点间是否有边的时间复杂度为 O(1)
适合表示稠密图

缺点：

空间复杂度为 O(V²)，其中 V 是顶点数，对于稀疏图会浪费大量空间
添加/删除顶点的操作较为复杂

邻接表表示法
邻接表使用一个数组，数组的每个元素是一个链表，用于存储与该顶点相邻的所有顶点：

数组索引 i 对应顶点 i
链表中存储的是与顶点 i 相邻的所有顶点

优点：

节省空间，特别适合稀疏图
容易找到给定顶点的所有邻接点
添加顶点操作简单

缺点：

查询两点间是否有边的时间复杂度为 O(V)，不如邻接矩阵高效
对有向图求逆邻接表较困难

十字链表表示法
十字链表是邻接表的改进版本，主要用于有向图：

结合了邻接表和逆邻接表的特点
每个顶点有一个出边表和一个入边表
每条边用一个结点表示，同时出现在起点的出边表和终点的入边表中

优点：

容易找到顶点的所有出边和入边
适合需要频繁查询入度和出度的应用场景

邻接多重表
邻接多重表主要用于无向图：

每条边只存储一次
每个顶点有一个边表
每条边连接到与其关联的两个顶点的边表中

优点：

便于删除和查找特定边
节省空间

边集数组表示法
边集数组直接存储图中所有的边：

使用一个一维数组存储所有顶点信息
使用另一个数组存储所有边的信息（包括起点、终点和权值）

优点：

实现简单
适合只关注边的操作，如最小生成树的Kruskal算法
适合稀疏图

缺点：

查找操作效率较低
不易找到与特定顶点相连的所有边

选择哪种表示方法取决于图的特性（如稠密度）和需要执行的操作类型（如是否需要频繁查询、添加或删除顶点和边）。
图的遍历方法
DFS(深度优先搜索)
深度优先搜索沿着图的深度尽可能远地探索，直到不能再深入为止，然后回溯到前一个节点，继续探索其他路径。
基本算法步骤

选择一个起始顶点，将其标记为已访问
对该顶点的每个未访问的邻接点，递归地应用DFS
如果当前顶点没有未访问的邻接点，则回溯

时间复杂度与空间复杂度

时间复杂度：O(V + E)
空间复杂度：O(V)

应用场景

拓扑排序
寻找连通分量
判断图中是否有环
寻找路径（如迷宫问题）

BFS(广度优先搜索)
广度优先搜索是一种按&quot;层次&quot;访问节点的方法，先访问起始顶点的所有邻接点，然后再访问这些邻接点的邻接点，以此类推。
基本算法步骤

选择一个起始顶点，将其标记为已访问，并放入队列中
当队列不为空时，执行以下操作：

取出队列中的第一个顶点u
访问顶点u
将u的所有未访问过的邻接点标记为已访问并加入队列



时间复杂度与空间复杂度

时间复杂度：O(V + E)，其中V是顶点数，E是边数
空间复杂度：O(V)，用于存储访问状态和队列

应用场景

寻找最短路径（无权图）
网络爬虫
社交网络中的关系距离（如&quot;六度分隔&quot;理论）
层次遍历

图的应用
最小生成树算法
无向图 G 的生成树是具有 G 的全部顶点，但边数最少的连通子图（连通子图并非连通分量）。一个图的生成树可能有多个。
Kruskal算法


思想：贪心策略，每次选择图中权值最小的边，加入生成树中（前提是不形成环）。
步骤：

对所有边按照权值进行排序。
依次从小到大考虑每条边，如果当前边加入后不构成环，则加入生成树。
重复直到生成树中包含 n−1 条边。


特点：适合边比较少的稀疏图，利用并查集来判断是否形成环。

Prim算法

思想：贪心策略，从一个起始顶点开始，每次选择与当前生成树相连的、权值最小的边，将新的顶点加入生成树。
步骤：

从任一顶点开始，将其加入生成树。
找出所有与当前生成树相连的边，选择其中权值最小的边，将边的另一端顶点加入生成树。
重复直到所有顶点都被加入生成树。


特点：适合边较多的稠密图，通过优先队列（堆）优化可以达到较高效率。

最短路径算法
Dijkstra算法
从起点开始，每次选择当前已知最短距离的顶点，然后更新与它相邻的顶点的距离。
基本流程

初始化：源点距离设为0，其他顶点距离设为无穷大
每次选择未访问的距离最小的顶点，标记为已访问
更新该顶点的邻居顶点的距离
重复步骤2-3直到所有顶点被访问

算法复杂度

时间复杂度：O(V²)，其中V是顶点数。如果使用优先队列优化，可以降低到O((V+E)logV)，其中E是边数。
空间复杂度：O(V)

算法的局限性

迪杰斯特拉算法不能处理负权边
在稠密图中，性能可能不如其他算法（如贝尔曼-福特算法）

应用场景

导航系统
网络路由协议
社交网络分析
电信网络规划

Floyd-Warshall算法
对于两个顶点i和j之间的最短路径，考虑是否存在一个中间顶点k，使得从i到k再到j的路径比直接从i到j的路径更短。算法通过三重循环遍历所有可能的中间顶点，不断更新距离矩阵。
基本流程

初始化一个距离矩阵，矩阵中元素dist[i][j]表示从顶点i到j的直接距离。如果i和j之间没有直接连接，则设为无穷大
对于每一个顶点k，检查所有顶点对(i,j)
对于每一个顶点对(i,j)，如果dist[i][k] + dist[k][j] &lt; dist[i][j]，则更新dist[i][j]的值
重复步骤2和3，直到遍历完所有顶点

算法复杂度

时间复杂度：O(V³)，其中V是顶点数
空间复杂度：O(V²)

与迪杰斯特拉算法的比较

范围不同：迪杰斯特拉是单源最短路径算法，而弗洛伊德是全源最短路径算法
适用场景：

对于稀疏图，运行V次迪杰斯特拉算法更高效
对于稠密图，弗洛伊德算法更优


负权边处理：弗洛伊德可以处理负权边，但两者都不能处理负权环

算法优势

实现简单，代码简洁
可以处理有向图和负权边（但不能有负权环）
一次运行可以得到所有顶点对之间的最短路径

应用场景

网络路由算法
交通规划
寻找图中的传递闭包
计算最大流问题

A*算法
A*算法是一种启发式搜索算法，常用于路径规划问题。

结合了Dijkstra算法和启发式搜索的优点
使用估价函数指导搜索方向
在有好的启发函数时效率很高

基本流程

初始化：起点放入开放列表
每次从开放列表中选择f值最小的节点n
如果n是目标节点，算法结束
否则，将n移到关闭列表，并检查所有邻居节点
对于每个邻居，计算经过n的路径长度g和估计总长度f=g+h
更新邻居节点信息并加入开放列表
重复步骤2-6直到找到目标或开放列表为空

拓扑排序
拓扑排序是一种对有向无环图 (DAG, Directed Acyclic Graph) 中顶点进行线性排序的算法，使得对于图中的每一条有向边 (u, v)，顶点 u 在排序中都出现在顶点 v 之前。拓扑排序常用于表示具有依赖关系的任务调度问题。
算法原理
每次选择入度为0的顶点（即没有前驱顶点的顶点），将其输出，然后删除该顶点及其所有出边，重复此过程直到图为空或无法找到入度为0的顶点。
时间复杂度与空间复杂度

时间复杂度: O(V + E)，其中V是顶点数，E是边数
空间复杂度: O(V)，需要存储顶点的访问状态和结果

特性和局限性

唯一性: 拓扑排序的结果通常不唯一，可能有多种合法的排序方式
环检测: 如果图中存在环，则无法进行拓扑排序
适用范围: 只适用于有向无环图（DAG）
初始点要求: 至少有一个顶点的入度为0，否则无法开始排序

]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>图论</tag>
      </tags>
  </entry>
  <entry>
    <title>基于深度学习的图像分类</title>
    <url>/2025/04/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[使用ResNet18预训练模型
由于笔记本性能太差，所以在服务器上运行的，显卡配置为4090。经大量实验判断，初始学习率为0.01最后效果较差，所以初始学习率应设为0.001。全部代码代码已上传到：https://github.com/wp-a/-CIFAR10-.git
库函数导入
import matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transformsfrom sklearn.metrics import confusion_matrix, classification_reportfrom itertools import chainimport multiprocessingdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
数据集加载及增强操作
transform_train = transforms.Compose([    transforms.Resize(224),	#图像从32x32放大到224x224是通过在原始像素之间插入新的像素点来实现的。插入的像素值是根据周围原始像素的值，通过不同的插值算法计算出来的，默认使用双线性插值。    transforms.RandomHorizontalFlip(), #以 0.5 的概率水平随机翻转图像    transforms.RandomRotation(10),  #将图像随机旋转 -10 到 10 度之间的角度    transforms.ToTensor(),  #将图像从 PIL 图像格式转换为 PyTorch 张量    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))     #使用为三个颜色通道中的每一个提供的平均和标准差来标准化图像的像素值(归一化)])transform_test = transforms.Compose([    transforms.Resize(224),    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])# 加载CIFAR-10数据集train_set = torchvision.datasets.CIFAR10(&quot;./data&quot;, download=True, transform=transform_train)test_set = torchvision.datasets.CIFAR10(&quot;./data&quot;, download=True, train=False, transform=transform_test)batch_size = 64  train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)#返回数字对应的类名def output_label(label):    output_mapping = &#123;        0: &quot;plane&quot;, 1: &quot;car&quot;, 2: &quot;bird&quot;, 3: &quot;cat&quot;, 4: &quot;deer&quot;,        5: &quot;dog&quot;, 6: &quot;frog&quot;, 7: &quot;horse&quot;, 8: &quot;ship&quot;, 9: &quot;truck&quot;    &#125;    input = (label.item() if type(label) == torch.Tensor else label)    return output_mapping[input]
加载预训练模型
def get_resnet18(pretrained=True):    model = torchvision.models.resnet18(pretrained=pretrained)    model.fc = nn.Linear(512, 10)  # 修改最后一层以适应CIFAR10的10个类别    return modelmodel = get_resnet18()model = model.to(device)
模型训练超参数设置
#定义损失函数，初始学习率，优化器criterion = nn.CrossEntropyLoss()learning_rate = 0.001optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#学习率调节器，前期需要较大步长来快速收敛，后期如还用较大步长可能会在极小值周围震荡，所以后期学习率逐步衰减。可以提高训练效率和性能。Adam负责在参数层面进行自适应的学习率调整，以加快收敛。学习率调度器负责在训练过程的宏观层面调整全局学习率，以提高模型的最终性能和泛化能力。二者不会产生冲突。def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=2):    lr = init_lr * (0.1 ** (epoch // lr_decay_epoch))    if epoch % lr_decay_epoch == 0:        print(f&#x27;LR is set to &#123;lr&#125;&#x27;)    for param_group in optimizer.param_groups:        param_group[&#x27;lr&#x27;] = lr    return optimizer
模型训练
num_epochs = 6count = 0loss_list = []iteration_list = []accuracy_list = []predictions_list = []labels_list = []for epoch in range(num_epochs):    model.train()     running_loss = 0.0    optimizer = exp_lr_scheduler(optimizer, epoch, init_lr=learning_rate, lr_decay_epoch=2)    for i, (images, labels) in enumerate(train_loader):        images, labels = images.to(device), labels.to(device)        outputs = model(images)        loss = criterion(outputs, labels)        optimizer.zero_grad()        loss.backward()        optimizer.step()        running_loss += loss.item()        count += 1        if (i + 1) % 50 == 0:            print(f&#x27;Epoch [&#123;epoch + 1&#125;/&#123;num_epochs&#125;], Step [&#123;i + 1&#125;/&#123;len(train_loader)&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;)            model.eval()              total = 0            correct = 0            test_predictions = []            test_labels = []            with torch.no_grad():                for images, labels in test_loader:                    images, labels = images.to(device), labels.to(device)                    test_labels.extend(labels.cpu().numpy())                    outputs = model(images)                    _, predicted = torch.max(outputs.data, 1)                    test_predictions.extend(predicted.cpu().numpy())                    total += labels.size(0)                    correct += (predicted == labels).sum().item()            accuracy = 100 * correct / total            print(f&#x27;Accuracy on test set: &#123;accuracy:.2f&#125;%&#x27;)            loss_list.append(loss.item())            iteration_list.append(count)            accuracy_list.append(accuracy)            predictions_list.append(test_predictions)            labels_list.append(test_labels)            model.train()print(&#x27;Finished Training&#x27;)
实验结果保存
torch.save(model.state_dict(), &#x27;resnet18_cifar10.pth&#x27;)print(&#x27;Model saved to resnet18_cifar10.pth&#x27;)plt.figure(figsize=(10, 5))plt.plot(iteration_list, loss_list)plt.xlabel(&quot;No. of Iteration&quot;)plt.ylabel(&quot;Loss&quot;)plt.title(&quot;Iterations vs Loss&quot;)plt.savefig(&#x27;loss_curve.png&#x27;)plt.show()plt.figure(figsize=(10, 5))plt.plot(iteration_list, accuracy_list)plt.xlabel(&quot;No. of Iteration&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.title(&quot;Iterations vs Accuracy&quot;)plt.savefig(&#x27;accuracy_curve.png&#x27;)plt.show()class_correct = [0. for _ in range(10)]total_correct = [0. for _ in range(10)]model.eval()with torch.no_grad():    for images, labels in test_loader:        images, labels = images.to(device), labels.to(device)        outputs = model(images)        _, predicted = torch.max(outputs, 1)        c = (predicted == labels).squeeze()        for i in range(labels.size(0)):            label = labels[i]            class_correct[label] += c[i].item()            total_correct[label] += 1for i in range(10):    print(f&quot;Accuracy of &#123;classes[i]&#125;: &#123;100 * class_correct[i] / total_correct[i]:.2f&#125;%&quot;)flat_predictions = list(chain.from_iterable(predictions_list))flat_labels = list(chain.from_iterable(labels_list))cm = confusion_matrix(flat_labels, flat_predictions)print(&quot;Confusion Matrix:&quot;)print(cm)print(&quot;Classification report for ResNet on CIFAR-10:&quot;)print(classification_report(flat_labels, flat_predictions, target_names=classes))
实验结果分析
lr分析
由实验结果可得lr=0.01时，效果较差，所以直接设置初始学习率为0.001可以更节省时间，提高效率。当学习率为1e-6时，发现准确率更新较小，所以准确率最小设置为1e-6。即学习率梯度为1e-3,1e-4,1e-5,1e-6，可以有较好的效果。即令epoch / lr_decay_epoch = 3或4 都可以，具体以epoch大小为准。
lr=0.01

lr=0.001

lr=1e6

batchsize分析
Batchsize：64    epoch：6      lr_decay_epoch：2      初始学习率为0.001       94.46%




Batchsize：128       epoch：12      lr_decay_epoch：3        初始学习率为0.001     95.23%




Batchsize：128    epoch：20      lr_decay_epoch：5       初始学习率为0.01        85.87%
和分析的一致，对比下面实验，控制变量，只有初始学习率改变，精度提升10%左右。




Batchsize：128    epoch：20      lr_decay_epoch：5       初始学习率为0.001        95.44%




Batchsize：64      epoch：20      lr_decay_epoch：5       初始学习率为0.001        95.66%




使用LeNet网络
与Resnet相比换了一个网络和改了数据加载模块，其他没啥变化。
库函数导入
import matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torchvisionimport torchvision.transforms as transformsfrom sklearn.metrics import confusion_matrix, classification_reportfrom itertools import chainimport multiprocessingdevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
数据集加载及增强操作
transform = transforms.Compose(        [transforms.RandomHorizontalFlip(),         transforms.RandomCrop(32, padding=4),         transforms.ToTensor(),         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,                                            download=True, transform=transform)train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,                                              shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False,                                           download=True, transform=transform)test_loader = torch.utils.data.DataLoader(testset, batch_size=32,                                             shuffle=False, num_workers=2)classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)def output_label(label):    output_mapping = &#123;        0: &quot;plane&quot;, 1: &quot;car&quot;, 2: &quot;bird&quot;, 3: &quot;cat&quot;, 4: &quot;deer&quot;,        5: &quot;dog&quot;, 6: &quot;frog&quot;, 7: &quot;horse&quot;, 8: &quot;ship&quot;, 9: &quot;truck&quot;    &#125;    input = (label.item() if type(label) == torch.Tensor else label)    return output_mapping[input]
LeNet模型
class C1(nn.Module):    def __init__(self):        super(C1, self).__init__()        self.c1 = nn.Sequential(OrderedDict([            (&#x27;conv1&#x27;, nn.Conv2d(3, 6, kernel_size=(5, 5))),            (&#x27;relu1&#x27;, nn.ReLU()),            (&#x27;pool1&#x27;, nn.MaxPool2d(kernel_size=(2, 2), stride=2))        ]))    def forward(self, img):        output = self.c1(img)        return outputclass C3(nn.Module):    def __init__(self):        super(C3, self).__init__()        self.c3 = nn.Sequential(OrderedDict([            (&#x27;conv3&#x27;, nn.Conv2d(6, 16, kernel_size=(5, 5))),            (&#x27;relu3&#x27;, nn.ReLU()),            (&#x27;pool3&#x27;, nn.MaxPool2d(kernel_size=(2, 2), stride=2))        ]))    def forward(self, img):        output = self.c3(img)        return outputclass F4(nn.Module):    def __init__(self):        super(F4, self).__init__()        self.f4 = nn.Sequential(OrderedDict([            (&#x27;fc4&#x27;, nn.Linear(16 * 5 * 5, 120)),            (&#x27;relu4&#x27;, nn.ReLU())        ]))    def forward(self, img):        output = self.f4(img)        return outputclass F5(nn.Module):    def __init__(self):        super(F5, self).__init__()        self.f5 = nn.Sequential(OrderedDict([            (&#x27;fc5&#x27;, nn.Linear(120, 84)),            (&#x27;relu5&#x27;, nn.ReLU())        ]))    def forward(self, img):        output = self.f5(img)        return outputclass F6(nn.Module):    def __init__(self):        super(F6, self).__init__()        self.f6 = nn.Sequential(OrderedDict([            (&#x27;fc6&#x27;, nn.Linear(84, 10))        ]))    def forward(self, img):        output = self.f6(img)        return outputclass LeNet5(nn.Module):    def __init__(self):        super(LeNet5, self).__init__()        self.c1 = C1()        self.c3 = C3()        self.f4 = F4()        self.f5 = F5()        self.f6 = F6()    def forward(self, img):        output = self.c1(img)        output = self.c3(output)        output = output.view(-1, 16 * 5 * 5)        output = self.f4(output)        output = self.f5(output)        output = self.f6(output)        return output
模型训练超参数设置
#定义损失函数，初始学习率，优化器criterion = nn.CrossEntropyLoss()learning_rate = 0.001optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#学习率调节器，前期需要较大步长来快速收敛，后期如还用较大步长可能会在极小值周围震荡，所以后期学习率逐步衰减。可以提高训练效率和性能。def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=2):    lr = init_lr * (0.1 ** (epoch // lr_decay_epoch))    if epoch % lr_decay_epoch == 0:        print(f&#x27;LR is set to &#123;lr&#125;&#x27;)    for param_group in optimizer.param_groups:        param_group[&#x27;lr&#x27;] = lr    return optimizer
模型训练
num_epochs = 20count = 0loss_list = []iteration_list = []accuracy_list = []predictions_list = []labels_list = []for epoch in range(num_epochs):    model.train()     running_loss = 0.0    optimizer = exp_lr_scheduler(optimizer, epoch, init_lr=learning_rate, lr_decay_epoch=2)    for i, (images, labels) in enumerate(train_loader):        images, labels = images.to(device), labels.to(device)        outputs = model(images)        loss = criterion(outputs, labels)        optimizer.zero_grad()        loss.backward()        optimizer.step()        running_loss += loss.item()        count += 1        if (i + 1) % 50 == 0:            print(f&#x27;Epoch [&#123;epoch + 1&#125;/&#123;num_epochs&#125;], Step [&#123;i + 1&#125;/&#123;len(train_loader)&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;)            model.eval()             total = 0            correct = 0            test_predictions = []            test_labels = []            with torch.no_grad():                for images, labels in test_loader:                    images, labels = images.to(device), labels.to(device)                    test_labels.extend(labels.cpu().numpy())                    outputs = model(images)                    _, predicted = torch.max(outputs.data, 1)                    test_predictions.extend(predicted.cpu().numpy())                    total += labels.size(0)                    correct += (predicted == labels).sum().item()            accuracy = 100 * correct / total            print(f&#x27;Accuracy on test set: &#123;accuracy:.2f&#125;%&#x27;)            loss_list.append(loss.item())            iteration_list.append(count)            accuracy_list.append(accuracy)            predictions_list.append(test_predictions)            labels_list.append(test_labels)            model.train()print(&#x27;Finished Training&#x27;)
实验结果保存
torch.save(model.state_dict(), &#x27;lenet5_cifar10.pth&#x27;) print(&#x27;Model saved to lenet5_cifar10.pth&#x27;)plt.figure(figsize=(10, 5))plt.plot(iteration_list, loss_list)plt.xlabel(&quot;No. of Iteration&quot;)plt.ylabel(&quot;Loss&quot;)plt.title(&quot;Iterations vs Loss&quot;)plt.savefig(&#x27;loss_curve.png&#x27;)plt.show()plt.figure(figsize=(10, 5))plt.plot(iteration_list, accuracy_list)plt.xlabel(&quot;No. of Iteration&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.title(&quot;Iterations vs Accuracy&quot;)plt.savefig(&#x27;accuracy_curve.png&#x27;)plt.show()class_correct = [0. for _ in range(10)]total_correct = [0. for _ in range(10)]model.eval()with torch.no_grad():    for images, labels in test_loader:        images, labels = images.to(device), labels.to(device)        outputs = model(images)        _, predicted = torch.max(outputs, 1)        c = (predicted == labels).squeeze()        for i in range(labels.size(0)):            label = labels[i]            class_correct[label] += c[i].item()            total_correct[label] += 1for i in range(10):    print(f&quot;Accuracy of &#123;classes[i]&#125;: &#123;100 * class_correct[i] / total_correct[i]:.2f&#125;%&quot;)flat_predictions = list(chain.from_iterable(predictions_list))flat_labels = list(chain.from_iterable(labels_list))cm = confusion_matrix(flat_labels, flat_predictions)print(&quot;Confusion Matrix:&quot;)print(cm)print(&quot;Classification report for LeNet on CIFAR-10:&quot;) print(classification_report(flat_labels, flat_predictions, target_names=classes))
实验结果分析
使用图像增强操作
Batchsize：128      epoch：20      lr_decay_epoch：5     初始学习率为 0.001       60.56%




未使用图像增强操作
Batchsize：128      epoch：20      lr_decay_epoch：5     初始学习率为 0.001       62.66%




使用FashionNet
代码详见github
图像增强操作训练结果分析
使用图像增强操作
Batchsize：128      epoch：20      lr_decay_epoch：5     初始学习率为 0.001        71.51%


未使用图像增强操作
Batchsize：128      epoch：30      lr_decay_epoch：6     初始学习率为 0.01        71.69%




Batchsize：128      epoch：20      lr_decay_epoch：5     初始学习率为 0.001        74.25%




Gradcam实现图像特征可视化
Grad-CAM 的目标层均为最后一个卷积层
Resnet效果图
参数配置    Batch_size：64    epoch：20      lr_decay_epoch：5     lr：0.001      95.66%
图片来自谷歌。
  
      
FashionNet效果图
参数配置      Bachsize：128      epoch：20      lr_decay_epoch：5     初始学习率为 0.001        74.25%
  
      
LeNet效果图
参数配置      Bachsize：128      epoch：20      lr_decay_epoch：5     初始学习率为 0.001       60.56%
  
      
总结
数据集在不同模型上的准确率



模型
准确率




ResNet
95.66%


LeNet
62.66%


FashionNet
74.25%



LeNet对数据集进行图像增强操作的影响
分析：由于模型过于简单，本来就会出现欠拟合现象，继续进行图像增强操作后会导致模型更加欠拟合。所以出现三个模型精度差距较大，主要是因为模型复杂度的差距。虽然是欠拟合，但是继续增加epoch也并不会提高精度，因为模型对数据已经学到了尽可能多的知识。



未归一化
归一化
图像增强操作
大量图像增强操作




58.75%
62.66%
60.56%
56.78%



神经网络参数对模型准确率的影响
batch_size：64或128并无太大影响。
初始学习率：由实验结果可得lr=0.01时，效果较差，三个网络都使用了0.01进行测试得出的结果，有的实验结果没贴图。所以直接设置初始学习率为0.001可以更节省时间，提高效率。当学习率为1e-6时，发现准确率更新较小，所以准确率最小设置为1e-6。即学习率梯度为1e-3,1e-4,1e-5,1e-6，可以有较好的效果。
lr_decay_epoch和epoch：即学习率梯度为1e-3,1e-4,1e-5,1e-6，可以有较好的效果。即令epoch / lr_decay_epoch = 3或4 都可以，具体以epoch大小为准。
]]></content>
      <categories>
        <category>手撕代码</category>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>图像分类</tag>
        <tag>Resnet</tag>
        <tag>Lenet</tag>
        <tag>CIFAR10</tag>
      </tags>
  </entry>
  <entry>
    <title>实例判别学习 - Non-Parametric Instance Discrimination精读</title>
    <url>/2025/11/%E5%AE%9E%E4%BE%8B%E5%88%A4%E5%88%AB%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[Unsupervised Feature Learning via Non-Parametric Instance Discrimination
论文地址：https://arxiv.org/pdf/1805.01978
代码地址：https://github.com/zhirongw/lemniscate.pytorch
引言
实例判别（Instance Discrimination） 是2018年提出的一种无监督特征学习方法，它将对比学习的思想推向极致：将每个图像实例视为一个独立的类别，通过区分不同实例来学习特征表示。这篇论文是MoCo、SimCLR等现代对比学习方法的重要先驱工作。
论文的核心洞察来自对监督学习的观察：在ImageNet上训练的神经网络，即使没有显式指导，也能自动发现视觉上相似的类别（如leopard和jaguar）之间的关联。作者将这一观察延伸到实例级别：如果能够区分每个实例，那么学习到的特征应该能够捕获实例间的视觉相似性。
实例判别的核心创新
将无监督问题转化为监督问题：
实例判别的天才之处在于一个简单的转换：

传统无监督学习：没有标签，难以定义学习目标
实例判别：每个图像就是自己的标签，变成N分类问题
好处：可以直接使用成熟的分类框架和技术

为什么这种方法有效？


视觉相似性的自然涌现

虽然每个实例是独立类别，但视觉相似的实例会在特征空间聚集
原因：它们共享底层视觉模式（边缘、纹理、形状等）
结果：学到的特征捕获了语义信息



信息论的解释

区分N个实例需要log(N)比特信息
模型被迫学习最有效的编码
这种编码自然对应于语义特征



避免平凡解

不能简单记住每个实例（泛化要求）
必须学习可迁移的特征
数据增强强制学习不变性



技术挑战与解决方案
挑战1：计算Softmax的分母
当有百万级实例时，计算softmax分母需要遍历所有实例，计算成本巨大。
解决方案：NCE近似
原始softmax概率：P(i|v) = exp(v·f_i) / Σ_j exp(v·f_j)NCE近似：将其转化为二分类问题（实例i vs 噪声）
挑战2：存储所有实例的特征
需要存储所有训练图像的特征表示，内存需求大。
解决方案：Memory Bank

使用动量更新：新特征 = m×旧特征 + (1-m)×当前特征
好处：平滑特征变化，提高训练稳定性
内存需求：128维×100万实例 ≈ 500MB（可接受）

挑战3：负样本采样
从百万实例中采样负样本，如何保证采样质量？
解决方案：Proximal Regularization

不是随机采样，而是基于特征相似度
优先采样相似但不同的实例（困难负样本）
提高学习效率

核心思想
基本动机
关键观察：

监督学习中，模型会自动学习到视觉相似类别之间的关联
这种相似性不是来自语义标注，而是来自视觉数据本身
如果将类别级别的监督推向实例级别，能否学到更好的表示？

核心假设：

每个图像实例本身就是一个独特的&quot;类别&quot;
通过区分不同实例，模型会学习到捕获视觉相似性的特征
视觉相似的实例在特征空间中应该更接近

方法概述
将无监督学习问题转化为实例级别的分类问题：

每个训练图像是一个独立的类别
使用非参数softmax进行分类
使用NCE（Noise-Contrastive Estimation）解决计算问题

方法详解
非参数Softmax分类器
参数化Softmax的问题
传统的参数化softmax：
$$
P(i|v) = \frac{\exp(w_i^T v)}{\sum_{j=1}^{n} \exp(w_j^T v)}
$$
其中 $w_j$ 是类别 $j$ 的权重向量。
问题：

权重向量 $w_j$ 只对训练类别有效，无法泛化到新实例
权重向量作为类别原型，阻止了实例间的直接比较
需要存储和更新大量权重参数

非参数Softmax
将权重向量 $w_j$ 替换为特征向量 $v_j$：
$$
P(i|v) = \frac{\exp(v_i^T v / \tau)}{\sum_{j=1}^{n} \exp(v_j^T v / \tau)}
$$
其中：

$v_i, v_j$ 是归一化后的特征向量（$||v|| = 1$）
$\tau$ 是温度参数，控制分布的尖锐程度

优势：

直接比较实例：特征向量直接作为&quot;类别原型&quot;
泛化能力强：学习到的特征可以应用到任何新实例
计算高效：无需存储和更新权重参数
训练测试一致：训练和测试都使用相同的度量空间

Memory Bank机制
问题：计算 $P(i|v)$ 需要所有实例的特征 ${v_j}$，但每次迭代只更新当前batch的特征。
解决方案：维护一个Memory Bank $V = {v_j}$ 存储所有实例的特征。
更新策略：

初始化：Memory Bank中的特征初始化为随机单位向量
更新：每次迭代后，将当前batch的特征更新到Memory Bank
使用：计算损失时，从Memory Bank中读取特征

优势：

避免每次重新计算所有特征
保持特征的历史信息
计算效率高

Noise-Contrastive Estimation (NCE)
问题：当实例数量 $n$ 很大（如ImageNet的1.2M）时，计算完整的softmax不可行。
解决方案：使用NCE将多类分类问题转化为二分类问题。
NCE原理
将问题转化为：区分数据样本和噪声样本。
定义：

数据分布：$P_d(i\mid v) = \frac{\exp(v_i^\top v / \tau)}{Z_i}$，其中 $Z_i = \sum_{j=1}^{n} \exp(v_j^\top v / \tau)$
噪声分布：$P_n(i) = 1/n$（均匀分布）
噪声样本频率：$m$ 倍于数据样本

后验概率：
$$ 
h(i, v) = P(D=1\mid i, v) = \frac{P_d(i\mid v)}{P_d(i\mid v) + m \cdot P_n(i)}
$$ 
损失函数：
$$ 
\mathcal{L}_{\text{NCE}}(\theta) = -\mathbb{E}_{(i,v)\sim P_d}[\log h(i, v)] - m \cdot \mathbb{E}_{(i,v')\sim P_n}[\log\big(1 - h(i, v')\big)]
$$ 
归一化常数估计
计算 $Z_i$ 仍然昂贵，使用蒙特卡洛估计：
$$ 
Z_i \approx \frac{n}{m} \sum_{k=1}^{m} \exp\big(v_{j_k}^\top v / \tau\big)
$$ 
其中 ${j_k}$ 是随机采样的索引。
复杂度：从 $O(n)$ 降低到 $O(1)$ 每个样本。
Proximal Regularization（近端正则化）
问题：每个&quot;类别&quot;只有一个实例，每个epoch每个类别只访问一次，训练过程振荡严重。
解决方案：引入近端正则化项，鼓励训练过程的平滑性。
正则化项：
$$ 
\lambda \big\lVert v_i^{(t)} - v_i^{(t-1)} \big\rVert_2^2
$$ 
其中：

$v_i^{(t)}$ 是当前迭代的特征
$v_i^{(t-1)}$ 是Memory Bank中存储的特征（上一迭代）

完整损失：
$$ 
\mathcal{L}_{\text{NCE}}(\theta) = -\mathbb{E}_{(i,v)\sim P_d}\Big[\log h\big(i, v_i^{(t-1)}\big) - \lambda \big\lVert v_i^{(t)} - v_i^{(t-1)} \big\rVert_2^2\Big] - m \cdot \mathbb{E}_{(i,v')\sim P_n}\Big[\log\big(1 - h(i, v'^{(t-1)})\big)\Big]
$$ 
加权k-NN分类器
测试时分类：

计算特征：$\hat{f} = f_\theta(\hat{x})$
相似度计算：$s_i = \cos(v_i, \hat{f})$
k-NN检索：找到top-k最近邻 $N_k$
加权投票：类别 $c$ 的权重为
$$
w_c = \sum_{i \in N_k} \alpha_i \cdot \mathbf{1}(c_i = c)
$$
其中 $\alpha_i = \exp(s_i / \tau)$

参数设置：

$k = 200$
$\tau = 0.07$（与训练时相同）

实验分析
参数化 vs 非参数化Softmax
在CIFAR-10上的对比实验：



方法
Linear SVM
k-NN




参数化Softmax
60.3%
63.0%


非参数化Softmax
75.4%
80.8%



关键发现：

非参数化方法显著优于参数化方法（+18%）
k-NN性能接近线性SVM，说明学习到的特征度量质量高

NCE近似质量
不同负样本数量 $m$ 的影响：



m
k-NN Accuracy




1
42.5%


10
63.4%


512
78.4%


4096
80.4%



结论：随着 $m$ 增加，NCE近似质量提升，$m=4096$ 时接近完整softmax。
ImageNet分类结果
不同网络架构的性能：



方法
conv5 (Linear)
k-NN
特征维度




Random
14.1%
3.5%
10K


Split-Brain
35.2%
11.8%
10K


Ours (AlexNet)
35.6%
31.3%
128


Ours (VGG16)
39.2%
33.9%
128


Ours (ResNet-18)
44.5%
41.0%
128


Ours (ResNet-50)
54.0%
46.5%
128



关键发现：

显著超越SOTA：在ImageNet上大幅超越之前的方法
网络深度优势：从AlexNet到ResNet-50，性能持续提升
紧凑表示：仅用128维特征就达到优异性能
存储高效：1.28M图像的特征仅需600MB存储

特征泛化能力
在Places数据集上的零样本评估（使用ImageNet训练的特征）：



方法
ResNet-50 (conv5)
ResNet-50 (k-NN)




Ours
45.5%
41.6%



结论：学习到的特征具有良好的跨数据集泛化能力。
训练目标与测试目标的一致性
关键观察：

训练损失持续下降
测试准确率同步提升
无过拟合迹象

意义：说明训练目标（实例判别）与测试目标（语义分类）是一致的，学习到的特征确实捕获了视觉相似性。
消融实验
特征维度：



维度
Top-1 Accuracy




32
34.0%


64
38.8%


128
41.0%


256
40.1%



结论：128维是最优选择，性能在128维达到峰值。
训练数据量：



数据比例
Accuracy




0.1%
3.9%


1%
10.7%


10%
23.1%


30%
31.7%


100%
41.0%



结论：性能随训练数据量增加而持续提升，说明方法具有良好的可扩展性。
半监督学习
使用不同比例的标注数据进行微调：



标注比例
Ours (ResNet)
Scratch (ResNet)
Split-Brain (AlexNet)




1%
48.0%
38.0%
35.0%


2%
57.0%
46.0%
42.0%


4%
63.0%
54.0%
50.0%


10%
70.0%
65.0%
62.0%


20%
75.0%
72.0%
70.0%



结论：在少样本场景下优势明显，1%标注数据时领先10%。
目标检测
在PASCAL VOC 2007上的检测性能：



方法
AlexNet
VGG16
ResNet-50




Supervised
56.8%
67.3%
76.2%


Ours
48.1%
60.5%
65.4%



结论：在检测任务上达到无监督方法的SOTA，且随网络加深性能提升。
代码实现
核心损失函数实现
import torchimport torch.nn as nnimport torch.nn.functional as Fclass InstanceDiscriminationLoss(nn.Module):    &quot;&quot;&quot;实例判别损失（使用NCE近似）&quot;&quot;&quot;        def __init__(self, memory_bank, temperature=0.07, m=4096, lambda_reg=10.0):        super().__init__()        self.memory_bank = memory_bank  # [n, dim] 所有实例的特征        self.temperature = temperature        self.m = m  # 负样本数量        self.lambda_reg = lambda_reg  # 正则化系数        def forward(self, features, indices, memory_bank_prev):        &quot;&quot;&quot;        Args:            features: [batch_size, dim] 当前batch的特征            indices: [batch_size] 当前batch的实例索引            memory_bank_prev: [batch_size, dim] Memory Bank中的特征（上一迭代）        &quot;&quot;&quot;        batch_size = features.size(0)        device = features.device                # L2归一化        features = F.normalize(features, p=2, dim=1)                # 从Memory Bank获取正样本特征        pos_features = memory_bank_prev  # [batch_size, dim]                # 随机采样负样本        neg_indices = torch.randint(0, len(self.memory_bank), (batch_size, self.m), device=device)        neg_features = self.memory_bank[neg_indices]  # [batch_size, m, dim]                # 计算相似度        pos_sim = torch.sum(features * pos_features, dim=1, keepdim=True) / self.temperature  # [batch_size, 1]        neg_sim = torch.bmm(            features.unsqueeze(1),  # [batch_size, 1, dim]            neg_features.transpose(1, 2)  # [batch_size, dim, m]        ) / self.temperature  # [batch_size, 1, m]                # 估计归一化常数（蒙特卡洛）        Z_approx = (len(self.memory_bank) / self.m) * torch.sum(torch.exp(neg_sim), dim=2)  # [batch_size, 1]                # 计算概率        pos_prob = torch.exp(pos_sim) / (torch.exp(pos_sim) + self.m * (1.0 / len(self.memory_bank)) * Z_approx)                # NCE损失        nce_loss = -torch.mean(torch.log(pos_prob + 1e-8))                # 近端正则化        reg_loss = self.lambda_reg * torch.mean(torch.sum((features - memory_bank_prev) ** 2, dim=1))                total_loss = nce_loss + reg_loss                return total_loss, nce_loss, reg_loss
完整训练流程
import torchimport torch.optim as optimfrom torch.utils.data import DataLoaderclass InstanceDiscriminationTrainer:    def __init__(self, model, memory_bank, dataset_size, temperature=0.07, m=4096):        self.model = model        self.memory_bank = memory_bank  # 初始化为随机单位向量        self.temperature = temperature        self.m = m                # 初始化Memory Bank        self.memory_bank.data = F.normalize(            torch.randn(dataset_size, 128), p=2, dim=1        )        def train_epoch(self, dataloader, optimizer, criterion):        self.model.train()        total_loss = 0                for batch_idx, (images, indices) in enumerate(dataloader):            # 前向传播            features = self.model(images)  # [batch_size, 128]                        # 从Memory Bank获取上一迭代的特征            memory_bank_prev = self.memory_bank[indices].clone()                        # 计算损失            loss, nce_loss, reg_loss = criterion(                features, indices, memory_bank_prev            )                        # 反向传播            optimizer.zero_grad()            loss.backward()            optimizer.step()                        # 更新Memory Bank（动量更新）            with torch.no_grad():                self.memory_bank[indices] = F.normalize(features, p=2, dim=1)                        total_loss += loss.item()                        if batch_idx % 100 == 0:                print(f&#x27;Batch &#123;batch_idx&#125;, Loss: &#123;loss.item():.4f&#125;, &#x27;                      f&#x27;NCE: &#123;nce_loss.item():.4f&#125;, Reg: &#123;reg_loss.item():.4f&#125;&#x27;)                return total_loss / len(dataloader)
k-NN分类器实现
def knn_classify(query_features, memory_bank, memory_labels, k=200, temperature=0.07):    &quot;&quot;&quot;    使用k-NN对查询特征进行分类        Args:        query_features: [n_query, dim] 查询特征        memory_bank: [n_memory, dim] Memory Bank中的特征        memory_labels: [n_memory] Memory Bank中的标签        k: 最近邻数量        temperature: 温度参数    &quot;&quot;&quot;    # 计算相似度矩阵    similarity = torch.mm(        F.normalize(query_features, p=2, dim=1),        F.normalize(memory_bank, p=2, dim=1).t()    )  # [n_query, n_memory]        # 找到top-k最近邻    _, topk_indices = torch.topk(similarity, k, dim=1)  # [n_query, k]    topk_similarities = torch.gather(similarity, 1, topk_indices)  # [n_query, k]    topk_labels = memory_labels[topk_indices]  # [n_query, k]        # 加权投票    weights = torch.exp(topk_similarities / temperature)  # [n_query, k]        # 对每个类别求和权重    num_classes = memory_labels.max().item() + 1    class_weights = torch.zeros(query_features.size(0), num_classes, device=query_features.device)        for c in range(num_classes):        mask = (topk_labels == c)  # [n_query, k]        class_weights[:, c] = (weights * mask.float()).sum(dim=1)        # 预测类别    predictions = class_weights.argmax(dim=1)        return predictions
技术细节
Memory Bank更新策略
选项1：直接替换
memory_bank[indices] = features
选项2：动量更新
memory_bank[indices] = momentum * memory_bank[indices] + (1 - momentum) * features
选项3：仅在训练时更新

训练时：每次迭代更新
测试时：固定不变

温度参数选择

典型值：0.07（与SimCLR、MoCo相同）
调优范围：0.05 - 0.15
影响：控制softmax分布的尖锐程度

NCE负样本数量

最小值：$m = 1$（性能较差）
推荐值：$m = 4096$（性能接近完整softmax）
权衡：$m$ 越大，性能越好，但计算成本也越高

近端正则化系数

典型值：$\lambda = 10.0$
作用：稳定训练，加速收敛
调优：根据训练稳定性调整

优缺点分析
优点

概念简单：将无监督学习转化为实例分类问题
非参数化：特征直接作为&quot;类别原型&quot;，泛化能力强
存储高效：128维特征，1M图像仅需600MB
可扩展性好：性能随数据和网络深度提升
训练测试一致：都使用相同的度量空间

缺点

计算成本：需要大量负样本（NCE）
Memory Bank维护：需要存储所有实例的特征
更新延迟：Memory Bank更新有延迟，可能影响性能
对数据增强依赖：需要合理的数据增强策略

与后续工作的关系
对MoCo的启发

Memory Bank → 队列：MoCo用队列替代Memory Bank
直接更新 → 动量更新：MoCo使用动量编码器
一致性保证：两者都关注特征的一致性

对SimCLR的启发

实例判别思想：SimCLR也使用实例判别
端到端训练：SimCLR去除了Memory Bank，使用大batch
数据增强重要性：SimCLR强调了数据增强的关键作用

对SupCon的启发

监督信号融入：SupCon将标签信息融入对比学习
同类样本作为正样本：SupCon扩展了正样本的定义

总结
实例判别学习通过将每个图像实例视为独立类别，使用非参数softmax和NCE来学习特征表示。其核心贡献包括：

非参数化设计：特征直接作为类别原型，泛化能力强
NCE近似：解决了大规模实例分类的计算问题
Memory Bank机制：高效存储和更新所有实例的特征
近端正则化：稳定训练过程

虽然后续的MoCo、SimCLR等方法在实现上更加优雅，但实例判别学习奠定了对比学习的基础思想，是这一领域的重要里程碑。
参考文献


Wu, Z., Xiong, Y., Yu, S. X., &amp; Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. CVPR 2018. https://arxiv.org/pdf/1805.01978


Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. AISTATS 2010.


He, K., et al. (2020). Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.


Chen, T., et al. (2020). A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.


思考题

为什么非参数softmax比参数化softmax更适合实例判别任务？
NCE如何将多类分类问题转化为二分类问题？为什么这样有效？
Memory Bank机制的优势和局限性是什么？与MoCo的队列机制有何区别？
近端正则化为什么能稳定训练？其背后的数学原理是什么？
实例判别学习与后续的MoCo、SimCLR在思想上有何联系和区别？
为什么k-NN分类器在测试时表现良好？这说明了什么？

思考题答案
1. 为什么非参数softmax比参数化softmax更适合实例判别任务？
参数化softmax的问题：

泛化能力弱：权重向量 $w_j$ 只对训练实例有效，无法应用到新实例
阻止直接比较：权重向量作为&quot;类别原型&quot;，阻止了实例特征间的直接比较
参数冗余：需要为每个实例存储一个权重向量，参数量大

非参数softmax的优势：

直接比较实例：特征向量 $v_j$ 直接作为&quot;类别原型&quot;，允许实例间的直接比较
泛化能力强：学习到的特征可以应用到任何新实例
参数高效：无需存储权重参数，只需存储特征
训练测试一致：训练和测试都使用相同的特征度量空间

实验验证：在CIFAR-10上，非参数方法比参数方法性能提升18%，证明了其优势。
2. NCE如何将多类分类问题转化为二分类问题？为什么这样有效？
转化过程：

原始问题：区分 $n$ 个类别（实例），需要计算所有类别的概率
NCE转化：将问题转化为&quot;区分数据样本和噪声样本&quot;的二分类问题
后验概率：
$$
h(i, v) = \frac{P_d(i\mid v)}{P_d(i\mid v) + m \cdot P_n(i)}
$$
其中 $P_d$ 是数据分布，$P_n$ 是噪声分布（均匀分布）

为什么有效：

计算效率：从 $O(n)$ 降低到 $O(1)$ 每个样本
近似质量：当负样本数量 $m$ 足够大时，近似质量接近完整softmax
理论基础：NCE有坚实的理论保证，是unnormalized模型的标准估计方法
实践验证：实验表明 $m=4096$ 时性能接近完整softmax

关键洞察：不需要计算所有类别的概率，只需要区分&quot;是正样本&quot;还是&quot;是噪声样本&quot;。
3. Memory Bank机制的优势和局限性是什么？与MoCo的队列机制有何区别？
Memory Bank的优势：

存储所有特征：可以访问训练集中所有实例的特征
计算高效：避免每次重新计算所有特征
历史信息：保留特征的历史状态

Memory Bank的局限性：

更新延迟：特征更新有延迟（使用上一迭代的特征）
内存需求：需要存储所有实例的特征
一致性挑战：特征可能因为编码器变化而变得不一致

与MoCo队列的区别：



特性
Memory Bank
MoCo队列




大小
所有训练实例
固定大小（如65536）


更新
直接替换
FIFO（先进先出）


一致性
可能不一致
通过动量编码器保证


存储
所有实例
仅最近batch的特征



MoCo的改进：

使用动量编码器保证Key的一致性
队列大小固定，内存可控
FIFO更新保证队列中的特征相对新鲜

4. 近端正则化为什么能稳定训练？其背后的数学原理是什么？
问题背景：

每个&quot;类别&quot;只有一个实例
每个epoch每个类别只访问一次
随机采样导致训练过程振荡严重

近端正则化的作用：

平滑性约束：鼓励当前特征 $v_i^{(t)}$ 与上一迭代特征 $v_i^{(t-1)}$ 接近
减少振荡：防止特征因为随机采样而剧烈变化
稳定梯度：使梯度更新更加平滑

数学原理：
近端正则化项：
$$
\lambda \big\lVert v_i^{(t)} - v_i^{(t-1)} \big\rVert_2^2
$$
这等价于在优化问题中加入平滑性约束：

当 $\lambda \to 0$：退化为原始损失
当 $\lambda \to \infty$：强制 $v_i^{(t)} = v_i^{(t-1)}$（不更新）

优化视角：
近端正则化是近端梯度方法的应用，用于优化非光滑或约束优化问题。在这里，它起到了稳定化的作用。
实验验证：图3显示，加入正则化后训练损失曲线更平滑，收敛更快。
5. 实例判别学习与后续的MoCo、SimCLR在思想上有何联系和区别？
共同思想：

实例判别：都将每个实例视为独立类别
对比学习：通过区分正负样本来学习特征
特征归一化：都使用L2归一化
温度参数：都使用温度缩放

主要区别：



特性
Instance Discrimination
MoCo
SimCLR




负样本来源
Memory Bank
队列
当前batch


特征更新
延迟更新
动量更新
实时更新


计算方式
NCE近似
完整softmax
完整softmax


batch size
小（256）
中等（1024）
大（≥4096）


实现复杂度
中等
较高
较低



演进关系：

Instance Discrimination：提出非参数softmax和Memory Bank
MoCo：改进Memory Bank为队列，引入动量编码器
SimCLR：去除Memory Bank，使用大batch和端到端训练

核心改进：

MoCo解决了Memory Bank的一致性问题
SimCLR简化了实现，但需要更多计算资源

6. 为什么k-NN分类器在测试时表现良好？这说明了什么？
k-NN表现良好的原因：

训练测试一致：训练和测试都使用相同的特征度量空间
非参数化设计：特征直接作为&quot;类别原型&quot;，适合k-NN
度量质量高：学习到的特征确实捕获了视觉相似性

实验证据：

在ImageNet上，k-NN准确率（46.5%）接近线性SVM（54.0%）
说明学习到的特征度量质量很高

深层含义：

训练目标有效：实例判别目标与语义分类目标一致
特征质量高：特征空间中的距离确实反映了视觉相似性
泛化能力强：学习到的表示可以泛化到新任务

对比其他方法：

Split-Brain：k-NN准确率（11.8%）远低于线性SVM（35.2%）
说明其学习到的特征度量质量较低

结论：k-NN的良好表现证明了实例判别学习的有效性，学习到的特征确实捕获了视觉相似性，而不仅仅是适合特定分类器的表示。
深入理解与实践思考

实例判别与语义判别的关系：虽然每个实例都被视作一个类别，但模型最终学习到的特征仍然会聚合出语义簇。这一现象说明视觉数据的结构性远强于我们赋予的标签结构。实际操作中可以通过t-SNE/UMAP观察到同类样本自然聚团。
Memory Bank vs. 队列：Memory Bank提供了全局视角，但需要处理“陈旧”特征的问题。后来MoCo中的动量编码器与固定长度队列，可以被视作对“特征一致性”与“可扩展性”的不同取舍。若业务中GPU显存紧张，可采用分片Memory Bank或结合近邻图的稀疏更新。
正则化的更广义含义：近端正则化不仅平滑了优化，还暗含“特征轨迹不能跳跃太远”的约束，相当于引入了一个时间维度的Lipschitz限制。这提醒我们，在设计自监督任务时，需要关注优化轨迹是否稳定。
评价指标的选择：k-NN性能高说明学到的度量空间质量好，但在工业场景往往还需关注召回、延迟等指标。实例判别学习天然适合构建向量检索服务（如图像去重、相似商品推荐）。
与现代方法的衔接：若希望复现最新SOTA，可在此框架上引入更强的编码器（ViT、ConvNeXt）、更丰富的数据增强（RandAugment、CutMix）以及更加稳定的优化器（AdamW + Cosine）。同时也可以尝试将SupCon的标签约束融入Memory Bank以获得更平滑的语义结构。

开放问题与研究方向

Memory更新策略：能否利用动量更新与自适应权重结合，既保持全局视野又不过度依赖旧特征？
负样本质量：如何更智能地采样“困难负样本”，避免CNN在易区分的负样本上浪费容量？
多模态扩展：能否直接在该框架中支持图像-文本或多模态实例判别，减少对比学习与跨模态模型之间的割裂？
理论解释：实例判别为何能自动恢复语义结构？是否存在可证明的聚类或流形假设支撑？
增量学习：面对不断新增的数据集，如何无缝扩展Memory Bank并避免遗忘？

]]></content>
      <categories>
        <category>论文精读</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>论文精读</tag>
        <tag>对比学习</tag>
        <tag>实例判别</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>对比学习综述</title>
    <url>/2025/11/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[对比学习综述：从理论到实践全面解析
引言
对比学习（Contrastive Learning） 是近年来自监督学习领域最重要的突破之一，它通过&quot;拉近正样本、推远负样本&quot;的简单思想，在无需大量标注数据的情况下学习到强大的视觉表示。从2020年的SimCLR、MoCo开始，对比学习在ImageNet等基准上取得了与监督学习相当甚至更好的性能，彻底改变了我们对无监督表示学习的认知。
对比学习的核心优势在于：

无需标注数据：可以在海量无标注图像上预训练
学习鲁棒表示：对数据增强、噪声等具有强鲁棒性
迁移能力强：预训练的特征在下游任务上表现优异
可扩展性好：可以轻松扩展到大规模数据和模型

什么是对比学习？
核心思想
对比学习的核心思想可以用一句话概括：通过对比正样本对和负样本对，学习到区分性的表示。
正样本对（Positive Pairs）：应该相似的样本对

无监督：同一图像的不同增强视图
有监督：同一类别的不同样本

负样本对（Negative Pairs）：应该不相似的样本对

无监督：不同图像的增强视图
有监督：不同类别的样本

学习目标：让模型学习到的特征空间中，正样本对的距离近，负样本对的距离远。
为什么需要对比学习？
1. 标注数据稀缺问题

获取大规模标注数据成本高昂（ImageNet标注花费数百万美元）
很多领域缺乏专家标注（医疗、卫星图像等）
标注质量难以保证，存在标注噪声

2. 自监督学习的优势

利用数据自身的结构作为监督信号
可以在海量无标注数据上预训练
学习到的特征更通用，迁移性更好

3. 对比学习的独特价值

判别性强：通过对比学习到的特征比重建任务（如自编码器）更有判别性
计算高效：比生成模型（GAN、VAE）训练更稳定、收敛更快
理论基础：有信息论的理论支撑（互信息最大化）

对比学习的工作原理
核心机制详解：


正负样本构造

正样本对：同一图像通过不同数据增强得到的两个视图
负样本对：来自不同图像的视图
关键点：正样本共享语义信息，负样本语义不同



特征提取流程


特征提取流程
graph LRA[输入图像] --&gt; B[数据增强]B --&gt; C[编码器]C --&gt; D[特征向量]D --&gt; E[投影头]E --&gt; F[对比损失]

编码器：提取高维语义特征（如ResNet输出2048维）
投影头：映射到低维空间（通常128维），便于计算相似度



相似度计算

使用余弦相似度：$sim(z_i, z_j) = \frac{z_i \cdot z_j}{||z_i|| \cdot ||z_j||}$
温度缩放：$sim(z_i, z_j) / \tau$，控制分布的集中程度



优化目标

最大化正样本对的相似度
最小化负样本对的相似度
通过InfoNCE损失实现这一目标



对比学习的发展历程
早期工作（2010s）

Triplet Loss（2015）：最早的形式化对比学习思想
NCE（Noise Contrastive Estimation）：从语言模型引入对比思想
Instance Discrimination：将每个样本视为一个类别

现代对比学习（2020-）

SimCLR（2020）：端到端训练，大batch size
MoCo（2020）：动量编码器 + 队列机制
SupCon（2020）：监督对比学习
BYOL/SimSiam（2020-2021）：无需负样本的对比学习
CLIP（2021）：跨模态对比学习

核心损失函数：InfoNCE / NT-Xent
InfoNCE损失详解
损失函数定义：
给定一个正样本对 $(x, x^+)$ 和 N-1 个负样本 ${x_i^-}_{i=1}^{N-1}$，InfoNCE损失定义为：
$$ 
\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(sim(x, x^+) / \tau)}{\exp(sim(x, x^+) / \tau) + \sum_{i=1}^{N-1} \exp(sim(x, x_i^-) / \tau)}
$$ 
逐步理解损失函数：


相似度计算 $sim(x, x^+)$：

计算查询样本$x$与正样本$x^+$的余弦相似度
值域：[-1, 1]，1表示完全相同，-1表示完全相反



温度缩放 $sim/\tau$：

$\tau$通常设置为0.07-0.5
较小的$\tau$让模型更关注困难负样本
较大的$\tau$让优化更平滑



Softmax归一化：

将相似度转换为概率分布
正样本的&quot;概率&quot;应该接近1
每个负样本的&quot;概率&quot;应该接近0



负对数似然：

最小化损失 = 最大化正样本的概率
这等价于N分类问题，正确类别是正样本



为什么InfoNCE有效？

归一化：特征先进行L2归一化
温度缩放：$\tau$ 控制softmax分布的尖锐程度

温度参数 $\tau$ 的作用
温度参数 $\tau$ 是对比学习中的关键超参数：


$\tau$ 较小（如0.05）：

分布更尖锐，模型更关注困难负样本
学习到的表示区分性更强
但可能训练不稳定



$\tau$ 较大（如0.2）：

分布更平滑，对所有样本的关注更均匀
训练更稳定
但区分性可能较弱



典型取值：0.07（SimCLR、MoCo等常用）


主要方法详解
1. SimCLR（A Simple Framework for Contrastive Learning）
论文：https://arxiv.org/pdf/2002.05709
代码：https://github.com/google-research/simclr
核心思想
SimCLR提出了一个简单而有效的对比学习框架：

对每个样本应用两次随机增强，得到两个视图
使用共享编码器提取特征
通过投影头映射到对比空间
使用NT-Xent损失进行对比学习

关键创新

强数据增强：发现数据增强是对比学习成功的关键
投影头：在编码器和损失之间加入非线性投影头
大batch size：需要大量负样本（batch size ≥ 4096）

架构
架构
graph LR    subgraph SimCLR架构    I[输入图像] --&gt; T1[增强视图 1]    I --&gt; T2[增强视图 2]    T1 --&gt; E1[编码器 f]    T2 --&gt; E2[编码器 f]    E1 --&gt; H1[投影头 g]    E2 --&gt; H2[投影头 g]    H1 --&gt; Z1[特征 z1]    H2 --&gt; Z2[特征 z2]    Z1 &lt;--&gt; L[对比损失]    Z2 &lt;--&gt; L    end
优缺点
优点：

框架简单，易于实现
端到端训练，无需额外机制
性能优异

缺点：

需要大batch size，对计算资源要求高
对数据增强策略敏感

2. MoCo（Momentum Contrast）
论文：https://arxiv.org/pdf/1911.05722
代码：https://github.com/facebookresearch/moco
核心思想
MoCo将对比学习看作字典查找任务：

Query：当前样本的编码
Key：字典中的样本编码
目标：Query与匹配的Key相似，与其他Key不相似

关键创新

动量编码器：使用动量更新维护一个缓慢变化的编码器
队列机制：用队列存储历史样本的特征，提供大量负样本
一致性：动量更新保证字典中Key的一致性

架构
架构
graph LR    subgraph MoCo架构    Q[Query图像] --&gt; EQ[编码器]    EQ --&gt; QF[Query特征]    K[Key图像] --&gt; EK[动量编码器]    EK --&gt; KF[Key特征]    KF --&gt; Que[队列 Dictionary]    QF &lt;--&gt; Loss[对比损失]    Que &lt;--&gt; Loss    end
优缺点
优点：

不依赖大batch size
训练稳定，收敛快
内存效率高

缺点：

实现相对复杂
需要维护队列和动量编码器

3. SupCon（Supervised Contrastive Learning）
论文：https://arxiv.org/pdf/2004.11362
代码：https://github.com/HobbitLong/SupContrast
核心思想
将标签信息融入对比学习：

正样本：同一类别的所有样本
负样本：不同类别的样本

损失函数
$$
\mathcal{L}{sup}^i = -\frac{1}{|P(i)|} \sum{p \in P(i)} \log \frac{\exp(z_i \cdot z_p / \tau)}{\sum_{a \in A(i)} \exp(z_i \cdot z_a / \tau)}
$$
其中 $P(i)$ 是与样本 $i$ 同类的样本集合。
优缺点
优点：

利用标签信息，性能更优
鲁棒性显著提升
在长尾学习、少样本学习上表现优异

缺点：

需要标注数据
计算复杂度较高（$O(N^2)$）

4. BYOL / SimSiam（无需负样本）
BYOL论文：https://arxiv.org/pdf/2006.07733
SimSiam论文：https://arxiv.org/pdf/2011.10566
核心思想
通过预测任务替代对比任务，无需负样本：

一个视图预测另一个视图
使用停止梯度（stop-gradient）防止崩溃

关键机制

预测头：预测一个视图的特征
停止梯度：防止模型学习到平凡解（所有特征相同）
对称损失：同时优化两个方向

优缺点
优点：

无需负样本，计算更高效
训练更稳定

缺点：

理论理解仍在发展中
性能可能略低于有负样本的方法

5. SwAV (Swapping Assignments between Views)
论文：https://arxiv.org/pdf/2006.09882
代码：https://github.com/facebookresearch/swav
核心思想
SwAV 将对比学习与聚类相结合。它不直接对比两个视图的特征，而是对比它们在聚类中心的分配（Assignment）。

核心假设：同一图像的两个视图应该属于同一个聚类中心。
机制：用一个视图的特征去预测另一个视图的聚类分配。

架构
graph LR    subgraph SwAV架构    I[输入图像] --&gt; V1[视图 1]    I --&gt; V2[视图 2]    V1 --&gt; E1[编码器]    V2 --&gt; E2[编码器]    E1 --&gt; Z1[特征 Z1]    E2 --&gt; Z2[特征 Z2]    Z1 --&gt; P1[原型 Prototypes]    Z2 --&gt; P2[原型 Prototypes]    P1 --&gt; C1[聚类分配 Q1]    P2 --&gt; C2[聚类分配 Q2]    Z1 -.预测.-&gt; C2    Z2 -.预测.-&gt; C1    end
优缺点

优点：无需大量负样本，无需大batch size，训练效率高。
缺点：需要在线聚类，实现稍复杂。

6. CLIP (Contrastive Language-Image Pre-training)
论文：https://arxiv.org/pdf/2103.00020
代码：https://github.com/openai/CLIP
核心思想
将对比学习扩展到多模态领域。使用海量的（图像，文本）对进行训练。

正样本：匹配的（图像，文本）对。
负样本：不匹配的（图像，文本）对。

架构
graph LR    subgraph CLIP架构    Img[图像] --&gt; ImgEnc[图像编码器]    Txt[文本] --&gt; TxtEnc[文本编码器]    ImgEnc --&gt; ImgFeat[图像特征]    TxtEnc --&gt; TxtFeat[文本特征]    ImgFeat &lt;--&gt; Loss[对比损失]    TxtFeat &lt;--&gt; Loss    end
影响
CLIP 的出现标志着对比学习从单纯的视觉表示学习走向了通用的多模态理解，为后来的 DALL-E、Stable Diffusion 等生成模型奠定了基础。
技术细节与实践指南
数据增强策略详解
核心增强技术：


随机裁剪（RandomResizedCrop）

最重要的增强，强制模型学习局部-整体关系
参数：scale=(0.08, 1.0)，ratio=(0.75, 1.33)
作用：模拟不同视角和距离



颜色抖动（ColorJitter）

调整亮度、对比度、饱和度、色相
参数：brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1
作用：学习颜色不变的特征



高斯模糊（GaussianBlur）

SimCLR发现这是关键增强之一
参数：kernel_size=图像宽度的1/10，sigma=[0.1, 2.0]
作用：强制关注高层语义而非纹理细节



随机灰度化（RandomGrayscale）

概率：通常设为0.2
作用：学习不依赖颜色的形状特征



增强组合的重要性：
# SimCLR推荐的增强pipelinetransforms = [    RandomResizedCrop(224, scale=(0.08, 1.0)),    RandomHorizontalFlip(p=0.5),    RandomApply([ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),    RandomGrayscale(p=0.2),    RandomApply([GaussianBlur(kernel_size=23)], p=0.5),    ToTensor(),    Normalize(mean, std)]
投影头设计原理
为什么需要投影头？


信息瓶颈作用

编码器输出包含丰富信息（2048维）
投影到低维（128维）强制提取关键信息
防止模型记住无关细节



任务分离

编码器学习通用表示（用于下游任务）
投影头学习对比特定表示（训练后丢弃）
保护编码器特征不被对比任务&quot;污染&quot;



实验发现

无投影头：下游任务性能下降约10%
线性投影头：有改善但不够
非线性投影头（2层MLP）：最佳性能



投影头架构：
# 典型的投影头设计projection_head = nn.Sequential(    nn.Linear(2048, 2048),  # 隐藏层    nn.ReLU(),    nn.Linear(2048, 128)    # 输出层)
训练技巧


学习率：

大batch size时使用大学习率（如0.3）
使用LARS优化器（大batch）或AdamW



学习率调度：

Cosine annealing
Warmup（10%的训练步数）



Batch Size：

SimCLR：≥ 4096
MoCo：256-1024即可
SupCon：根据类别数调整



训练时长：

通常需要较长的训练（100-1000 epochs）
使用更多数据可以缩短训练时间



特征归一化
为什么需要归一化？

控制特征尺度，避免某些维度 dominate
使余弦相似度计算有意义
提高训练稳定性

归一化方式：
z = F.normalize(features, p=2, dim=1)  # L2归一化
代码实现
InfoNCE损失实现
import torchimport torch.nn as nnimport torch.nn.functional as Fclass InfoNCELoss(nn.Module):    &quot;&quot;&quot;InfoNCE损失函数&quot;&quot;&quot;        def __init__(self, temperature=0.07):        super().__init__()        self.temperature = temperature        def forward(self, z1, z2):        &quot;&quot;&quot;        Args:            z1, z2: [batch_size, feature_dim] 归一化后的特征        Returns:            loss: scalar        &quot;&quot;&quot;        batch_size = z1.size(0)                # 合并所有特征        z = torch.cat([z1, z2], dim=0)  # [2*batch_size, feature_dim]                # 计算相似度矩阵        sim_matrix = torch.mm(z, z.t()) / self.temperature  # [2*batch_size, 2*batch_size]                # 构建正样本mask        labels = torch.arange(batch_size, device=z.device)        labels = torch.cat([labels + batch_size, labels], dim=0)  # [2*batch_size]        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()                # 移除自身相似度        logits_mask = torch.scatter(            torch.ones_like(mask),            1,            torch.arange(2 * batch_size, device=z.device).view(-1, 1),            0        )        mask = mask * logits_mask                # 计算exp        exp_logits = torch.exp(sim_matrix) * logits_mask                # 计算log_prob        log_prob = sim_matrix - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)                # 平均正样本的log_prob        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)                # 损失（取负）        loss = -mean_log_prob_pos.mean()                return loss
完整的对比学习训练示例
import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoaderclass ContrastiveModel(nn.Module):    def __init__(self, encoder, projection_dim=128):        super().__init__()        self.encoder = encoder        # 移除分类头        if hasattr(encoder, &#x27;fc&#x27;):            self.encoder.fc = nn.Identity()                # 投影头        encoder_dim = 2048  # ResNet-50的输出维度        self.projector = nn.Sequential(            nn.Linear(encoder_dim, 2048),            nn.ReLU(),            nn.Linear(2048, projection_dim)        )        def forward(self, x):        features = self.encoder(x)        projections = self.projector(features)        return F.normalize(projections, dim=1)def train_contrastive(model, dataloader, num_epochs=100):    criterion = InfoNCELoss(temperature=0.07)    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)        model.train()    for epoch in range(num_epochs):        for batch_idx, (images, _) in enumerate(dataloader):            # 数据增强（假设已经在dataloader中完成）            # images: [batch_size, 2, C, H, W] (两个增强视图)            batch_size = images.size(0)            images = images.view(batch_size * 2, *images.shape[2:])                        # 前向传播            projections = model(images)  # [2*batch_size, projection_dim]            z1, z2 = projections.chunk(2, dim=0)                        # 计算损失            loss = criterion(z1, z2)                        # 反向传播            optimizer.zero_grad()            loss.backward()            optimizer.step()                        if batch_idx % 100 == 0:                print(f&#x27;Epoch &#123;epoch&#125;, Batch &#123;batch_idx&#125;, Loss: &#123;loss.item():.4f&#125;&#x27;)                scheduler.step()
应用场景
1. 图像分类

预训练：在ImageNet等大规模数据集上预训练
微调：在下游分类任务上微调编码器
性能：可以达到或超越监督预训练的性能

2. 目标检测

Backbone预训练：使用对比学习预训练检测器的backbone
迁移学习：将预训练特征迁移到检测任务
性能提升：显著提升检测精度，特别是少样本场景

3. 语义分割

特征提取器预训练：预训练分割网络的特征提取部分
少样本分割：在少样本分割任务上表现优异

4. 图像检索

特征学习：学习到适合检索的判别性特征
相似度计算：直接使用学习到的特征进行检索

5. 长尾学习

SupCon优势：监督对比学习在长尾分布上表现优异
类间分离：显式推远异类样本，提高尾部类别性能

对比学习 vs 其他方法
vs. 监督学习



特性
监督学习
对比学习




数据需求
需要大量标注
无需标注


表示质量
任务相关
通用性强


鲁棒性
一般
更强


计算成本
较低
较高（大batch）



vs. 生成式方法（VAE、GAN）



特性
生成式方法
对比学习




目标
重建/生成
表示学习


训练稳定性
不稳定
较稳定


表示质量
可能包含无关信息
更聚焦判别性


计算效率
较低
较高



vs. 自编码器



特性
自编码器
对比学习




目标
重建输入
区分样本


表示性质
可能包含冗余
更紧凑


下游任务
需要额外设计
直接可用



扩展领域：NLP中的对比学习
对比学习不仅在CV领域大放异彩，在NLP领域也同样重要。
SimCSE (Simple Contrastive Learning of Sentence Embeddings)
论文：https://arxiv.org/pdf/2104.08821
核心思想：

无监督 SimCSE：利用 Dropout 作为数据增强。将同一个句子输入预训练模型（如BERT）两次，由于 Dropout 的存在，得到两个略有不同的 embedding，作为正样本对。
有监督 SimCSE：利用 NLI 数据集中的（蕴含，前提）作为正样本，（矛盾，前提）作为负样本。

影响：SimCSE 极大地提升了句向量的质量，成为 NLP 句向量表示的标准基线。
前沿探讨：对比学习 vs 掩码图像建模 (MIM)
随着 MAE (Masked Autoencoders) 和 BEiT 的提出，视觉预训练领域出现了新的范式竞争。
掩码图像建模 (MIM)

代表作：MAE, BEiT, SimMIM
核心思想：类似 BERT，遮挡图像的一部分 patch，让模型重建被遮挡的像素或特征。
优势：

训练效率高（只需处理可见 patch）。
学习到的特征包含更多细节信息，利于检测和分割任务。
扩展性极强（Scaling Law）。



对比学习 (CL) vs MIM



特性
对比学习 (CL)
掩码图像建模 (MIM)




核心目标
区分样本 (全局语义)
重建细节 (局部关系)


数据增强
极其依赖 (强增强)
不太依赖 (仅需 Mask)


特征性质
线性可分性好，适合分类
细节丰富，适合定位/分割


训练效率
较低 (需处理全图)
较高 (仅处理部分)


当前趋势
多模态对齐 (CLIP)
视觉基础模型 (ViT Pretraining)



结论：两者并非对立，正在趋于融合（如 IBOT, EVA 等工作尝试结合两者的优势）。
当前挑战与解决方案
主要技术挑战
1. 大批次训练的硬件需求
问题：

SimCLR需要4096-8192的batch size才能达到最佳性能
需要多GPU训练，单GPU难以实现

解决方案：

MoCo方法：使用队列存储负样本，减少GPU内存需求
梯度累积：多次前向传播累积梯度，模拟大batch
混合精度训练：使用FP16减少内存使用

2. 负样本的质量问题
问题：

随机采样的负样本可能包含语义相似的样本（假负样本）
简单负样本提供的学习信号有限

解决方案：

困难负样本挖掘：选择相似度较高的负样本
去偏采样：使用先验知识避免假负样本
自适应温度：动态调整温度参数关注困难样本

3. 数据增强的设计
问题：

不同任务需要不同的增强策略
过强增强可能破坏语义信息

解决方案：

AutoAugment：自动搜索最优增强策略
任务特定增强：根据下游任务设计增强
增强强度调度：训练过程中逐渐增强强度

常见问题与调试
1. 模型崩溃（所有样本映射到同一点）
症状：损失快速下降到0，所有特征相同
解决：

检查是否有stop-gradient操作
确保使用了负样本
添加正则化项

2. 性能不提升
可能原因：

Batch size太小（&lt; 256）
温度参数设置不当
数据增强太弱或太强

调试步骤：

可视化增强后的图像，确保保留语义
监控正负样本相似度分布
尝试不同的温度参数（0.05-0.5）

3. 下游任务性能差
原因分析：

预训练与下游任务domain gap
使用了投影头的特征而非编码器特征
预训练不充分

改进方法：

在目标domain数据上预训练
使用编码器特征进行下游任务
增加预训练epochs

总结
对比学习通过&quot;拉近正样本、推远负样本&quot;的简单思想，在无需大量标注数据的情况下学习到强大的视觉表示。从SimCLR、MoCo到SupCon，对比学习的方法不断演进，性能不断提升。
关键要点：

数据增强是关键：强数据增强是对比学习成功的重要因素
负样本数量很重要：需要足够的负样本才能学到好的表示
温度参数需要调优：$\tau$ 对性能有重要影响
投影头很重要：在编码器和损失之间加入投影头能提升性能
可以结合监督信号：SupCon证明了标签信息可以进一步提升性能

对比学习的成功证明了无监督表示学习的巨大潜力，为未来的研究指明了方向。
参考文献


Chen, T., et al. (2020). A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020. https://arxiv.org/pdf/2002.05709


He, K., et al. (2020). Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020. https://arxiv.org/pdf/1911.05722


Khosla, P., et al. (2020). Supervised Contrastive Learning. NeurIPS 2020. https://arxiv.org/pdf/2004.11362


Grill, J. B., et al. (2020). Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. NeurIPS 2020. https://arxiv.org/pdf/2006.07733


Chen, X., &amp; He, K. (2021). Exploring Simple Siamese Representation Learning. CVPR 2021. https://arxiv.org/pdf/2011.10566


Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. ICML 2021. https://arxiv.org/pdf/2103.00020


思考题

为什么对比学习需要大量的负样本？负样本数量如何影响学习效果？
温度参数 $\tau$ 的物理意义是什么？如何根据任务选择合适的 $\tau$？
数据增强在对比学习中的作用是什么？为什么某些增强（如crop）比其他增强更重要？
MoCo的动量编码器和队列机制是如何解决SimCLR的大batch size问题的？
监督对比学习（SupCon）相比无监督对比学习（SimCLR/MoCo）的优势和劣势是什么？
为什么BYOL和SimSiam可以在没有负样本的情况下工作？停止梯度机制的作用是什么？

思考题答案
1. 为什么对比学习需要大量的负样本？负样本数量如何影响学习效果？
为什么需要大量负样本？

提供对比信号：负样本提供了&quot;什么不应该相似&quot;的信息，帮助模型学习区分性
防止崩溃：足够的负样本防止模型学习到平凡解（所有特征相同）
提高表示质量：更多负样本意味着更丰富的对比信号，学习到的表示更具判别性

负样本数量的影响：

太少（&lt; 64）：对比信号不足，模型难以学习到好的表示
适中（256-4096）：性能随负样本数量增加而提升
太多（&gt; 8192）：收益递减，计算成本显著增加

实验发现：

SimCLR：batch size从256增加到8192，性能持续提升
MoCo：队列大小从128增加到65536，性能提升但收益递减

2. 温度参数 $\tau$ 的物理意义是什么？如何根据任务选择合适的 $\tau$？
物理意义：
温度参数 $\tau$ 控制softmax分布的尖锐程度（entropy）：

$\tau \to 0$：分布接近one-hot，模型只关注最相似的样本
$\tau \to \infty$：分布接近均匀，模型对所有样本的关注相等

数学上：$\tau$ 是softmax中的缩放因子，影响梯度的尺度
选择策略：

起始值：0.07（SimCLR、MoCo等常用）
任务特性：

细粒度分类（类别相似）→ 较小的$\tau$（0.05）
粗粒度分类（类别差异大）→ 较大的$\tau$（0.1-0.15）


观察训练：

损失下降快但验证性能差 → 降低$\tau$
训练不稳定 → 提高$\tau$


网格搜索：在[0.05, 0.07, 0.1, 0.15, 0.2]范围内搜索

3. 数据增强在对比学习中的作用是什么？为什么某些增强（如crop）比其他增强更重要？
数据增强的作用：

构造正样本对：通过增强同一图像得到不同的视图，作为正样本
提高鲁棒性：学习对增强不变的表示，提高泛化能力
增加数据多样性：在有限数据上模拟更多场景

为什么crop最重要？

语义保持：crop保留了图像的主要语义内容
视角变化：模拟了不同的观察视角，是自然的变化
空间不变性：帮助模型学习空间不变的特征

增强的重要性排序（SimCLR实验）：

RandomResizedCrop（最重要）
RandomHorizontalFlip
ColorJitter
RandomGrayscale
GaussianBlur

组合效应：多个增强的组合效果 &gt; 单个增强的简单叠加
4. MoCo的动量编码器和队列机制是如何解决SimCLR的大batch size问题的？
SimCLR的问题：

需要大batch size（≥ 4096）提供足够负样本
对GPU内存和计算资源要求高

MoCo的解决方案：


队列机制：

维护一个FIFO队列存储历史样本的特征
队列大小可以很大（如65536），远超batch size
每次用新batch的特征替换最旧的队列元素



动量编码器：

Key编码器通过动量更新：$\theta_k \leftarrow m \theta_k + (1-m) \theta_q$
保证队列中Key的一致性（不会因为编码器快速变化而失效）
动量系数通常为0.999



优势：

不需要大batch size（256-1024即可）
队列提供大量且一致的负样本
内存效率高（只存储特征，不存储图像）

5. 监督对比学习（SupCon）相比无监督对比学习（SimCLR/MoCo）的优势和劣势是什么？
优势：

正样本更明确：同类样本作为正样本，比增强视图更可靠
性能更优：在分类任务上通常超越无监督方法
鲁棒性更强：对对抗样本、噪声等更鲁棒
长尾学习：在类别不平衡数据上表现优异
少样本学习：学习到的表示泛化能力更强

劣势：

需要标注数据：无法利用无标注数据
计算成本高：需要计算所有样本对的相似度（$O(N^2)$）
任务相关：学习到的表示可能更偏向特定任务
类别依赖：需要知道类别信息，限制了应用场景

适用场景：

SupCon：有标注数据，关注分类性能和鲁棒性
无监督：无标注数据，需要通用表示

6. 为什么BYOL和SimSiam可以在没有负样本的情况下工作？停止梯度机制的作用是什么？
为什么可以工作？

预测任务替代对比：通过预测一个视图的特征来学习表示
对称损失：同时优化两个方向的预测
停止梯度：防止模型学习到平凡解

停止梯度机制：
在BYOL/SimSiam中，一个分支的梯度被停止：
# SimSiam示例z1 = encoder(x1)z2 = encoder(x2)p1 = predictor(z1)p2 = predictor(z2)# 停止z2的梯度loss = 0.5 * (d(p1, z2.detach()) + d(p2, z1.detach()))
作用：

防止崩溃：如果没有停止梯度，两个分支可能学习到相同的表示（平凡解）
非对称性：创造非对称的学习信号，使模型必须学习有意义的表示
稳定训练：避免两个分支相互&quot;追逐&quot;，训练更稳定

理论理解：

停止梯度创造了一个&quot;教师-学生&quot;的关系
一个分支作为&quot;教师&quot;提供目标，另一个作为&quot;学生&quot;学习
这种非对称性防止了表示空间的坍塌

深度思考与实践经验

对比信号的本质：无论是SimCLR还是MoCo，本质上都在重建一个“相似样本的局部图结构”。从这个角度看，数据增强、负样本采样、温度参数都是在调节局部图的形状。实践中可以通过构建邻接矩阵或最近邻图来检查模型学习到的结构是否符合预期。
大batch与动量编码器的权衡：SimCLR依赖大batch，MoCo依赖动量编码器与队列。前者更适合TPU或多节点GPU环境，后者对资源要求更低但需要额外调节动量系数。在工业部署时，可根据硬件与延迟要求选择不同方案。
Projection Head的价值：许多工程实践表明，只要下游任务不是k-NN检索，就应当保留投影头并在微调时丢弃。投影头相当于一个“噪声抑制器”，把与对比任务高度相关但与下游任务无关的因素隔离开。
数据增强的语义边界：对比学习依赖“语义不变”的增强。如果增强破坏了语义一致性（例如在细粒度识别中使用过强的随机裁剪），模型可能误学到错误关联。设计增强时应结合业务常识：什么变化对用户看来仍是同一对象？
InfoNCE作为下界：InfoNCE提供了互信息的可计算下界。下界松紧程度受负样本数量、温度、特征容量、优化状态影响。若训练良好却仍觉得效果不足，可以尝试提高下界（更多负样本、更低温度）或改用其他目标（如Barlow Twins、VICReg）。

开放问题

困难负样本自动发现：目前多靠随机采样，能否结合难例挖掘或生成模型来构建更具区分度的负样本？
跨模态对比的统一框架：CLIP等方法表明文本-图像对比极具潜力，是否存在统一的对比学习范式涵盖视觉、语言、音频？
长序列与视频：视频对比学习如何解决时间维度的冗余与语义错配问题？现有工作（VideoMAE、TimeSformer）仍在探索。
理论上界与可解释性：对比学习能否提供误差上界或泛化保证？如何解释实例判别自动聚类的现象？
增量与联邦场景：当数据分布随时间变化或分散在不同客户端时，如何稳定地维护对比学习的记忆（队列/Memory Bank）？

]]></content>
      <categories>
        <category>论文精读</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>自监督学习</tag>
        <tag>对比学习</tag>
        <tag>深度学习</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2026/02/%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84%20+%20%E5%88%86%E5%B8%83%E5%BB%BA%E6%A8%A1%20+%20%E5%8E%9F%E5%9E%8B%E6%BC%94%E5%8C%96/</url>
    <content><![CDATA[原型=动态演化的语义锚点
原型的关键作用
1. 类级表示
经过层级学习、去噪后的特征，更纯净、更有代表性
2. 对比学习的监督信号
不只是&quot;拉近同类&quot;，而是&quot;拉向类中心&quot;

语义信息：
原型包含了这个类的核心特征
例如：猫原型 = “有毛” + “四条腿” + “小体型”
辅助分类：
新样本可以和各类原型对比
与哪个原型最接近，就属于哪一类
知识迁移：
原型可以在不同任务间共享

]]></content>
  </entry>
  <entry>
    <title>开学三个月小记</title>
    <url>/2025/12/%E5%BC%80%E5%AD%A6%E4%B8%89%E4%B8%AA%E6%9C%88%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[时光飞逝，转眼间开学已经三个月了。如果一直这么下去就又荒废了三年。
第一没有输入，感觉三个月没有什么知识输入，输出也没有，因为基础太差了，感觉陷入了一个恶性循环，没有时间学习，好吧，其实是不知道学啥，也不想学，静不下心来去学习，需要补的基础的东西太多了，不知道从哪里开始。
第二过度依赖AI，用AI写的博客基本上也没认真看，AI虽然好用，但是现在完全没有发挥出它的作用帮我在能力上有所提升。
第三生活比较混乱，主要是没有平衡好每天该做什么，又回到高中大学那种天天想干啥就干啥的状态了，课也基本上都不用去上，其实从高中开始就经常逃课，晚自习不上，大学疫情更是如此了，自由习惯了，很难再改变。每天过的倒是挺自由，也快一个月没开过组会了，也不知道要汇报啥。倒是偶尔健身，健身一个多月了，虽然也是三天打鱼两天晒网，但是确实是有效果的。
上边三点的解决方法是现在急需制定一个切实可行的大体方案，每天按照计划去做，但是经常会忘记去看每日计划，效果一般，可以每天记录反思一下做了什么，这样应该可以，单独创建一个记事本，正好今天下载了一个新的笔记管理软件，尝试每天记录一下。其实就和我当时创建网站的目的一样，当时是为了监督我复试学习，每天记录一下做了什么，以防像第一年复试笔试没准备完，面试和机试一点没准备，以初试第三被刷掉。对于AI的话，如果用AI进行学习的话，还是很不错的，不需要繁琐的信息检索，可以达到及时反馈修改错误直觉的作用。


“生活不是等待暴风雨过去，而是学会在雨中跳舞。”

]]></content>
      <categories>
        <category>生活记录</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>随笔</tag>
        <tag>研究生生活</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕 Vision Transformer</title>
    <url>/2025/06/%E6%89%8B%E6%92%95%20Vision%20Transformer/</url>
    <content><![CDATA[手撕 Vision Transformer

​	之前接触过挺多pytorch框架写的代码的，但是一直没有学过，觉得没啥可学的，上个月看了b站上的一个博主的速成课，感觉确实没啥学的，就是几个函数而已，不过通一下也是很有收获的。我现在准备练习一下代码能力，所以尝试手写一下Vit代码。这也是第一次写除了算法题之外的代码，之前复试结束后联系了一个老师问我写过代码吗，我说没有，也是很尴尬了，最后老师也没要我，嫌我基础太差了，希望能在开学之前把代码能力练好，并且把基础的一些东西弄明白。
​	根据Gemini给我生成了个提示，手写了一遍，不过中间有好几处错误，目前已经更正，不是很熟练，还需要再练习一下。
import torchimport torch.nn as nn# 1. 图像分块与嵌入 (Patch Embedding)# 模块： PatchEmbedding(nn.Module)class PatchEmbedding(nn.Module):    def __init__(self,in_channels,patch_size,embed_dim,image_size):        super().__init__()        self.patch_num=(image_size//patch_size)**2        self.emb=nn.Conv2d(in_channels,embed_dim,kernel_size=patch_size,stride=patch_size)        self.pos_emb=nn.Parameter(torch.randn(1,self.patch_num+1,embed_dim))        self.cls_emb=nn.Parameter(torch.randn(1,1,embed_dim))    def forward(self,x):        x=self.emb(x)        b,d,h,w=x.shape        x=x.permute(0,2,3,1).reshape(b,h*w,d)        cls_emb=self.cls_emb.expand(b,-1,-1)        x=torch.cat((cls_emb,x),dim=1)        out=x+self.pos_emb[:,:1+h*w]        return out# 2. 多头自注意力机制 (Multi-Head Self-Attention, MSA)# 模块： MultiHeadSelfAttention(nn.Module)class MultiHeadSelfAttention(nn.Module):    def __init__(self,embed_dim,num_heads,dropout=0.0):        super().__init__()        self.embed_dim=embed_dim        self.num_heads=num_heads        self.head_dim=embed_dim//num_heads        self.proj_q=nn.Linear(embed_dim,embed_dim)        self.proj_k=nn.Linear(embed_dim,embed_dim)        self.proj_v=nn.Linear(embed_dim,embed_dim)        self.out_proj=nn.Linear(embed_dim,embed_dim)        self.drop=nn.Dropout(dropout)        def forward(self,x):        b,l,_=x.shape        q = self.proj_q(x).reshape(b, l, self.num_heads, self.head_dim).transpose(1, 2)        k = self.proj_k(x).reshape(b, l, self.num_heads, self.head_dim).transpose(1, 2)        v = self.proj_v(x).reshape(b, l, self.num_heads, self.head_dim).transpose(1, 2)        atten_score=torch.matmul(q,k.transpose(-2,-1))/self.head_dim**0.5        atten_weight=torch.nn.functional.softmax(atten_score,dim=-1)        atten_weight = self.drop(atten_weight)        atten = torch.matmul(atten_weight, v).transpose(1, 2).reshape(b, l, self.embed_dim)        out=self.out_proj(atten)        out=self.drop(out)        return out# 3. 多层感知机块 (MLP Block)# 模块： MlpBlock(nn.Module)class MlpBlock(nn.Module):    def __init__(self,embed_dim,mlp_dim,out_dim,dropout=0.0):        super().__init__()        self.fc1=nn.Linear(embed_dim,mlp_dim)        self.fc2=nn.Linear(mlp_dim,out_dim)        self.act=nn.GELU()        self.drop=nn.Dropout(dropout)        def forward(self,x):        x=self.fc1(x)        x=self.act(x)        x=self.drop(x)        x=self.fc2(x)        out=self.drop(x)        return out# 4. Transformer 编码器块 (Transformer Encoder Block)# 模块： TransformerEncoderBlock(nn.Module)class TransformerEncoderBlock(nn.Module):    def __init__(self,embed_dim,num_heads,mlp_dim,dropout=0.0):        super().__init__()        self.norm1=nn.LayerNorm(embed_dim)        self.norm2=nn.LayerNorm(embed_dim)        self.mlp = MlpBlock(embed_dim, mlp_dim, embed_dim, dropout)         self.msa = MultiHeadSelfAttention(embed_dim, num_heads, dropout)        def forward(self,x):        norm=self.norm1(x)        msa=self.msa(norm)        res=msa+x        norm=self.norm2(res)        mlp=self.mlp(norm)        out=mlp+res        return out# 5. Transformer 编码器 (Transformer Encoder)# 模块： TransformerEncoder(nn.Module)class TransformerEncoder(nn.Module):    def __init__(self,embed_dim,num_heads,mlp_dim,depth,dropout=0.0):        super().__init__()        self.layers=nn.ModuleList([])        for _ in range(depth):             layer = TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout)            self.layers.append(layer)        self.norm=nn.LayerNorm(embed_dim)    def forward(self,x):        for layer in self.layers:             x = layer(x)         out=self.norm(x)        return out# 6. 完整的 Vision Transformer (ViT) 模型# 模块： VisionTransformer(nn.Module)class VisionTransformer(nn.Module):    def __init__(self,image_size,in_channels,num_classes,patch_size,embed_dim,depth,num_heads,mlp_dim,dropout=0.0):        super().__init__()        self.emb=PatchEmbedding(in_channels,patch_size,embed_dim,image_size)        self.trans=TransformerEncoder(embed_dim,num_heads,mlp_dim,depth,dropout)        self.lin=nn.Linear(embed_dim,num_classes)        def forward(self,x):        emb=self.emb(x)        trans_output=self.trans(emb)        cls_token_output = trans_output[:, 0]        out=self.lin(cls_token_output)        return out
]]></content>
      <categories>
        <category>手撕代码</category>
        <category>图像分类</category>
      </categories>
      <tags>
        <tag>Vit</tag>
        <tag>手撕代码</tag>
      </tags>
  </entry>
  <entry>
    <title>扩散模型</title>
    <url>/2025/12/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[扩散模型是一种特殊的VAE，其灵感来自于热力学：一个分布可以通过不断地添加噪声变成另一个分布。放到图像生成任务里，就是来自训练集的图像可以通过不断添加噪声变成符合标准正态分布的图像。
图像生成网络会学习如何把一个向量映射成一幅图像。设计网络架构时，最重要的是设计学习目标，让网络生成的图像和给定数据集里的图像相似。VAE的做法是使用两个网络，一个学习把图像编码成向量，另一个学习把向量解码回图像，它们的目标是让复原图像和原图像尽可能相似。学习完毕后，解码器就是图像生成网络。扩散模型是一种更具体的VAE。它把编码过程固定为加噪声，并让解码器学习怎么样消除之前添加的每一步噪声。
]]></content>
  </entry>
  <entry>
    <title>基础知识--排序算法</title>
    <url>/2025/03/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[排序算法详解
排序算法是计算机科学中最基础也是最重要的算法之一。本文将详细介绍几种常见的排序算法，包括它们的实现原理、时间复杂度和适用场景。本文所有代码示例使用 C++ 实现，需要包含以下头文件：
#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;
1. 冒泡排序 (Bubble Sort)
冒泡排序是最简单的排序算法之一，它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。
实现原理

时间复杂度：O(n²)
空间复杂度：O(1)
稳定性：稳定

实现代码
void bubbleSort(vector&lt;int&gt;&amp; arr) &#123;    int n = arr.size();    bool swapped;    for(int i = 0; i &lt; n-1; i++) &#123;        swapped = false;        // 每一轮比较        for(int j = 0; j &lt; n-i-1; j++) &#123;            // 相邻元素比较并交换            if(arr[j] &gt; arr[j+1]) &#123;                swap(arr[j], arr[j+1]);                swapped = true;            &#125;        &#125;        // 如果没有发生交换，说明数组已经有序        if(!swapped) break;    &#125;&#125;
2. 选择排序 (Selection Sort)
选择排序的工作原理是每次从待排序的数据中选出最小（或最大）的元素，存放在序列的起始位置。
实现原理

时间复杂度：O(n²)
空间复杂度：O(1)
稳定性：不稳定

实现代码
void selectionSort(vector&lt;int&gt;&amp; arr) &#123;    int n = arr.size();    for(int i = 0; i &lt; n-1; i++) &#123;        int min_idx = i;        // 在未排序部分找最小值        for(int j = i+1; j &lt; n; j++) &#123;            if(arr[j] &lt; arr[min_idx]) &#123;                min_idx = j;            &#125;        &#125;        // 将找到的最小值放到已排序序列的末尾        if(min_idx != i) &#123;            swap(arr[i], arr[min_idx]);        &#125;    &#125;&#125;
3. 插入排序 (Insertion Sort)
插入排序的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。
实现原理

时间复杂度：O(n²)
空间复杂度：O(1)
稳定性：稳定

实现代码
void insertionSort(vector&lt;int&gt;&amp; arr) &#123;    int n = arr.size();    for(int i = 1; i &lt; n; i++) &#123;        int key = arr[i];        int j = i - 1;        // 将大于key的元素都向后移动        while(j &gt;= 0 &amp;&amp; arr[j] &gt; key) &#123;            arr[j+1] = arr[j];            j--;        &#125;        arr[j+1] = key;    &#125;&#125;
4. 快速排序 (Quick Sort)
快速排序是一种分治算法，它通过选择一个&quot;基准&quot;元素，将数组分成两个子数组，小于基准的元素放在左边，大于基准的元素放在右边。
实现原理

时间复杂度：平均 O(nlogn)，最坏 O(n²)
空间复杂度：O(logn)
稳定性：不稳定

实现代码
int partition(vector&lt;int&gt;&amp; arr, int low, int high) &#123;    int pivot = arr[high];    // 选择最右边的元素作为基准    int i = low - 1;          // 小于基准的元素的最后一个位置    for(int j = low; j &lt; high; j++) &#123;        // 如果当前元素小于基准，则将其交换到前面        if(arr[j] &lt; pivot) &#123;            i++;            swap(arr[i], arr[j]);        &#125;    &#125;    swap(arr[i + 1], arr[high]);    return i + 1;&#125;void quickSort(vector&lt;int&gt;&amp; arr, int low, int high) &#123;    if(low &lt; high) &#123;        // 获取分区点        int pi = partition(arr, low, high);        // 递归排序左右两部分        quickSort(arr, low, pi - 1);        quickSort(arr, pi + 1, high);    &#125;&#125;// 封装函数，方便调用void quickSort(vector&lt;int&gt;&amp; arr) &#123;    quickSort(arr, 0, arr.size() - 1);&#125;
5. 归并排序 (Merge Sort)
归并排序是一种分治算法，它将数组分成两半，递归地排序两半，然后将它们合并起来。
实现原理

时间复杂度：O(nlogn)
空间复杂度：O(n)
稳定性：稳定

实现代码
void merge(vector&lt;int&gt;&amp; arr, int left, int mid, int right) &#123;    vector&lt;int&gt; temp(right - left + 1);    int i = left, j = mid + 1, k = 0;        // 合并两个有序数组    while(i &lt;= mid &amp;&amp; j &lt;= right) &#123;        if(arr[i] &lt;= arr[j]) &#123;            temp[k++] = arr[i++];        &#125; else &#123;            temp[k++] = arr[j++];        &#125;    &#125;        // 复制剩余元素    while(i &lt;= mid) temp[k++] = arr[i++];    while(j &lt;= right) temp[k++] = arr[j++];        // 将临时数组复制回原数组    for(i = 0; i &lt; k; i++) &#123;        arr[left + i] = temp[i];    &#125;&#125;void mergeSort(vector&lt;int&gt;&amp; arr, int left, int right) &#123;    if(left &lt; right) &#123;        int mid = left + (right - left) / 2;        mergeSort(arr, left, mid);        mergeSort(arr, mid + 1, right);        merge(arr, left, mid, right);    &#125;&#125;// 封装函数，方便调用void mergeSort(vector&lt;int&gt;&amp; arr) &#123;    mergeSort(arr, 0, arr.size() - 1);&#125;
6. 堆排序 (Heap Sort)
堆排序是利用堆这种数据结构所设计的一种排序算法。它通过构建最大堆或最小堆来进行排序。
实现原理

时间复杂度：O(nlogn)
空间复杂度：O(1)
稳定性：不稳定

实现代码
void heapify(vector&lt;int&gt;&amp; arr, int n, int i) &#123;    int largest = i;    int left = 2 * i + 1;    int right = 2 * i + 2;    // 如果左子节点大于根节点    if(left &lt; n &amp;&amp; arr[left] &gt; arr[largest]) &#123;        largest = left;    &#125;        // 如果右子节点大于最大值    if(right &lt; n &amp;&amp; arr[right] &gt; arr[largest]) &#123;        largest = right;    &#125;        // 如果最大值不是根节点    if(largest != i) &#123;        swap(arr[i], arr[largest]);        heapify(arr, n, largest);    &#125;&#125;void heapSort(vector&lt;int&gt;&amp; arr) &#123;    int n = arr.size();        // 构建最大堆    for(int i = n/2-1; i &gt;= 0; i--) &#123;        heapify(arr, n, i);    &#125;        // 一个个从堆顶取出元素    for(int i = n-1; i &gt; 0; i--) &#123;        swap(arr[0], arr[i]);        heapify(arr, i, 0);    &#125;&#125;
排序算法的比较



排序算法
平均时间复杂度
最坏时间复杂度
空间复杂度
稳定性




冒泡排序
O(n²)
O(n²)
O(1)
稳定


选择排序
O(n²)
O(n²)
O(1)
不稳定


插入排序
O(n²)
O(n²)
O(1)
稳定


快速排序
O(nlogn)
O(n²)
O(logn)
不稳定


归并排序
O(nlogn)
O(nlogn)
O(n)
稳定


堆排序
O(nlogn)
O(nlogn)
O(1)
不稳定



如何选择排序算法？

数据量小（n &lt; 50）：插入排序
数据量大：

要求稳定：归并排序
不要求稳定：快速排序


内存空间有限：堆排序
数据基本有序：插入排序
数据量特别大且有重复：计数排序/基数排序

总结
每种排序算法都有其特点和适用场景。在实际应用中，我们需要根据具体情况（数据规模、稳定性要求、空间限制等）来选择合适的排序算法。大多数编程语言的标准库中的排序实现都是一种改进的快速排序，它能够很好地适应各种情况。
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>排序</tag>
        <tag>冒泡排序</tag>
        <tag>选择排序</tag>
        <tag>插入排序</tag>
        <tag>快速排序</tag>
        <tag>归并排序</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习优化器全家桶：从 SGD 到 AdamW 及未来</title>
    <url>/2025/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E5%99%A8/</url>
    <content><![CDATA[在深度学习的训练过程中，优化器 (Optimizer) 扮演着至关重要的角色。它决定了网络参数更新的方式，直接影响模型的收敛速度和最终性能。本文将深入剖析深度学习中常见的优化器，从最基础的 SGD 到目前最流行的 AdamW，以及一些前沿的变体。
1. 梯度下降家族 (Gradient Descent Variants)
1.1 BGD, SGD 与 Mini-batch SGD

BGD (Batch Gradient Descent)：每次迭代使用全部样本计算梯度。

优点：梯度准确，收敛稳定。
缺点：计算量大，内存无法承受，无法在线更新。


SGD (Stochastic Gradient Descent)：每次迭代使用一个样本。

优点：计算快，引入噪声有助于跳出局部最优。
缺点：震荡剧烈，收敛慢，无法利用向量化加速。


Mini-batch SGD：折中方案，每次使用一批样本（如 32, 64）。这是实际中最常用的形式。
$$ w_{t+1} = w_t - \eta \cdot \nabla L(w_t) $$

1.2 Momentum (动量法)
为了抑制 SGD 的震荡（特别是在峡谷地形），引入了物理学中的动量概念。
$$ v_t = \gamma v_{t-1} + \eta \nabla L(w_t) $$
$$ w_{t+1} = w_t - v_t $$

核心：参数更新方向不仅取决于当前梯度，还保留了之前的速度 $v_{t-1}$。
效果：在梯度方向一致的维度加速，在梯度方向改变的维度减速（抑制震荡）。

1.3 NAG (Nesterov Accelerated Gradient)
Momentum 是“盲目”的冲刺，NAG 则是“先看一眼再走”。
$$ v_t = \gamma v_{t-1} + \eta \nabla L(w_t - \gamma v_{t-1}) $$
$$ w_{t+1} = w_t - v_t $$

区别：NAG 先按动量走一步，计算那个位置的梯度，再修正。这使得 NAG 收敛更快，震荡更小。

2. 自适应学习率家族 (Adaptive Learning Rate)
SGD 系列对所有参数使用相同的学习率，这在稀疏数据或特征频率差异大时效果不佳。
2.1 AdaGrad
$$ G_t = G_{t-1} + g_t^2 $$
$$ w_{t+1} = w_t - \frac{\eta}{\sqrt{G_t + \epsilon}} g_t $$

机制：累积历史梯度的平方和 $G_t$ 作为分母。梯度大的参数，学习率衰减快；梯度小的参数，学习率衰减慢。
缺点：$G_t$ 单调递增，导致学习率过早衰减至 0，训练提前停止。

2.2 RMSProp
为了解决 AdaGrad 的问题，Geoff Hinton 提出了 RMSProp（在 Coursera 课程中提出，未发表论文）。
$$ E[g^2]t = \beta E[g^2]{t-1} + (1-\beta) g_t^2 $$
$$ w_{t+1} = w_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t $$

机制：使用指数加权移动平均来计算梯度平方和。它只关注“最近”的梯度，解决了学习率过早消失的问题。适合 RNN 等变长序列任务。

2.3 Adadelta
Adadelta 也是为了解决 AdaGrad 学习率递减问题，它甚至不需要设置全局学习率 $\eta$，而是维护一个更新量的移动平均。
3. 集大成者：Adam 及其变体
3.1 Adam (Adaptive Moment Estimation)
Adam = Momentum + RMSProp。它同时利用了一阶矩（均值）和二阶矩（方差）。

计算梯度 $g_t$
更新一阶矩：$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$
更新二阶矩：$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$
偏差修正：$\hat{m}_t = m_t / (1-\beta_1^t)$, $\hat{v}_t = v_t / (1-\beta_2^t)$
更新参数：$w_{t+1} = w_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$


特点：收敛快，对超参数不敏感，是目前的默认选择。

3.2 AdaMax
Adam 中 $v_t$ 是基于 $L_2$ 范数的，AdaMax 将其推广到 $L_\infty$ 范数，更加稳定。
3.3 Nadam
Nadam = NAG + Adam。将 Nesterov 动量引入 Adam，理论上收敛速度更快。
4. 现代优化器：修正与进化
4.1 AdamW (Decoupled Weight Decay)
这是目前 Transformer (BERT, ViT, GPT) 的标配。

问题：在 Adam 中，L2 正则化通常直接加在梯度上。但在自适应学习率算法中，这样做会导致正则化效果被学习率缩放，变得不均匀。
解决：AdamW 将权重衰减 (Weight Decay) 从梯度更新中剥离，直接作用于权重：
$$ w_{t+1} = w_t - \eta (\dots) - \eta \lambda w_t $$

4.2 RAdam (Rectified Adam)
Adam 在训练初期，由于样本少，二阶矩估计方差大，导致学习率激增，模型发散。RAdam 通过引入“整流器”机制，在初期自动降低学习率（起到 Warmup 的作用），后期恢复正常。
4.3 Lookahead
被称为“优化器的优化器”。它维护两组权重：快权重 (Fast Weights) 和慢权重 (Slow Weights)。快权重由标准优化器（如 Adam）更新 k 次，然后慢权重向快权重方向迈一步。这大大提高了收敛稳定性。
5. 总结与选型指南



优化器
特点
适用场景




SGD + Momentum
收敛慢但稳，泛化性好
CV (ResNet, VGG)，对精度要求极高的任务


Adam
收敛快，自适应
NLP, RL，快速原型开发


AdamW
修正了权重衰减，训练更稳定
Transformer (BERT, ViT)，生成模型


RMSProp
适合非平稳目标
RNN, LSTM



一句话建议：

搞 CV，先试 SGD + Momentum，再试 AdamW。
搞 NLP/Transformer，无脑上 AdamW。
搞 新手入门，用 Adam。

]]></content>
      <categories>
        <category>算法解析</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>优化器</tag>
        <tag>SGD</tag>
        <tag>Adam</tag>
        <tag>算法详解</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习损失函数：从 MSE 到 Focal Loss</title>
    <url>/2025/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[在深度学习中，损失函数 (Loss Function) 是连接模型预测与真实标签的桥梁，它定义了模型的优化目标。选择合适的损失函数往往能起到事半功倍的效果。本文将对深度学习中常见的损失函数进行梳理，从基础的回归/分类到进阶的难例挖掘和度量学习。
1. 回归任务 (Regression)
回归任务的目标是预测连续值。
1.1 MSE (L2 Loss)
均方误差 (Mean Squared Error)：
$$ L = (y - \hat{y})^2 $$

特点：收敛快，但对异常值 (Outliers) 非常敏感（因为误差被平方放大了）。

1.2 MAE (L1 Loss)
平均绝对误差 (Mean Absolute Error)：
$$ L = |y - \hat{y}| $$

特点：对异常值鲁棒，但在 0 点处不可导，梯度恒定可能导致收敛困难。

1.3 Smooth L1 Loss
结合了 L1 和 L2 的优点：

在误差较小时（$|x| &lt; 1$）使用 L2（平滑，可导）。
在误差较大时（$|x| \ge 1$）使用 L1（梯度恒定，防止梯度爆炸）。
应用：Faster R-CNN 的边界框回归。

2. 分类任务 (Classification)
2.1 Cross Entropy (CE)
交叉熵损失是分类任务的标准配置：
$$ CE = - \sum y \log(p) $$

本质：衡量两个概率分布的距离（KL 散度）。
局限：对噪声标签敏感，且容易导致模型过度自信 (Over-confidence)。

2.2 Label Smoothing
为了解决 CE 的过度自信，Label Smoothing 将 One-hot 标签软化：
$$ y_{new} = (1 - \epsilon) y + \epsilon / K $$

作用：防止模型在训练集上过拟合，提升泛化能力。

3. 进阶：解决不平衡与难例
当数据存在严重的类别不平衡或大量简单负样本时，标准 CE 往往失效。
3.1 Focal Loss
最初用于目标检测 (RetinaNet)，旨在解决 One-stage 检测器中极端的正负样本失衡。

公式：$FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)$
核心机制：

$\gamma$ (Focusing Parameter)：降低易分样本（$p_t \approx 1$）的权重，迫使模型关注难分样本。
$\alpha$ (Balancing Parameter)：平衡正负样本比例。


价值：不仅用于检测，在长尾分类任务中也非常有效。

# Focal Loss PyTorch 实现class FocalLoss(nn.Module):    def __init__(self, alpha=1, gamma=2):        super().__init__()        self.alpha = alpha        self.gamma = gamma            def forward(self, inputs, targets):        ce_loss = F.cross_entropy(inputs, targets, reduction=&#x27;none&#x27;)        pt = torch.exp(-ce_loss)        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss        return focal_loss.mean()
4. 进阶：度量学习 (Metric Learning)
目标是学习一个特征空间，使得同类样本距离近，异类样本距离远。
4.1 Triplet Loss
$$ L = \max(d(a, p) - d(a, n) + margin, 0) $$

输入：三元组 (Anchor, Positive, Negative)。
难点：需要复杂的三元组挖掘 (Triplet Mining) 策略，否则训练效率极低。

4.2 InfoNCE (Contrastive Loss)
自监督学习（如 SimCLR, MoCo）的核心。

思想：将 Triplet 扩展到 N 个负样本，转化为一个 N+1 类的分类问题。
优势：利用大量负样本，学习到的特征更具判别性。



总结

回归：首选 Smooth L1 或 MSE。
分类：首选 Cross Entropy，配合 Label Smoothing。
不平衡/难例：必选 Focal Loss。
特征学习：尝试 InfoNCE 或 Triplet Loss。


]]></content>
      <categories>
        <category>算法解析</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>损失函数</tag>
        <tag>基础理论</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习杂谈：残差、MAE与特征维度的本质思考</title>
    <url>/2025/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%B0%88%EF%BC%9A%E6%AE%8B%E5%B7%AE%E3%80%81MAE%E4%B8%8E%E7%89%B9%E5%BE%81%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[最近有一些问题，正好记录下来了一些，用AI探讨了一下这些问题。
1. 残差 (Residual) 的本质：仅仅是保留原始信息吗？
问题： 残差的本质是什么？为什么有用？是因为保留了之前的原始信息的特征吗？那么添加动量 (Momentum) 也是保留之前的原始信息，和残差的本质有什么区别吗？
残差连接 (Skip Connection)
残差网络 (ResNet) 的核心公式是 $y = F(x) + x$。
确实，从直观上看，$+x$ 这一项直接将上一层的原始信息“保留”并传递到了下一层。这使得网络在初始化阶段即使 $F(x)$ 接近于 0，整个网络也近似于一个恒等映射 (Identity Mapping)，梯度可以无损地反向传播。
本质区别：

残差 (ResNet) 解决的是 模型结构 (Model Architecture) 和 梯度流 (Gradient Flow) 的问题。它是在空间/层级维度上，让深层网络更容易训练，避免梯度消失。它让网络“有机会”去学习恒等映射，如果某一层是多余的，网络可以将 $F(x)$ 权重置为 0，自动“跳过”这一层。
动量 (Momentum) 解决的是 优化算法 (Optimization) 的问题。它是在时间/迭代维度上，利用历史梯度的加权平均来平滑优化路径。它保留的是“之前的更新方向”，而不是“之前的特征图”。动量帮助优化器冲出局部极小值，加速收敛。

总结： 残差保留的是特征信息（为了更好训练深层网络），动量保留的是梯度惯性（为了更快收敛）。两者作用的维度完全不同。

2. MAE 中 Mask 的作用：仅仅是加噪声吗？
问题： MAE (Masked Autoencoders) 中遮住图片块的作用是什么？可以用到我的项目中吗？
回答：
MAE 中遮住 75% 甚至更多的图片块，确实可以被视为一种极端的数据增强 (Data Augmentation) 或 去噪 (Denoising) 任务。

本质： 它的核心目的不仅仅是防止过拟合，而是强迫模型去学习图像的 全局语义上下文 (Global Semantic Context)。如果只遮住 10%，模型可以通过插值等局部纹理信息轻松复原；但遮住 75%，模型必须“理解”画面——“这里是一只狗的尾巴，那么左边大概率是狗的身体”，这种推理需要高层的语义理解。
缺点： 正如你所说，训练非常慢，需要大量的计算资源和数据量才能收敛出好的特征。
能否用于你的项目：

能用： 如果你的任务是自监督预训练 (Self-supervised Pre-training)，且有大量无标签数据，MAE 是非常好的特征提取器。
慎用： 如果你的数据集很小，或者直接在监督任务中作为一种 Augmentation 手段（即输入 Mask 后的图直接做分类），效果可能适得其反，因为丢失了太多信息。



启示： 找经典论文中的方法，要看它解决的根本矛盾是什么。MAE 解决的是 Vision Transformer 如何高效利用无标签数据进行预训练的问题。

3. 特征维度：越高越好吗？
问题： 特征维度越高会不会表征能力越强呢？
回答：
理论上，高维空间能容纳更复杂的信息，但这有一个边际效应递减甚至负面效应的问题：

维度灾难 (Curse of Dimensionality)： 随着维度增加，数据在空间中会变得极度稀疏。为了填满高维空间，所需的数据量是指数级增长的。如果数据量不够，高维特征极易导致过拟合。
流形假设 (Manifold Hypothesis)： 真实世界的图像虽然像素维度很高（如 224x224x3），但它们通常分布在一个低维的流形上。我们希望模型学到的是这个“低维流形”的坐标，而不是简单地把维度撑大。
计算冗余： 过高的维度带来巨大的计算和存储开销。

结论： 维度不是越高越好，而是**“足够表达且紧凑”**最好。通常在 ResNet50 中 2048 维已经非常丰富，很多轻量级模型 512 维甚至 128 维也能达到很好的效果。

4. 编码器后的 MLP (Projection Head)：神来之笔？
问题： 在编码器后边加一个单独的 MLP 层为什么会起作用，数学原理是什么？我在我的算法后边加入 Non-linear 试一下。
回答：
这在对比学习（如 SimCLR, BYOL）中非常常见。

现象： 训练时加上 MLP (Projection Head) 计算 Loss，但在下游任务（如分类）时去掉 MLP，直接用 Encoder 的输出，效果反而更好。
原理 (Information Bottleneck)：

对比学习的目标是让增强后的两个视图特征尽可能一致（Invariant）。
但是，数据增强（如裁剪、变色）会引入一些与物体类别无关的变换信息。
MLP 的作用是作为一个“缓冲区”或“过滤器”。它负责去适应这些变换（吸收掉与下游任务无关的增强信息），从而保护 Encoder 学到的特征保持纯粹的语义不变性。
如果直接用 Encoder 输出算 Loss，Encoder 就被迫去拟合这些变换，导致特征中混入了噪音。



建议： 在你的算法后加入 Non-linear Projection Head (Linear -&gt; ReLU -&gt; Linear) 绝对值得一试！特别是在做特征对齐、对比学习或度量学习的任务中，这往往能带来几个点的提升。


思考总结：
深度学习的很多“黑魔法”，背后往往对应着对信息流、梯度流或优化曲面的某种直觉性的修正。多问几个“为什么”，尝试用不同的视角（如信息论、优化理论）去解释，是进阶的关键。

]]></content>
      <categories>
        <category>学术思考</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>计算机视觉</tag>
        <tag>论文笔记</tag>
        <tag>思考</tag>
      </tags>
  </entry>
  <entry>
    <title>监督对比学习</title>
    <url>/2025/11/%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[Supervised Contrastive Learning
论文地址：https://arxiv.org/pdf/2004.11362
代码地址：https://github.com/HobbitLong/SupContrast
引言
监督对比学习（Supervised Contrastive Learning, SupCon） 是2020年提出的一种结合监督学习和对比学习优势的深度学习方法。与传统的交叉熵损失相比，SupCon通过显式地拉近同类样本、推远异类样本，在图像分类、鲁棒性学习等任务上取得了显著提升。
为什么需要监督对比学习？
传统交叉熵损失的局限：


只关注分类边界

交叉熵只要求正确分类，不关心特征空间的结构
导致同类样本在特征空间可能很分散
决策边界可能过于接近某些样本



忽略类间关系

将所有错误分类同等对待
&quot;猫被分类为狗&quot;和&quot;猫被分类为汽车&quot;的惩罚相同
无法利用类别间的语义相似性



对噪声标签敏感

错误标签直接影响分类边界
难以从噪声中恢复



监督对比学习的优势：


更好的特征空间结构

同类样本紧密聚集
不同类样本明确分离
形成清晰的聚类结构



更强的鲁棒性

对标签噪声更鲁棒
对对抗样本更鲁棒
更好的泛化能力



灵活的应用

可以自然处理开放集识别
支持少样本学习
便于增量学习新类别



方法对比



特性
交叉熵损失
监督对比损失




优化目标
分类准确率
特征空间结构


类内约束
无
拉近同类样本


类间约束
间接（通过分类）
直接推远异类


特征分布
可能分散
紧密聚类


鲁棒性
一般
更强


计算复杂度
O(NC)
O(N²)



其中N是batch size，C是类别数。
背景知识
对比学习简介
对比学习的核心思想是：通过对比正样本对和负样本对，学习到区分性的表示。
无监督对比学习（如SimCLR、MoCo）：

正样本：同一图像的不同增强视图
负样本：不同图像的增强视图
目标：学习到对数据增强不变的表示

关键挑战：

需要大量负样本才能学到好的表示
对数据增强策略敏感
可能学到与下游任务无关的特征

监督学习与对比学习的结合
传统监督学习（交叉熵）：

只关注样本与标签的匹配
忽略了同类样本间的相似性
对对抗样本和噪声不够鲁棒

监督对比学习的优势：

利用标签信息，正样本对更明确（同类样本）
负样本对更丰富（所有异类样本）
学习到的表示更具判别性和鲁棒性

论文核心思想
主要贡献

提出监督对比损失（Supervised Contrastive Loss）：将标签信息融入对比学习框架
理论分析：证明了SupCon损失的梯度特性优于交叉熵
实验验证：在ImageNet等数据集上取得SOTA性能，并显著提升鲁棒性

核心创新
关键洞察：在监督学习中，同类样本应该聚集在一起，异类样本应该分离。这与对比学习的目标天然一致。
方法：将同一类别的所有样本视为正样本，不同类别的样本视为负样本，构建对比学习目标。
方法原理
监督对比损失（Supervised Contrastive Loss）
给定一个batch的样本 ${x_1, x_2, …, x_N}$ 及其标签 ${y_1, y_2, …, y_N}$，对每个样本 $x_i$：
$$ 
\mathcal{L}_{sup} = \sum_{i=1}^{N} \mathcal{L}_{sup}^i
$$ 
其中：
$$ 
\mathcal{L}_{sup}^i = -\frac{1}{|P(i)|} \sum_{p \in P(i)} \log \frac{\exp(z_i \cdot z_p / \tau)}{\sum_{a \in A(i)} \exp(z_i \cdot z_a / \tau)}
$$ 
符号说明：

$z_i = f(x_i)$：样本 $x_i$ 的归一化特征表示
$P(i) = {p \in A(i): y_p = y_i}$：与 $x_i$ 同类的样本集合（正样本）
$A(i) = {1, 2, …, N} \backslash {i}$：除 $i$ 外的所有样本（正样本+负样本）
$\tau$：温度参数，控制分布的尖锐程度

直观理解：

分子：拉近同类样本的相似度
分母：推远所有样本（包括异类和同类）的相似度
归一化：确保同类样本的贡献相等（$\frac{1}{|P(i)|}$）

与交叉熵损失的对比
交叉熵损失：
$$ 
\mathcal{L}_{CE} = -\log \frac{\exp(W_{y_i}^T z_i)}{\sum_{j=1}^{C} \exp(W_j^T z_i)}
$$ 
关键区别：



特性
交叉熵
监督对比损失




优化目标
样本与分类器权重匹配
样本间相似性


梯度特性
只关注当前样本
同时考虑所有同类/异类样本


表示学习
间接（通过分类器）
直接（样本间关系）


鲁棒性
较弱
更强



训练流程

数据增强：对每个样本应用两次随机增强，得到两个视图
特征提取：使用编码器 $f(\cdot)$ 提取特征
归一化：对特征进行L2归一化
计算损失：使用监督对比损失
反向传播：更新编码器参数

注意：SupCon可以单独使用，也可以与交叉熵损失结合使用。
损失函数详解
温度参数 $\tau$ 的作用

$\tau$ 较小：分布更尖锐，模型更关注困难样本
$\tau$ 较大：分布更平滑，模型对所有样本的关注更均匀
典型取值：0.07 或 0.1

梯度分析
SupCon损失的梯度特性：
$$ 
\frac{\partial \mathcal{L}_{sup}^i}{\partial z_i} = \frac{1}{\tau} \left[ \sum_{p \in P(i)} \frac{z_p}{|P(i)|} - \sum_{a \in A(i)} w_a z_a \right]
$$ 
其中 $w_a$ 是softmax权重。
关键观察：

梯度包含所有同类样本的平均（第一项）
梯度包含所有样本的加权平均（第二项）
这比交叉熵只关注单个样本-权重匹配更丰富

与InfoNCE的关系
SupCon可以看作监督版本的InfoNCE：

InfoNCE：正样本是同一图像的不同增强
SupCon：正样本是同一类别的所有样本

实验分析
数据集
论文在多个数据集上进行了实验：

ImageNet：大规模图像分类基准
CIFAR-10/100：小规模图像分类
STL-10：无监督/半监督学习基准

实验设置
网络架构：

ResNet-50/200 作为backbone
投影头：2层MLP（2048→128）

训练细节：

优化器：LARS（ImageNet）或SGD
学习率：0.3（ImageNet）或0.1（CIFAR）
Batch size：1024（ImageNet）或256（CIFAR）
温度参数：$\tau = 0.07$
数据增强：RandomResizedCrop、ColorJitter、RandomHorizontalFlip等

实验结果
ImageNet分类性能：



方法
Top-1 Acc
Top-5 Acc




Cross-Entropy
76.5%
93.1%


SupCon
78.0%
93.8%


SupCon + CE
78.4%
94.0%



鲁棒性提升：

对对抗攻击的鲁棒性显著提升
对常见数据损坏（噪声、模糊等）的鲁棒性更好
在长尾分布数据集上表现更优

消融实验：

温度参数 $\tau$：0.07 效果最好
投影头维度：128维足够
数据增强：重要，但SupCon对增强策略的敏感性低于无监督对比学习

代码实现
PyTorch实现
import torchimport torch.nn as nnimport torch.nn.functional as Fclass SupConLoss(nn.Module):    &quot;&quot;&quot;Supervised Contrastive Learning Loss&quot;&quot;&quot;        def __init__(self, temperature=0.07, base_temperature=0.07):        super().__init__()        self.temperature = temperature        self.base_temperature = base_temperature    def forward(self, features, labels):        &quot;&quot;&quot;        Args:            features: hidden vector of shape [bsz, n_views, ...] or [bsz * n_views, ...].            labels: ground truth of shape [bsz].        Returns:            A loss scalar.        &quot;&quot;&quot;        device = features.device                # 如果features是[batch_size, n_views, dim]，reshape为[batch_size * n_views, dim]        if len(features.shape) &lt; 3:            features = features.unsqueeze(1)        batch_size = features.shape[0]                if labels is not None:            # 扩展labels以匹配增强后的样本数            labels = labels.contiguous().view(-1, 1)            if labels.shape[0] != batch_size:                raise ValueError(&#x27;Num of labels does not match num of features&#x27;)            mask = torch.eq(labels, labels.T).float().to(device)        else:            # 无监督情况：同一图像的增强视图为正样本            mask = torch.eye(batch_size, dtype=torch.float32).to(device)                # 归一化特征        features = F.normalize(features, dim=-1)                # 计算相似度矩阵        contrast_feature = features        anchor_feature = features                # 计算所有样本对之间的相似度        anchor_dot_contrast = torch.div(            torch.matmul(anchor_feature, contrast_feature.T),            self.temperature        )                # 数值稳定性：减去最大值        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)        logits = anchor_dot_contrast - logits_max.detach()                # 计算exp        exp_logits = torch.exp(logits)                # 计算log_prob        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))                # 计算每个样本的正样本平均log_prob        mask = mask.repeat(1, 1)  # 扩展mask以匹配batch_size        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)                # 损失        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos        loss = loss.mean()                return loss# 使用示例class SupConModel(nn.Module):    def __init__(self, encoder, projection_dim=128):        super().__init__()        self.encoder = encoder        self.projector = nn.Sequential(            nn.Linear(encoder.fc.in_features, 2048),            nn.ReLU(),            nn.Linear(2048, projection_dim)        )        # 移除分类头（如果存在）        if hasattr(encoder, &#x27;fc&#x27;):            self.encoder.fc = nn.Identity()        def forward(self, x):        features = self.encoder(x)        projections = self.projector(features)        return F.normalize(projections, dim=-1)# 训练循环示例def train_step(model, images, labels, criterion, optimizer):    # 假设images已经是增强后的[batch_size, 2, C, H, W]    batch_size = images.shape[0]    images = images.view(batch_size * 2, *images.shape[2:])        # 前向传播    features = model(images)    features = features.view(batch_size, 2, -1)        # 计算损失    loss = criterion(features, labels)        # 反向传播    optimizer.zero_grad()    loss.backward()    optimizer.step()        return loss.item()
简化版本
def supervised_contrastive_loss(features, labels, temperature=0.07):    &quot;&quot;&quot;    简化版监督对比损失        Args:        features: [batch_size, feature_dim] 归一化特征        labels: [batch_size] 标签        temperature: 温度参数    &quot;&quot;&quot;    device = features.device    batch_size = features.shape[0]        # 计算相似度矩阵    similarity_matrix = torch.matmul(features, features.T) / temperature        # 构建正样本mask（同类样本）    labels = labels.contiguous().view(-1, 1)    mask = torch.eq(labels, labels.T).float().to(device)        # 移除自身    logits_mask = torch.scatter(        torch.ones_like(mask),        1,        torch.arange(batch_size).view(-1, 1).to(device),        0    )    mask = mask * logits_mask        # 计算exp    exp_logits = torch.exp(similarity_matrix) * logits_mask        # 计算log_prob    log_prob = similarity_matrix - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)        # 平均正样本的log_prob    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)        # 损失    loss = -mean_log_prob_pos.mean()        return loss
技术细节与优化
数据增强策略
SupCon对数据增强的依赖低于无监督对比学习，但仍需要合理的增强：
推荐增强：

RandomResizedCrop
RandomHorizontalFlip
ColorJitter（适度）
RandomGrayscale（可选）

避免过度增强：过度增强可能破坏语义信息，反而降低性能。
投影头设计

层数：2层MLP通常足够
维度：128-256维
激活函数：ReLU或GELU
归一化：输出层L2归一化

温度参数调优

起始值：0.07
调优范围：0.05 - 0.2
原则：如果模型难以区分困难样本，降低$\tau$；如果训练不稳定，提高$\tau$

优缺点分析
优点

性能提升：在多个数据集上超越交叉熵损失
鲁棒性强：对对抗样本、噪声、数据损坏更鲁棒
表示质量高：学习到的特征更具判别性和泛化能力
易于实现：损失函数简单，易于集成到现有框架

缺点

计算成本：需要计算所有样本对的相似度，batch size较大时内存消耗高
需要标签：相比无监督对比学习，需要标注数据
超参数敏感：温度参数等需要仔细调优

与其他方法的关系
vs. 交叉熵损失

交叉熵：关注样本-分类器匹配
SupCon：关注样本-样本关系
结合使用：SupCon + CE 通常效果最好

vs. 无监督对比学习

SimCLR/MoCo：正样本是同一图像的不同视图
SupCon：正样本是同一类别的所有样本
优势：SupCon利用标签信息，正样本对更明确

vs. 三元组损失

三元组损失：每次只考虑一个正样本和一个负样本
SupCon：同时考虑所有正样本和负样本
优势：SupCon的梯度更稳定，训练更高效

应用场景

图像分类：提升分类精度和鲁棒性
长尾学习：在类别不平衡数据上表现优异
少样本学习：学习到的表示泛化能力强
鲁棒性训练：提升模型对对抗攻击的防御能力

总结
监督对比学习通过将标签信息融入对比学习框架，成功结合了监督学习和对比学习的优势。其核心思想是：同类样本应该聚集，异类样本应该分离。通过显式优化样本间的相似性关系，SupCon学习到的表示更具判别性、鲁棒性和泛化能力。
SupCon的提出证明了对比学习不仅适用于无监督场景，在监督学习中同样有效，为后续的对比学习研究提供了重要启发。
参考文献


Khosla, P., et al. (2020). Supervised Contrastive Learning. Advances in Neural Information Processing Systems, 33. https://arxiv.org/pdf/2004.11362


Chen, T., et al. (2020). A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.


He, K., et al. (2020). Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.


Hadsell, R., Chopra, S., &amp; LeCun, Y. (2006). Dimensionality reduction by learning an invariant mapping. CVPR 2006.


思考题

为什么监督对比学习比交叉熵损失更能提升模型的鲁棒性？
温度参数 $\tau$ 如何影响模型的学习？如何选择合适的 $\tau$？
在类别数量很多的情况下，SupCon损失的计算复杂度如何？如何优化？
SupCon与交叉熵损失结合使用时，如何平衡两者的权重？
监督对比学习在少样本学习场景下的优势是什么？

思考题答案
1. 为什么监督对比学习比交叉熵损失更能提升模型的鲁棒性？
原因分析：


表示学习方式不同：

交叉熵：只关注样本与分类器权重的匹配，可能学到与分类器相关的脆弱特征
SupCon：直接优化样本间的相似性，学习到更本质的表示



梯度特性：

交叉熵：梯度只来自当前样本与分类器的匹配
SupCon：梯度来自所有同类和异类样本，信息更丰富，训练更稳定



特征空间结构：

交叉熵：可能形成不规则的决策边界
SupCon：显式地拉近同类、推远异类，形成更紧凑的类内分布和更大的类间间隔



对噪声的鲁棒性：

SupCon通过对比学习，模型学会关注样本间的相对关系而非绝对特征，对噪声更鲁棒



2. 温度参数 $\tau$ 如何影响模型的学习？如何选择合适的 $\tau$？
$\tau$ 的影响：


$\tau$ 较小（如0.05）：

分布更尖锐，模型更关注困难样本（hard negatives）
学习到的表示区分性更强
但可能训练不稳定，容易过拟合



$\tau$ 较大（如0.2）：

分布更平滑，对所有样本的关注更均匀
训练更稳定
但可能学习到的表示区分性较弱



选择策略：

起始值：从0.07开始（论文推荐值）
观察训练曲线：

如果损失下降很快但验证集性能差 → 降低$\tau$
如果训练不稳定或损失不下降 → 提高$\tau$


网格搜索：在[0.05, 0.1, 0.15, 0.2]范围内搜索
任务相关：

细粒度分类（类别相似度高）→ 较小的$\tau$
粗粒度分类（类别差异大）→ 较大的$\tau$



3. 在类别数量很多的情况下，SupCon损失的计算复杂度如何？如何优化？
复杂度分析：

时间复杂度：$O(N^2 \cdot d)$，其中$N$是batch size，$d$是特征维度
空间复杂度：$O(N^2)$（相似度矩阵）

优化策略：


减小batch size：

使用梯度累积保持有效batch size
或使用负样本采样（但会损失部分性能）



混合精度训练：

使用FP16/BF16降低内存和计算成本



分布式训练：

将batch分散到多个GPU，每个GPU计算部分损失



近似方法：

只计算部分负样本（如hard negative mining）
使用memory bank存储历史特征（类似MoCo）



损失近似：

使用NCE（Noise Contrastive Estimation）近似
或使用采样方法估计分母



4. SupCon与交叉熵损失结合使用时，如何平衡两者的权重？
结合方式：
$$ 
\mathcal{L}_{total} = \lambda_{sup} \mathcal{L}_{sup} + \lambda_{ce} \mathcal{L}_{ce}
$$ 
权重选择策略：


等权重：$\lambda_{sup} = \lambda_{ce} = 1.0$（常见起始点）


动态调整：

早期训练：$\lambda_{sup}$较大，学习好的表示
后期训练：$\lambda_{ce}$较大，微调分类边界



任务相关：

如果表示学习更重要（如few-shot）→ 增大$\lambda_{sup}$
如果分类精度更重要 → 增大$\lambda_{ce}$



实验验证：

在验证集上搜索：$\lambda_{sup} \in [0.5, 1.0, 1.5, 2.0]$
通常$\lambda_{sup} = 1.0, \lambda_{ce} = 0.5$效果较好



注意事项：

两个损失的尺度可能不同，需要归一化或调整权重
可以先用SupCon预训练，再用CE微调

5. 监督对比学习在少样本学习场景下的优势是什么？
优势分析：


更好的表示学习：

SupCon学习到的特征更具判别性和泛化能力
即使样本少，也能学到类别的本质特征



类内紧凑性：

显式拉近同类样本，形成紧凑的类内分布
在少样本情况下，这有助于减少类内方差



类间分离性：

显式推远异类样本，增大类间间隔
在少样本情况下，这有助于提高分类精度



数据效率：

每个样本都参与多个正样本对和负样本对的学习
充分利用有限的标注数据



泛化能力：

学习到的表示对数据增强、噪声等更鲁棒
在测试时遇到新样本时泛化更好



实际应用：

在few-shot learning中，先用SupCon在base classes上预训练
然后在novel classes上用few-shot learning方法微调
通常能取得比直接使用交叉熵更好的效果

]]></content>
      <categories>
        <category>论文精读</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>对比学习</tag>
        <tag>深度学习</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识--线性表</title>
    <url>/2025/02/%E7%BA%BF%E6%80%A7%E8%A1%A8/</url>
    <content><![CDATA[线性表
线性表是⼀对⼀的逻辑结构，线性表中除了表头元素每个元素有且仅有唯⼀⼀个前驱元素，除了表尾元素，每个结点都有唯⼀⼀个后继节点。
顺序存储（顺序表）
存取
顺序表随机存取，存取某个元素的时间复杂度为O(1)。
查找
平均时间复杂度O(n)
插入删除
平均时间复杂度O(n)，删除插⼊元素需要移动⼤量元素。（如果是删除最后⼀个元素或者在最后⼀个节点后⾯插⼊⼀个新的节点，则复杂度为O(1)，因为不需要移动元素）
链式存储（链表）
链表因其链状的结构，能方便地删除、插入数据，操作次数是 O(1)。但也因为这样，寻找、读取数据的效率不如数组高，在随机访问数据中的操作次数是 O(n)。
单链表
单链表的基本组成单元是节点 (Node)。 每个节点通常包含两个主要部分：

数据域 (Data): 用于存储实际的数据元素。 数据域可以是任何数据类型，例如整数、浮点数、字符、字符串，甚至更复杂的数据结构。
指针域 (Pointer) 或 下一个节点指针 (Next Pointer):  用于存储指向链表中下一个节点的地址（或引用）。  对于单链表，每个节点只有一个指针，指向它后面的节点。  链表的最后一个节点的指针域通常指向一个特殊值，表示链表的末尾，这个特殊值通常是 空指针 (NULL 或 None)。

创建链表
**头插法：**每次在链表头结点后⾯插⼊新的结点。头插法的元素顺序与插⼊顺序相反，类似于栈。
**尾插法：**每次在链表的尾结点插⼊新的结点，并且是尾结点更新（使尾指针指向新的尾结点）。尾插法得到的顺序与插⼊顺序相同，类似于队列。
查找
查找方式是线性的，平均和最坏情况下的时间复杂度均为 O(n)，其中 n 是链表中的节点数。
插入删除
在链表中插入或删除节点，通常只需要修改指针的指向，而不需要像数组那样移动大量的元素（特别是插入和删除中间位置的元素时）。  时间复杂度通常为 O(1) (在已知插入/删除位置的前驱节点的情况下)。
单链表的缺点

访问效率较低:  要访问链表中的某个特定节点，必须从头节点开始顺序遍历，直到找到目标节点。  无法像数组那样通过索引直接访问，访问时间复杂度为 O(n)，其中 n 是链表的长度。
存储开销:  每个节点除了存储数据外，还需要额外的空间存储指针，增加了存储开销。
不适合随机访问: 由于只能顺序访问，链表不适合需要频繁随机访问的应用场景。

双链表
双链表的节点 (Node) 结构在单链表的基础上增加了一个前驱指针：

前驱指针域 (Previous Pointer):  存储指向链表中前一个节点的地址（或引用）。 链表的头节点 (Head) 的前驱指针通常指向一个特殊值，表示链表头部，这个值通常是 空指针 (NULL 或 None)。
数据域 (Data):  与单链表相同，用于存储数据元素。
后继指针域 (Next Pointer):  与单链表相同，存储指向链表中下一个节点的地址（或引用）。 链表的尾节点 (Tail) 的后继指针通常指向一个特殊值，表示链表尾部，这个值通常是 空指针 (NULL 或 None)。

创建链表
创建一个空的双链表，通常将头指针和尾指针都初始化为 NULL。
查找
与单链表类似，双链表的查找也是线性的，时间复杂度为 O(n)。
双链表的优点

双向遍历:  可以从头到尾，也可以从尾到头双向遍历，提高了数据访问的灵活性。
删除节点更高效:  删除节点时，特别是删除中间节点和尾节点时，由于有前驱指针，可以更方便地找到前一个节点，无需像单链表那样需要从头开始遍历找到前驱节点。 这在某些情况下可以提高删除操作的效率。
某些操作更方便:  例如，在已知节点指针的情况下，插入和删除操作更加直接，不需要总是从头开始查找前驱节点。

双链表的缺点

存储开销更大:  每个节点需要额外的空间存储前驱指针，相比单链表，存储相同的数据需要更多的内存。
插入和删除操作更复杂:  虽然删除操作效率更高，但插入和删除操作的代码逻辑相对单链表来说更复杂，需要维护更多的指针关系（前驱和后继指针都需要更新）。
访问效率仍然较低:  虽然可以双向遍历，但访问链表中的特定节点仍然需要顺序遍历，随机访问效率仍然不如数组。

]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>链表</tag>
        <tag>线性表</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识--绪论</title>
    <url>/2025/02/%E7%BB%AA%E8%AE%BA/</url>
    <content><![CDATA[绪论
数据结构
数据结构是一门研究非数值计算程序设计中的操作对象，以及这些对象之间的关系和操作的学科。
逻辑结构
线性结构
一般线性表：线性表
特殊线性表：栈、队列、字符串
线性表的推广：数组、广义表
非线性结构
树结构：树、二叉树
图结构：有向图、无向图
集合结构
存储结构
顺序存储
逻辑上连续，物理上连续
优点：

随机访问高效：通过下标可直接访问元素，时间复杂度为 O(1)。
内存连续，缓存友好：数据连续存放，充分利用 CPU 缓存机制，访问效率高。
空间开销小：仅需存储数据，无需额外指针字段，内存利用率高。

缺点：

插入/删除效率低：在中间或头部操作时，需移动大量元素，时间复杂度为 O(n)。
固定容量：需预先分配连续内存空间，扩容需复制全部数据，动态扩展成本高。
内存浪费：若预分配空间过大，可能造成内存冗余。

链式存储
逻辑上不要求连续，物理上⼀定连续
优点：

动态内存分配：无需预先分配固定空间，按需动态扩展，内存利用率高。
插入/删除高效：仅需修改指针指向，时间复杂度为 O(1)（需先定位到操作位置）。
灵活性强：支持多种衍生结构（如双向链表、循环链表）。

缺点：


无法随机访问：查找元素需从头遍历，时间复杂度为 O(n)。


额外空间开销：每个节点需存储指针字段，占用更多内存。


内存碎片化：节点非连续存储，缓存命中率低，访问速度较慢。


算法的效率
算法是为了解决某类问题而规定的一个有限长的操作序列。
算法时间复杂度
一般情况，算法中基本语句重复执行的次数是问题规模n的某个函数f(n)，算法的时间量记作: T(n)=O(f(n))
算法空间复杂度
所⽤到的临时空间⼤⼩
]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>绪论</tag>
      </tags>
  </entry>
  <entry>
    <title>门控注意力</title>
    <url>/2025/12/%E9%97%A8%E6%8E%A7%E6%B3%A8%E6%84%8F%E5%8A%9B/</url>
    <content><![CDATA[ 
]]></content>
  </entry>
  <entry>
    <title>强化学习数学原理 - 序章：一张图看懂强化学习 (Course Introduction)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/00_Course_Introduction/</url>
    <content><![CDATA[1. 欢迎来到强化学习的数学世界
强化学习（Reinforcement Learning, RL）近年来在围棋（AlphaGo）、游戏（Dota 2, StarCraft II）以及大模型训练（RLHF）中取得了举世瞩目的成就。然而，很多初学者往往陷入“会调包，不懂原理”的困境。
本系列笔记基于 西湖大学赵世钰老师 的经典教材《Mathematical Foundations of Reinforcement Learning》及其 B 站配套课程整理而成。我们的目标是：不仅知道 RL 怎么用，更要从数学底层看透它为什么能跑通。

2. 一张图讲完强化学习原理
在课程的开头，赵老师用一张极其经典的图概括了 RL 的全貌。理解了这张图，你就理解了 RL 的核心动力学。
核心循环 (The Loop)

Agent (智能体)：决策的主体（比如机器人、AI 程序）。
Environment (环境)：Agent 交互的对象（比如物理世界、棋盘）。
State ($S_t$)：环境给 Agent 的反馈信息（你在哪？情况如何？）。
Action ($A_t$)：Agent 根据 $S_t$ 做的决策（往哪走？做什么？）。
Reward ($R_{t+1}$)：环境给出的即时奖惩（做对了吗？）。

三个关键函数
在数学建模中，我们其实一直在围着这三个函数打转：

Policy $\pi(a|s)$：你的决策规则。
Transition $p(s’|s,a)$：环境的物理规律。
Value $v(s)$：评估一个处境的“长远前途”。


3. 本系列笔记的学习路径 (Roadmap)
我们将按照“基础 -&gt; 算法 -&gt; 进阶”的顺序，分 10 个章节进行深度拆解：
第一阶段：数学地基 (Chapters 1-4)

第1章：基本概念 —— 认识 MDP，理解 RL 的舞台。
第2章：贝尔曼方程 —— 评估策略好坏的数学标尺。
第3章：贝尔曼最优方程 —— 定义什么是“完美策略”。
第4章：值迭代与策略迭代 —— 拥有“上帝视角”时的最优解法。

第二阶段：实战演操 (Chapters 5-7)

第5章：蒙特卡洛方法 —— 丢掉“上帝视角”，从试错中学习。
第6章：随机近似 —— 现代 RL 更新公式背后的硬核数学（SGD 的本质）。
第7章：时序差分 (TD) —— 强化学习的灵魂，Sarsa 与 Q-learning。

第三阶段：深度进化 (Chapters 8-10)

第8章：值函数近似 —— 引入神经网络，征服无限状态 (DQN)。
第9章：策略梯度 —— 直接优化决策动作，处理连续空间 (REINFORCE)。
第10章：Actor-Critic —— 演员与评论家的终极配合 (A2C)。


4. 为什么这门课与众不同？

严谨性：不同于 Sutton 的教材（更偏直觉），赵老师的课非常强调数学证明（如压缩映射定理、RM 条件）。
工程化：每章笔记我们都会配上 Python/PyTorch 代码，确保公式能落地。
系统性：从简单的 GridWorld（网格世界）出发，一步步推导到现代大模型都在用的 RL 技术。

现在，请系好安全带，让我们从 第1章：基本概念 开始这段数学旅程。

下一章：第1章 - 基本概念 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Roadmap</tag>
        <tag>Introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 必备数学基础 (Mathematical Preliminaries)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/00_Math_Foundation/</url>
    <content><![CDATA[1. 为什么 RL 的数学门槛高？
强化学习被公认为 AI 领域最难学的分支之一，原因在于它融合了：

概率论（状态随机性）。
优化论（梯度下降）。
随机过程（轨迹演化）。
泛函分析（算子与不动点）。

为了让你在后续章节不被公式劝退，本篇博文基于教材附录，总结了最核心的数学工具。

2. 概率论：不只是扔骰子
2.1 全期望公式 (Law of Total Expectation) - 必考点
在推导贝尔曼方程时，你会反复看到这个技巧：
$$ \mathbb{E}[X] = \mathbb{E} [ \mathbb{E}[X | Y] ] $$
在 RL 中，它表现为：当前状态的价值 = 下一个可能状态价值的加权平均。
2.2 状态分布 (Stationary Distribution)
当 Agent 在环境中跑了很久以后，它处于各个格子的概率会稳定下来。

数学本质：转移矩阵 $P_\pi$ 的左特征向量。
意义：这决定了我们在优化目标函数时，应该给哪些格子更大的权重。


3. 随机过程：关于“收敛”的艺术
3.1 鞅 (Martingale)
如果你在后续章节看到算法收敛性证明，通常会提到“鞅”。

定义：$\mathbb{E}[X_{n+1} | X_1, \dots, X_n] = X_n$。
直观理解：这就是公平博弈。如果你在赌场玩公平的游戏，你明天的钱数期望等于今天的。
应用：RL 算法的残差更新过程常被构造为一个鞅（或超鞅），从而利用收敛定理证明算法最后能停在最优解。


4. 优化论：梯度下降的深层细节
4.1 凸性 (Convexity)

如果损失函数是凸的，SGD 一定能找到全局最优。
痛点：深度强化学习（Deep RL）的损失函数通常是非凸的，因此 DQN、PPO 等算法需要很多工程技巧（如随机初始化、多随机种子）来避免掉进局部最优。

4.2 Lipschitz 连续性
为什么学习率 $\alpha$ 不能太大？
如果梯度变化太猛，模型就会跑飞。Lipschitz 常数限制了梯度的变化速度，是确定“安全步长”的关键。

5. 总结：你应该掌握到什么程度？
你不需要成为数学家，但当你看到以下符号时，脑中应该有画面感：

$\sum_a \pi(a|s)$：是在做选择的平均。
$\sum_{s’} p(s’|s,a)$：是在做物理世界的随机平均。
$\nabla \ln \pi$：是在问“参数往哪挪，动作概率会变大”。

有了这些地基，我们就可以正式进入 RL 的世界了！

下一章：第1章 - 基本概念 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Probability</tag>
        <tag>Gradient Descent</tag>
        <tag>Martingale</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第2章：贝尔曼方程 (Bellman Equation)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/02_Bellman_Equation/</url>
    <content><![CDATA[1. 为什么需要贝尔曼方程？
在第1章中，我们建立了 MDP 模型，但留下了一个核心问题：如何评价一个策略（Policy）的好坏？
直观上，如果一个策略能让 Agent 获得更多的 Return（累积回报），它就是好的。但 Return $G_t$ 是一个随机变量（取决于未来的随机状态转移和奖励），我们不能直接比较随机变量。因此，我们引入 期望（Expectation），即 状态价值（State Value）。
贝尔曼方程（Bellman Equation）就是描述状态价值之间关系的数学工具，它是强化学习的基石。

2. 状态价值函数 (State Value Function)
2.1 定义
给定一个策略 $\pi$，状态 $s$ 的价值 $v_\pi(s)$ 定义为从状态 $s$ 出发，遵循策略 $\pi$ 能获得的期望回报：
$$ v_\pi(s) = \mathbb{E}[G_t \mid S_t = s] $$
2.2 为什么叫 “Bootstrapping” (自举)？
我们可以把 Return 展开：
$$ \begin{aligned} G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \ &amp;= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \ &amp;= R_{t+1} + \gamma G_{t+1} \end{aligned} $$
对两边求期望：
$$ v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s] $$
这看起来像是一个死循环：要算当前状态的价值，得先知道未来状态的价值。这种“自己提着鞋带把自己提起来”的思想在 RL 中称为 Bootstrapping。但实际上，这是一组联立方程，完全可以求解。

3. 贝尔曼方程的推导 (Step-by-Step)
为了把上面的期望展开成具体的计算公式，我们需要用到 MDP 的转移概率 $p(s’|s,a)$ 和策略 $\pi(a|s)$。

\begin{aligned}
v_\pi(s)
&= \mathbb{E}_{\pi}\!\left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s \right] \\
&= \sum_{a \in \mathcal{A}} \pi(a \mid s)\sum_{s' \in \mathcal{S}} p(s' \mid s,a)\left[ r(s,a,s') + \gamma v_\pi(s') \right] \\
&= \sum_{a \in \mathcal{A}} \pi(a \mid s)\, q_\pi(s,a)
\end{aligned}

这个公式可以拆解为两步理解（对应视频中的树状图）：

平均动作: 我可能选动作 A，也可能选动作 B，按概率 $\pi(a|s)$ 加权。
平均结果: 选了动作 A 后，环境可能把我送到 $s’_1$，也可能送到 $s’_2$，按概率 $p(s’|s,a)$ 加权。


4. 矩阵形式与代码求解 (Matrix Form)
对于有限状态 MDP，我们可以把贝尔曼方程写成非常漂亮的矩阵形式。这是理解“求解价值”本质的关键。
4.1 矩阵公式
假设状态有 $n$ 个，我们将 $v_\pi(s)$ 排列成向量 $V \in \mathbb{R}^n$：
$$ V = R_\pi + \gamma P_\pi V $$
其中：

$R_\pi$ 是平均即时奖励向量，其第 $s$ 个元素为 $\sum_a \pi(a|s) \sum_{s’} p(s’|s,a) r(s,a,s’)$。
$P_\pi$ 是状态转移矩阵，其第 $(s, s’)$ 元素为 $\sum_a \pi(a|s) p(s’|s,a)$。

4.2 解析解 (Closed-form Solution)
通过简单的线性代数变换：
$$ \begin{aligned} V - \gamma P_\pi V &amp;= R_\pi \ (I - \gamma P_\pi) V &amp;= R_\pi \ V &amp;= (I - \gamma P_\pi)^{-1} R_\pi \end{aligned} $$
只要 $\gamma &lt; 1$，矩阵 $(I - \gamma P_\pi)$ 就是可逆的。这意味着：只要环境模型 ($P, R$) 和策略 ($\pi$) 已知，状态价值 $V$ 是唯一的，且可以直接算出！
4.3 深度思考：矩阵逆与无穷级数的统一
这里隐藏着一个极美的数学联系。我们知道，对于标量 $x$ ($|x|&lt;1$)，有级数展开 $\frac{1}{1-x} = 1 + x + x^2 + \dots$。
同样的逻辑应用到矩阵上，Neumann Series 告诉我们：
$$ (I - \gamma P)^{-1} = I + \gamma P + (\gamma P)^2 + (\gamma P)^3 + \dots $$
将这个展开式代入 $V = (I - \gamma P)^{-1} R$，我们得到：
$$ V = R + \gamma P R + \gamma^2 P^2 R + \dots $$
物理意义震撼人心：

$R$：第一步的平均奖励。
$\gamma P R$：第二步的平均奖励（$P$ 转移了一次）。
$\gamma^2 P^2 R$：第三步的平均奖励（$P$ 转移了两次）。

结论：“解线性方程组”（代数视角）和**“累加未来所有奖励”**（轨迹视角）在数学上是完全等价的！这也解释了为什么 $\gamma$ 必须小于 1：为了保证这个无穷级数收敛。
4.4 Python 代码实战
让我们用 NumPy 实现这个求解过程。假设一个简单的 4 状态环境（如书中 Figure 2.3）：
import numpy as npdef solve_bellman_equation(P, R, gamma=0.9):    &quot;&quot;&quot;    求解 v = R + gamma * P * v    :param P: 状态转移矩阵 (num_states x num_states)    :param R: 平均奖励向量 (num_states)    &quot;&quot;&quot;    num_states = P.shape[0]    I = np.eye(num_states)        # 核心公式: V = inv(I - gamma * P) * R    # 注意: np.linalg.solve(A, b) 比 inv(A) @ b 更快更稳    V = np.linalg.solve(I - gamma * P, R)    return V# --- 示例数据 ---# 假设 4 个状态，策略导致的状态转移概率如下：# s1 -&gt; 0.5-&gt;s1, 0.5-&gt;s2# s2 -&gt; 1.0-&gt;s3# s3 -&gt; 1.0-&gt;s4# s4 -&gt; 1.0-&gt;s4 (终点/吸收态)P_pi = np.array([    [0.5, 0.5, 0.0, 0.0],    [0.0, 0.0, 1.0, 0.0],    [0.0, 0.0, 0.0, 1.0],    [0.0, 0.0, 0.0, 1.0]])# 每个状态出发获得的平均即时奖励R_pi = np.array([-1.0, 0.0, 1.0, 0.0])# 计算价值V = solve_bellman_equation(P_pi, R_pi)print(f&quot;State Values: &#123;V&#125;&quot;)
运行结果会告诉你每个状态“值多少钱”。如果 $V(s_1) = -2.5$，意味着从 $s_1$ 开始玩，平均会亏 2.5 分。

5. 迭代法求解 (Iterative Solution)
虽然矩阵求逆很帅，但在状态很多（比如围棋 $10^{170}$）时，求逆矩阵 $O(n^3)$ 是不可能的。这时我们需要迭代法。
$$ v_{k+1} = R_\pi + \gamma P_\pi v_k $$
算法逻辑：

随便猜一个 $V_0$（比如全 0）。
用贝尔曼方程右边算出一个新的 $V_1$。
再把 $V_1$ 代入右边算出 $V_2$…
一直算，直到 $V_k$ 和 $V_{k+1}$ 几乎一样（收敛）。

这正是动态规划 (Dynamic Programming) 的雏形，也是后续 Value Iteration 算法的基础。

6. 动作价值函数 (Action Value, Q-function)
除了状态价值 $v_\pi(s)$，还有一个更重要的概念：动作价值 $q_\pi(s, a)$。
它表示：在状态 $s$ 强制采取动作 $a$，之后遵循策略 $\pi$ 的期望回报。
6.1 为什么要 Q 值？
如果只有 $v_\pi(s)$，Agent 到了路口不知道怎么选，因为它只知道“我在这个路口平均能得 10 分”，但不知道“往左走得多少分，往右走得多少分”。
$q_\pi(s, a)$ 明确给出了每个动作的评分，有了 Q 值，Agent 就能直接选 Q 值最大的动作（Greedy Policy），从而改进策略。
6.2 V 与 Q 的关系
$$ \begin{aligned} v_\pi(s) &amp;= \sum_{a} \pi(a|s) q_\pi(s,a) \ q_\pi(s,a) &amp;= \sum_{s’} p(s’|s,a) [ r(s,a,s’) + \gamma v_\pi(s’) ] \end{aligned} $$

7. 总结
本章我们攻克了 RL 中最核心的数学堡垒：

State Value $v(s)$: 衡量处境好坏。
Bellman Equation: $v(s)$ 必须满足的递归方程。
求解方法: 小规模用矩阵求逆，大规模用迭代更新。
Action Value $q(s,a)$: 衡量动作好坏，是策略改进的桥梁。

有了评价标准，下一章我们终于可以聊如何寻找最优策略了。我们将引入 Bellman Optimality Equation (BOE)。

上一章：第1章 - 基本概念 | 下一章：第3章 - 最优状态值与贝尔曼最优方程 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Math</tag>
        <tag>Bellman Equation</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第1章：基本概念 (Basic Concepts)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/01_Basic_Concepts/</url>
    <content><![CDATA[1. 为什么需要强化学习？
在进入数学公式之前，我们需要理解强化学习（Reinforcement Learning, RL）解决的是什么核心问题。
与监督学习（Supervised Learning）不同，RL 没有“上帝视角”的标签（Label）。Agent（智能体）是在未知的环境中，通过**试错（Trial-and-Error）**来学习的。这就像一个婴儿学习走路，没有说明书，只有摔倒时的疼痛（负奖励）和站稳时的喜悦（正奖励）。
本章我们将基于 Westlake University 赵世钰老师 的课程，结合配套的 GridWorld 代码，从零构建 RL 的数学大厦——马尔可夫决策过程 (MDP)。

2. 核心图景：Agent 与 Environment 的交互
RL 的世界观可以浓缩为一张图（对应视频中的核心板书）：

Time $t$: Agent 处于状态 $S_t$（State）。
Decision: Agent 观察到 $S_t$，根据策略 $\pi$ 选择动作 $A_t$（Action）。
Interaction: Agent 将 $A_t$ 施加给 Environment。
Feedback: Environment 根据物理规则（Dynamics）演变到新状态 $S_{t+1}$，并反馈即时奖励 $R_{t+1}$（Reward）。
Loop: 时间来到 $t+1$，循环继续。

这个循环产生的序列称为轨迹 (Trajectory)：
$$ \tau = S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots $$

3. 马尔可夫决策过程 (MDP) 深度解析
为了用数学描述上述过程，我们定义 MDP 五元组 $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$。
3.1 状态空间 (State Space, $\mathcal{S}$)
状态是 Agent 做决策的依据。

GridWorld 实例: 在一个 $5 \times 5$ 的网格中，状态空间是所有方格的集合。
$$ \mathcal{S} = { (x,y) \mid x,y \in {0,1,2,3,4} } $$
或者将其展平为索引 $s \in {0, 1, \dots, 24}$。
代码视角:
在 src/grid_world.py 中，self.num_states = env_size[0] * env_size[1] 定义了状态大小。

3.2 动作空间 (Action Space, $\mathcal{A}$)

GridWorld 实例: Agent 可以向四个方向移动或原地不动。
$$ \mathcal{A} = { \text{Up}, \text{Down}, \text{Left}, \text{Right}, \text{Stay} } $$
代码视角:# 动作被编码为坐标的变化量self.action_space = [(0, -1), (0, 1), (-1, 0), (1, 0), (0, 0)]


3.3 状态转移概率 (State Transition Probability, $\mathcal{P}$) —— 核心中的核心
这是环境的“物理定律”。当我们执行动作 $a$ 时，环境并不一定 100% 听话。

定义:
$$ p(s’ | s, a) = \mathbb{P}(S_{t+1} = s’ \mid S_t = s, A_t = a) $$
它表示：在状态 $s$ 采取动作 $a$，跳转到状态 $s’$ 的概率。
性质: $\sum_{s’ \in \mathcal{S}} p(s’ | s, a) = 1$。
GridWorld 代码逻辑:
虽然书中的数学定义允许随机性（例如：想向右走，但有 10% 概率被风吹向上），但在配套代码 grid_world.py 的基础实现中，转移是确定性 (Deterministic) 的：def _get_next_state_and_reward(self, state, action):    # 确定性计算：当前坐标 + 动作向量    new_state = tuple(np.array(state) + np.array(action))    # ... (然后进行边界检查，撞墙则弹回)
注意：理解 MDP 时，必须时刻记住 $p(s’|s,a)$ 通常是一个概率分布，确定性只是概率为 1 的特例。

3.4 奖励函数 (Reward Function, $\mathcal{R}$)
奖励是指挥棒，引导 Agent 学习我们期望的行为。

定义:
$$ r(s, a) = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a] $$
或者更通用的形式 $r(s, a, s’)$，依赖于下一状态。
GridWorld 设计哲学:
在代码中，Reward 设计不仅是为了告诉 Agent “好坏”，更是为了“引导”。

reward_target = 1: 正向激励，到达终点。
reward_forbidden = -10: 强负反馈，踩到陷阱（禁区）。
reward_step = -0.1 (假设值): 生存惩罚。每走一步都扣分，这不仅不是坏事，反而是为了鼓励 Agent 走最短路径（因为走得越久扣得越多）。



3.5 折扣因子 (Discount Factor, $\gamma$)
这是一个常数 $\gamma \in [0, 1)$。为什么要它？

数学收敛性: 避免无限时间序列求和发散。
经济学/心理学: “双鸟在林不如一鸟在手”。未来的奖励充满不确定性，且价值随时间衰减。


4. 回报 (Return) —— 真正要最大化的目标
初学者容易混淆 Reward (即时奖励) 和 Return (回报)。

Reward ($R_t$): 这一步走得怎么样？
Return ($G_t$): 从这一步开始，未来这一辈子总共能拿多少分？

数学定义（Discounted Return）:
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$
从代码看 Return 的计算:
在 example_grid_world.py 的循环中，我们其实是在收集 $R$：
state = env.reset()total_return = 0gamma = 0.9for t in range(1000):    action = random.choice(env.action_space) # 随机策略    next_state, reward, done, info = env.step(action)        # 累积 Return (这里演示的是未折扣的累加，实际 RL 算法使用折扣形式)    # 若要计算 discounted return，需要保存整条轨迹然后反向计算    print(f&quot;Step: &#123;t&#125;, Reward: &#123;reward&#125;&quot;)

5. 策略 (Policy, $\pi$)
MDP 给了我们环境规则，Policy 则是 Agent 的玩法。

定义: $\pi(a|s)$ 是在状态 $s$ 选择动作 $a$ 的概率分布。
$$ \sum_{a \in \mathcal{A}} \pi(a|s) = 1 $$
代码实战:
在 example_grid_world.py 中，有一段代码手动添加了一个策略矩阵：# 创建一个随机策略矩阵 (num_states x num_actions)policy_matrix = np.random.rand(env.num_states, len(env.action_space))# 归一化：保证每一行的概率和为 1policy_matrix /= policy_matrix.sum(axis=1)[:, np.newaxis]env.add_policy(policy_matrix) # 可视化策略箭头
可视化的结果是：在每个格子上，会有不同粗细的箭头指向不同方向，箭头越粗代表 $\pi(a|s)$ 越大。


6. 本章总结与下章预告
本章我们构建了 RL 的静态框架：

我们有了地图 (State)。
我们知道能怎么走 (Action)。
我们知道走一步会发生什么 (Transition)。
我们知道什么是好的结果 (Reward)。
我们定义了终极目标：最大化 Return ($G_t$)，而不是短视地看 $R_t$。

遗留问题:
现在 Agent 只是在“瞎走”（Random Policy）。如果你是 Agent，站在某个格子上，你怎么知道往哪走才是通往高 Return 的路？
你需要一个“地图导航”，告诉你每个格子的价值 (Value)。
这就引出了下一章的核心：状态价值函数 (State Value) 与 贝尔曼方程 (Bellman Equation) —— 它们是 RL 中评估策略好坏的标尺。

下一章：第2章 - 状态值与贝尔曼方程 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Math</tag>
        <tag>Python</tag>
        <tag>GridWorld</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第4章：值迭代与策略迭代 (Value &amp; Policy Iteration)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/04_Value_Policy_Iteration/</url>
    <content><![CDATA[1. 动态规划的登场
在第3章中，我们推出了 Bellman Optimality Equation (BOE)：
$$ v^(s) = \max_{a} \sum_{s’} p(s’|s,a) [ r + \gamma v^(s’) ] $$
因为 $\max$ 的非线性，我们无法直接解方程。本章我们将介绍求解 BOE 的两把“倚天屠龙剑”：

Value Iteration (VI, 值迭代)：粗暴直接，对着 BOE 猛算。
Policy Iteration (PI, 策略迭代)：优雅从容，交替进行评估和改进。

这两类算法统称为 动态规划 (Dynamic Programming, DP) 方法。它们要求我们完全已知环境模型（$P$ 和 $R$）。

2. 值迭代 (Value Iteration)
2.1 算法逻辑
VI 的思想非常简单：既然 $v^*$ 是 BOE 的不动点，那我就把 BOE 当作更新规则，一直迭代直到收敛。
$$ v_{k+1}(s) = \max_{a} \underbrace{\sum_{s’} p(s’|s,a) [ r(s,a,s’) + \gamma v_k(s’) ]}_{q_k(s,a)} $$
2.2 Python 实现 (Standard VI)
这是一个标准的 VI 算法模板，可以直接用于解 GridWorld 问题。
import numpy as npdef value_iteration(P, R, gamma=0.9, theta=1e-4):    &quot;&quot;&quot;    :param P: 状态转移概率 [num_states, num_actions, num_states]              注意：这里为了通用，假设 P 是三维张量    :param R: 奖励矩阵 [num_states, num_actions, num_states]    &quot;&quot;&quot;    num_states, num_actions, _ = P.shape    V = np.zeros(num_states) # 初始化 V        while True:        delta = 0        # 1. 对每个状态进行更新        for s in range(num_states):            v_old = V[s]                        # 2. 计算所有动作的 Q 值            # Q[a] = sum(P[s,a,s&#x27;] * (R[s,a,s&#x27;] + gamma * V[s&#x27;]))            q_values = np.zeros(num_actions)            for a in range(num_actions):                for s_prime in range(num_states):                    prob = P[s, a, s_prime]                    if prob &gt; 0:                        reward = R[s, a, s_prime]                        q_values[a] += prob * (reward + gamma * V[s_prime])                        # 3. 贪心更新 (Bellman Optimality Backup)            V[s] = np.max(q_values)                        # 4. 记录最大变化量            delta = max(delta, abs(v_old - V[s]))                    # 5. 收敛检查        if delta &lt; theta:            break                # 6. 提取最优策略    policy = np.zeros(num_states, dtype=int)    for s in range(num_states):        q_values = np.zeros(num_actions)        for a in range(num_actions):            for s_prime in range(num_states):                 q_values[a] += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])        policy[s] = np.argmax(q_values)            return policy, V

3. 策略迭代 (Policy Iteration)
3.1 算法逻辑
PI 的思想更符合人类直觉：

先随便定个策略（比如全往左走）。
Policy Evaluation (PE): 算算这个策略到底能得多少分 ($v_\pi$)。

这里解的是线性方程 $v = r_\pi + \gamma P_\pi v$，比 VI 的 $\max$ 容易。


Policy Improvement (PI): 既然知道各状态的分数了，看看有没有哪个路口换个动作能得分更高？($\pi_{new}(s) = \arg\max q_\pi(s,a)$)。
重复 2-3，直到策略不再变化。

3.2 为什么 PI 会收敛？
书中给出了严格证明：策略改进定理 (Policy Improvement Theorem) 保证了每次改进后的策略 $v_{\pi_{new}}(s) \ge v_{\pi_{old}}(s)$。因为有限 MDP 的策略总数是有限的（$|\mathcal{A}|^{|\mathcal{S}|}$），所以一定会在有限步内收敛到最优。
3.3 Python 实现
def policy_iteration(P, R, gamma=0.9):    num_states, num_actions, _ = P.shape    # 1. 初始化随机策略    policy = np.random.randint(0, num_actions, num_states)    V = np.zeros(num_states)        while True:        # --- Step 2: Policy Evaluation ---        while True:            delta = 0            for s in range(num_states):                v_old = V[s]                a = policy[s]                # 只计算当前策略选的动作 a 的价值                v_new = 0                for s_prime in range(num_states):                    v_new += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])                V[s] = v_new                delta = max(delta, abs(v_old - V[s]))            if delta &lt; 1e-4: break                # --- Step 3: Policy Improvement ---        policy_stable = True        for s in range(num_states):            old_action = policy[s]                        # 寻找当前 V 下最好的动作            q_values = np.zeros(num_actions)            for a in range(num_actions):                for s_prime in range(num_states):                    q_values[a] += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])                        best_action = np.argmax(q_values)            policy[s] = best_action                        if old_action != best_action:                policy_stable = False                        if policy_stable:            break                return policy, V

4. 截断策略迭代 (Truncated Policy Iteration)
4.1 VI 和 PI 的统一视角

PI: Policy Evaluation 步骤要求精确求解 $v_\pi$（比如迭代直到 $\delta &lt; 10^{-4}$，或者解矩阵方程）。这非常耗时。
VI: 其实可以看作 Policy Evaluation 只做了一步（更新一次 $V$），就立刻进行 Policy Improvement（取 $\max$）。

Truncated PI: 我们可以折中一下。在 Evaluation 阶段，我不求精确解，只迭代 $k$ 次（比如 5 次），然后就去改进策略。

当 $k=1$ 时，它就是 VI。
当 $k=\infty$ 时，它就是 PI。

这个视角非常重要，它告诉我们：不用追求完美的 Evaluation，差不多的 Value 就能指导出更好的 Policy。
4.2 深度思考：广义策略迭代 (Generalized Policy Iteration, GPI)
Sutton 在其经典教材中提出了 GPI 的概念，它不仅仅是 VI 和 PI 的统称，更揭示了 RL 的核心动力学。
RL 就像是一场**“拔河比赛”**：

Evaluation ($V \to V_\pi$)：试图让 $V$ 贴合当前的 $\pi$（说实话）。
Improvement ($\pi \to \text{greedy}(V)$)：试图让 $\pi$ 变得更贪心（想耍赖）。

这两个过程往往是冲突的：一旦 $\pi$ 变贪心了，$V$ 就不准了；一旦 $V$ 算准了，$\pi$ 又不够贪心了。
GPI 的美妙之处：只要我们持续不断地交替进行这两个过程（哪怕 Evaluation 不准，哪怕 Improvement 不完全），最终它们会在最优解处达成和解（Nash 均衡点）。
$$ v^* = \text{greedy}(v^*) $$
这就是所有 RL 算法（包括 DQN, PPO）能够收敛的底层逻辑。

5. 总结与对比



特性
Value Iteration (VI)
Policy Iteration (PI)




核心操作
一直取 $\max$ (BOE)
交替 Eval (解方程) 和 Improve (取 $\max$)


单步计算量
小 (只算一遍 Q)
大 (PE 阶段要解方程或多次迭代)


收敛步数
多 (线性收敛)
少 (通常几步就收敛)


适用场景
状态空间大，不要求极高精度
状态空间小，追求精确解



到此为止，我们已经掌握了有模型 (Model-based) 的所有核心武功。
但在现实中，我们往往不知道 $P$ 和 $R$（比如你不知道股票涨跌的概率公式）。这时候该怎么办？
下一章，我们将进入 Model-free (无模型) 的世界，第一站：蒙特卡洛方法 (Monte Carlo Methods)。

上一章：第3章 - 贝尔曼最优方程 | 下一章：第5章 - 蒙特卡洛方法 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Python</tag>
        <tag>Dynamic Programming</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第5章：蒙特卡洛方法 (Monte Carlo Methods)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/05_Monte_Carlo/</url>
    <content><![CDATA[1. 跨越：从“有模型”到“无模型”
在之前的章节中，我们假设自己是“上帝”，完全知道环境的秘密：状态转移概率 $P$ 和奖励函数 $R$。但在现实中（如炒股、下棋），这些参数通常是不可知的。
蒙特卡洛 (Monte Carlo, MC) 方法 的出现，标志着我们进入了 Model-free (无模型) 学习的领域。它的核心思想极度朴素：“实践出真知” —— 如果我不知道概率，那我就多试几次，然后算平均值。

2. 蒙特卡洛估算 (MC Prediction)
2.1 核心思想：大数定律
我们想求 $v_
ps(s) = [G_t | S_t = s]$。
根据大数定律，如果我们能生成很多条从状态 $s$ 出发的轨迹（Episodes），并记录每一条的回报 $G$，那么它们的平均值就会收敛于期望。
$$ v_
ps(s) r rac{1}{N} sum_{i=1}^N G_i
$$
2.2 两种采样方式

First-visit MC: 在一个 Episode 中，如果多次经过 $s$，只计算第一次访问后的回报。
Every-visit MC: 只要经过 $s$，就计算一次回报。
结论: 两种方法都会收敛，但 First-visit 在数学处理上更简单。


3. 蒙特卡洛控制 (MC Control)
我们不仅想评估策略，还想改进策略。在 Model-free 场景下，我们面临一个尴尬：

如果只有 $v(s)$，即便我知道当前状态值多少钱，我也不知道该做哪个动作，因为我没有 $P$ 矩阵来算下一步去哪。

解决方案: 直接估计 动作价值 $q_
ps(s, a)$。
有了每个动作的 Q 值，我就能通过 $max$ 直接改进策略。
3.1 MC Basic 算法
这是最原始的无模型算法逻辑：

评估: 通过大量采样，算出当前策略 $
ps$ 的 $Q(s, a)$ 表。
改进: $
ps_{new}(s) = max_a Q(s, a)$。
重复直到收敛。

3.2 探索性初始化 (Exploring Starts)
致命缺陷: 如果我们的 Agent 比较“固执”，它可能永远不去尝试某些动作，导致那些动作的 Q 值永远得不到更新。
Exploring Starts: 强制要求采样时，每一对 $(s, a)$ 都要有一定概率作为起始点。但这在现实中很难（比如自动驾驶，你不能强制让它从“撞墙”动作开始）。

4. 探索与利用 (Exploration vs Exploitation)
为了在不依赖 Exploring Starts 的情况下保持探索，我们引入了著名的 $$-greedy 策略。

Exploitation (利用): 以 $1-$ 的概率选择当前最优动作（榨取已知信息）。
Exploration (探索): 以 $$ 的概率随机选一个动作（寻找潜在可能）。

$$
ps(a|s) = egin{cases} 1 -  + rac{}{||} &amp; 	ext{if } a = a^* \ rac{}{||} &amp; 	ext{if } a
e a^*
u}
$$
视频中赵老师强调：$$-greedy 是一种折中，它虽然可能导致我们在已知最优的情况下偶尔“乱走”，但它保证了我们不会错过更好的世界。

5. Python 代码实战：MC 价值估算
在这个例子中，我们不再需要 $P$ 矩阵，而是需要一个可以 env.step() 的环境。
import numpy as npfrom collections import defaultdictdef mc_prediction(env, policy, num_episodes, gamma=1.0):    &quot;&quot;&quot;    使用 First-visit MC 估计策略的价值函数 V    &quot;&quot;&quot;    # 存储每个状态的回报总和与访问次数    returns_sum = defaultdict(float)    returns_count = defaultdict(float)    V = defaultdict(float)    for i in range(num_episodes):        # 1. 生成一个 Episode (s0, a0, r1, s1, a1, r2, ...)        episode = []        state = env.reset()        while True:            action = policy[state]            next_state, reward, done = env.step(action)            episode.append((state, reward))            if done: break            state = next_state        # 2. 计算每个状态的回报 G (反向遍历效率更高)        G = 0        visited_states = set()        for s, r in reversed(episode):            G = r + gamma * G            # First-visit 检查            if s not in visited_states:                returns_sum[s] += G                returns_count[s] += 1                V[s] = returns_sum[s] / returns_count[s]                visited_states.add(s)                    return V

6. MC 方法的优缺点 (Bilibili 重点)

优点:

不需要模型: 只要能交互就能学。
局部更新: 如果我只关心某个特定状态的价值，我只需要采样相关的轨迹，不需要像 DP 那样遍历全图。


缺点:

低效: 必须等到 Episode 结束才能更新。
方差大: $G_t$ 受到整条轨迹随机性的累积影响。



预告:
既然 MC 要等到结束才更新，能不能“走一步更新一步”？这就是下一章我们要聊的强化学习最伟大的发明：时序差分 (Temporal Difference, TD)。

上一章：第4章 - 值迭代与策略迭代 | 下一章：第6章 - 随机近似 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Python</tag>
        <tag>Monte Carlo</tag>
        <tag>Model-free</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第6章：随机近似 (Stochastic Approximation)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/06_Stochastic_Approximation/</url>
    <content><![CDATA[1. 为什么学 RL 要学“随机近似”？
本章看似是数学插曲，实则是连接蒙特卡洛 (MC) 和时序差分 (TD) 的桥梁。
所有现代 RL 算法（Q-learning, DQN, Policy Gradient）的更新公式都长这样：
$$ w ← w + α (	ext{Target} - w) $$
这个公式是怎么来的？为什么它能收敛？这背后的数学原理就是 Robbins-Monro 算法。

2. 均值估计：从批量到增量
假设我们想求随机变量 $X$ 的期望 $w = ℕ[X]$。
我们可以采样 $x_1, x_2, …, x_k$。
2.1 批量计算 (Batch)
$$ w_k = rac{1}{k} ∑_{i=1}^k x_i $$
缺点：每次都要把所有数加起来除以 $k$，或者保存所有数。
2.2 增量计算 (Incremental)
我们可以推导出递推公式：
$$ w_{k+1} = w_k + rac{1}{k+1} (x_{k+1} - w_k) $$
这就是最简单的随机近似！

$w_k$: 当前的估计值。
$x_{k+1}$: 新的样本。
$(x_{k+1} - w_k)$: 误差 (Error)。
$rac{1}{k+1}$: 步长 (Step Size)，通常记为 $α_k$。


3. Robbins-Monro (RM) 算法
RM 算法解决更通用的问题：求解方程 $g(w) = 0$。
难点在于：我们不知道 $g(w)$ 的具体解析式，而且我们每次观测 $g(w)$ 时都带有噪声 $η$。
$$ rig(w) = g(w) + η $$
RM 更新公式:
$$ w_{k+1} = w_k - a_k rig(w_k) $$
收敛条件:
只要步长序列 {$a_k$} 满足：

$∑ a_k = ∞$ (步子总长足够大，能走到解)。
$∑ a_k^2 &lt; ∞$ (步子变小的速度足够快，保证最后收敛不震荡)。
那么 $w_k$ 就会以概率 1 收敛到方程的根 $w^*$。


4. 随机梯度下降 (SGD)
SGD 是 RM 算法在优化问题上的应用。
目标：$≈ J(w)$。等价于求梯度为 0 的根：$
abla J(w) = 0$。
这里 $g(w) =
abla J(w)$。
由于我们通常无法计算全量梯度（数据太多），只能用一个样本的梯度 $ƒ}
abla J(w)$ 来近似。
$$ w_{k+1} = w_k - a_k \widehat{\nabla J}(w_k) $$
这就是深度学习中天天用的 SGD。

5. 深度思考：学习率 (Step Size) 的哲学
在 Robbins-Monro 条件中，要求 $\sum a_k = \infty$ 且 $\sum a_k^2 &lt; \infty$（例如 $a_k = 1/k$）。这意味着学习率必须逐渐衰减到 0。
为什么？
因为我们的观测 $g(w) + \eta$ 是含噪的。如果步长不缩小，即便我们到了最优解附近，噪声 $\eta$ 也会推着我们来回震荡，永远无法稳定。衰减步长是为了抵消噪声的方差。
但是，在 Deep RL (如 DQN) 中，我们通常用常数学习率 (Fixed Learning Rate)，如 1e-4。为什么违反理论？
因为 RL 的环境往往是 Non-stationary (非平稳) 的：

Target 在变：在 DQN 中，Target Network 会定期更新，我们追逐的目标 $y$ 本身就在变。
分布在变：随着策略 $\pi$ 变好，Agent 访问的状态分布 $d_\pi(s)$ 也在变。

如果我们让 $\alpha \to 0$，Agent 就会失去**“适应变化”**的能力（Tracking ability）。
结论：

理论收敛：需要 $\alpha \to 0$。
实际工程：使用小的常数 $\alpha$，在“收敛精度”和“追踪速度”之间做 Trade-off。这导致结果会在最优解附近形成一个“震荡区”，而无法精准停在一点。


6. Python 实战：RM 算法找根
我们来模拟一个场景：
求解方程 $g(w) = 	anh(w - 1) = 0$ (解显然是 $w=1$)。
但我们观测时有高斯噪声。
import numpy as npimport matplotlib.pyplot as pltdef true_function(w):    return np.tanh(w - 1)def noisy_observation(w):    return true_function(w) + np.random.normal(0, 0.5) # 添加噪声def robbins_monro(w_init, num_iter):    w = w_init    w_history = [w]        for k in range(1, num_iter + 1):        # 满足 RM 条件的步长: a_k = 1/k        # 实际 RL 中常设为固定小常数 (如 0.01)，牺牲收敛性换取非平稳适应性        a_k = 1.0 / k                 # 获取含噪观测        measurement = noisy_observation(w)                # RM 更新        w = w - a_k * measurement        w_history.append(w)            return w_history# 运行模拟w_hist = robbins_monro(w_init=-5.0, num_iter=1000)print(f&quot;Final Estimate: &#123;w_hist[-1]:.4f&#125;&quot;) # 应该接近 1.0

6. RL 中的应用 (Preview)
在下一章 TD Learning 中，我们将看到这样的公式：
$$ V(S_t) ← V(S_t) + α [ (R + γ V(S_{t+1})) - V(S_t) ] $$
对比 RM 公式 $w ← w - α rig(w)$，你会发现：

$w$ 就是 $V(S_t)$。
$rig(w)$ 就是 $V(S_t) - (R + γ V(S_{t+1}))$（预测误差）。
我们的目标就是让这个误差期望为 0，即 $V(S_t) = ℕ[R + γ V(S_{t+1})]$ (Bellman Equation)。

结论：TD Learning 本质上就是用 RM 算法来解 Bellman Equation！

上一章：第5章 - 蒙特卡洛方法 | 下一章：第7章 - 时序差分方法 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>SGD</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Math</tag>
        <tag>Python</tag>
        <tag>Robbins-Monro</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第3章：最优状态值与贝尔曼最优方程 (Bellman Optimality)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/03_Bellman_Optimality/</url>
    <content><![CDATA[1. 终极目标：寻找最优策略
在第2章中，我们学会了“给定一个策略，评估它好不好（Policy Evaluation）”。但这只是第一步，RL 的终极目标是找到最好的那个策略（Optimal Policy, $\pi^*$）。
什么叫“最好”？
数学定义：如果策略 $\pi^$ 的状态价值 $v_{\pi^}(s)$ 在每一个状态 $s$ 上都不低于其他任何策略 $\pi$ 的价值 $v_\pi(s)$，那么 $\pi^$ 就是最优策略。
$$ v_{\pi^}(s) \ge v_\pi(s), \quad \forall s \in \mathcal{S}, \forall \pi $$
直观理解：
最优策略就像一个完美的导航仪，无论你现在身处何地（哪怕是被风吹到了错误的格子），它都能告诉你接下来该怎么走才能获得最大的未来回报。

2. 贝尔曼最优方程 (Bellman Optimality Equation, BOE)
2.1 从 Bellman Equation 到 BOE
回忆普通贝尔曼方程：
$$ v_\pi(s) = \sum_{a} \pi(a|s) \underbrace{\left( \sum_{s’} p(s’|s,a) [ r + \gamma v_\pi(s’) ] \right)}{q\pi(s,a)} $$
如果我们想要 $v(s)$ 最大，那么在选择动作 $a$ 时，就不应该犹豫，而应该只选 Q 值最大的那个动作。
于是，求和号 $\sum_a \pi(a|s)$ 变成了取最大值 $\max_a$：
$$ v^(s) = \max_{a \in \mathcal{A}} \underbrace{\sum_{s’} p(s’|s,a) [ r(s,a,s’) + \gamma v^(s’) ]}_{q^*(s,a)} $$
这就是 Bellman Optimality Equation (BOE)。它描述了最优状态价值 $v^*$ 必须满足的递归关系。
2.2 为什么 BOE 很难解？
对比一下：

Bellman Equation (BE): $v = R_\pi + \gamma P_\pi v$。这是线性方程，可以直接 $v = (I - \gamma P)^{-1} R$。
Bellman Optimality Equation (BOE): $v = \max (R + \gamma P v)$。这里有个 $\max$ 操作。

$\max(a+b) \ne \max(a) + \max(b)$。
$\max$ 是非线性的。
结论：我们不能用矩阵求逆来解 BOE！必须使用迭代算法（Value Iteration）。




3. 压缩映射定理 (Contraction Mapping Theorem)
这是 RL 中最硬核的数学理论之一，它保证了 Value Iteration 算法一定会收敛。
3.1 什么是压缩映射？
把 BOE 的右边看作一个算子（Operator）$f$：
$$ f(v) = \max_a (R_a + \gamma P_a v) $$
如果能证明对于任意两个向量 $v_1, v_2$，经过 $f$ 变换后，它们的距离变小了：
$$ | f(v_1) - f(v_2) |\infty \le \gamma | v_1 - v_2 |\infty $$
因为 $\gamma &lt; 1$，所以 $f$ 是一个压缩映射。
3.2 不动点 (Fixed Point)
根据 Banach Fixed Point Theorem：

存在性：$v^* = f(v^*)$ 一定存在。
唯一性：$v^*$ 是唯一的。
收敛性：任意初始化 $v_0$，只要不断算 $v_{k+1} = f(v_k)$，最终一定收敛到 $v^*$。

简单人话：你随便猜一个 Value Table，只要不断用“贪心”的方式更新它，最后它一定会变成真正的最优 Value Table。
3.3 深度思考：压缩映射的物理意义
为什么 $\gamma &lt; 1$ 就能保证压缩？
$$ | v_{k+1} - v^* | \le \gamma | v_k - v^* | $$
这表示每次迭代，我们离真理的距离都会缩小 $\gamma$ 倍。
物理意义：
$\gamma$ 是折扣因子，它代表了我们对未来的**“遗忘速度”**。

如果 $\gamma \approx 0$，Agent 是超级近视眼，只看眼前。任何遥远的误差都影响不到现在，所以收敛极快。
如果 $\gamma \approx 1$，Agent 是远视眼，千秋万代都要考虑。远处的微小误差都会传递到现在，所以收敛很慢。
结论：压缩映射的本质是**“用时间的衰减来消除初始猜测的错误”**。


4. 最优策略的确定性
一个有趣且重要的结论：对于有限 MDP，总是存在一个确定性的最优策略（Deterministic Optimal Policy）。
这意味着最优策略不需要随机扔骰子（不需要 $\pi(a|s) = 0.5$）。
$$ \pi^(s) = \arg\max_{a} q^(s, a) $$
在每个路口，只要坚定地走 Q 值最大的那条路就行了。如果有多个动作 Q 值一样大，随便选一个确定的即可。

5. Python 代码实战：验证最优性
在第2章的代码中，我们计算了一个随机策略的 Value。现在我们来“手动”算一下最优策略。
import numpy as np# 假设环境参数 (同 Chapter 2)# s1, s2, s3, s4# 只有 s1 面临选择：# Action 1 (去 s1/s2): r=-1# Action 2 (去 s3): r=0 (假设有一条捷径)# ... (定义 P 和 R) ...def calculate_optimal_value(max_iter=1000, gamma=0.9):    # 初始化 v    v = np.zeros(4)        for k in range(max_iter):        v_old = v.copy()                # 对 s1: max( q(s1, a1), q(s1, a2) )        # q(s1, a1) = -1 + gamma * (0.5*v[0] + 0.5*v[1])        # q(s1, a2) = 0 + gamma * v[2]                q_s1_a1 = -1.0 + gamma * (0.5 * v[0] + 0.5 * v[1])        q_s1_a2 = 0.0 + gamma * v[2] # 假设 a2 直接到 s3                v[0] = max(q_s1_a1, q_s1_a2) # BOE 的核心 max                # 其他状态假设不动        v[1] = 0 + gamma * v[3]        v[2] = 1 + gamma * v[3]        v[3] = 0 # 终点                if np.max(np.abs(v - v_old)) &lt; 1e-6:            print(f&quot;Converged at step &#123;k&#125;&quot;)            break                return vv_star = calculate_optimal_value()print(&quot;Optimal State Values:&quot;, v_star)
通过这段代码逻辑，我们可以看到 Value Iteration 的雏形：算 Q 值 -&gt; 取 Max -&gt; 更新 V。

6. 总结
本章解决了 RL 的核心目标问题：

定义最优：$v^*(s) \ge v_\pi(s)$。
方程描述：$v^* = \max (R + \gamma P v^*)$。
求解难点：非线性，无法矩阵求逆。
理论保障：压缩映射定理保证了迭代法的收敛性。

这为下一章正式介绍 Value Iteration (VI) 和 Policy Iteration (PI) 算法铺平了道路。

上一章：第2章 - 贝尔曼方程 | 下一章：第4章 - 值迭代与策略迭代 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Math</tag>
        <tag>Python</tag>
        <tag>Bellman Optimality</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第7章：时序差分方法 (Temporal-Difference Methods)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/07_Temporal_Difference/</url>
    <content><![CDATA[1. TD：结合 MC 与 DP 的艺术
时序差分 (TD) 是强化学习中最核心、最独特、最优雅的思想。它不需要像 DP 那样已知模型，也不需要像 MC 那样等到 Episode 结束。它边走边学 (Bootstrapping from Experience)。
核心公式 (TD(0)):
$$ V(S_t) &lt; V(S_t) + α [  R_{t+1} + γ V(S_{t+1})  - V(S_t) ] 
$$
直观解释：

Target: 我走了这一步，拿到了 $R_{t+1}$，加上我对下一步状态价值的预估 $γ V(S_{t+1})$，我觉得这才是更靠谱的价值。
Error: 那个 Target 和我原来的估计 $V(S_t)$ 之间的差，就是惊讶程度 (TD Error)。
Update: 用这个差乘以学习率 $α$ 来修正我的观念。


2. Sarsa: On-policy 的稳健派
2.1 算法流程 (State-Action-Reward-State-Action)
Sarsa 之所以叫 Sarsa，是因为它一次更新需要的数据五元组是 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$。
$$ Q(S_t, A_t) &lt; Q(S_t, A_t) + α [ R_{t+1} + γ Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]
$$
关键点：$A_{t+1}$ 是 Agent 实际将在下一步执行的动作（通常由 $ε$-greedy 选出）。这意味着 Sarsa 在优化的是它正在执行的那个策略（On-policy）。如果策略比较“浪”（$ε$ 很大），Sarsa 就会学得比较保守，远离悬崖。
2.2 Python 实现 (Sarsa)
def sarsa(env, num_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):    Q = defaultdict(lambda: np.zeros(env.action_space.n))        for i in range(num_episodes):        state = env.reset()        action = epsilon_greedy(Q, state, epsilon) # 选动作 A                while True:            next_state, reward, done = env.step(action)            next_action = epsilon_greedy(Q, next_state, epsilon) # 选动作 A&#x27;                        # Sarsa 更新            td_target = reward + gamma * Q[next_state][next_action] * (not done)            Q[state][action] += alpha * (td_target - Q[state][action])                        if done: break            state, action = next_state, next_action                return Q

3. Q-learning: Off-policy 的激进派
3.1 算法流程
Q-learning 的更新公式只有一点点不同，但引起了质变：
$$ Q(S_t, A_t) &lt; Q(S_t, A_t) + α [ R_{t+1} + γ     Q(S_{t+1}, a) - Q(S_t, A_t) ]
$$
关键点：不管下一步实际做什么，我在计算 Target 时都假设我下一步会做最好的动作 ($    Q$ )。
这意味着 Q-learning 学习的是最优策略 (Off-policy)，哪怕它实际行为策略是在乱走。
3.2 Sarsa vs Q-learning (悬崖漫步例子)
视频中经典的 Cliff Walking 例子完美展示了区别：

环境: 走到悬崖边掉下去会得 -100 分。
Q-learning: 学出一条紧贴悬崖边的最短路径（它假设自己是最优的，不会手滑）。但在训练时（因为有 $ε$），它经常掉下去。
Sarsa: 学出一条远离悬崖的安全路径。因为它知道自己有 $ε$ 的概率乱走，走悬崖边太危险了。

3.3 Python 实现 (Q-learning)
def q_learning(env, num_episodes, alpha=0.1, gamma=0.9, epsilon=0.1):    Q = defaultdict(lambda: np.zeros(env.action_space.n))        for i in range(num_episodes):        state = env.reset()                while True:            action = epsilon_greedy(Q, state, epsilon) # 行为策略            next_state, reward, done = env.step(action)                        # Q-learning 更新：直接取 max，无视 next_action            best_next_action = np.argmax(Q[next_state])            td_target = reward + gamma * Q[next_state][best_next_action] * (not done)                        Q[state][action] += alpha * (td_target - Q[state][action])                        if done: break            state = next_state                return Q

4. 统一视角
我们可以把 MDP 中的各种算法看作是同一公式的不同变体：
$$ 	ext{NewEstimate} &lt; 	ext{OldEstimate} + 	ext{StepSize} 	imes [	ext{Target} - 	ext{OldEstimate}]
$$



算法
Target 是什么？
样本来源




MC
$G_t$ (真实回报)
一整条 Episode


DP (VI)
$[R + γ V(S’)]$
遍历所有 $S’$ (模型计算)


Sarsa
$R + γ Q(S’, A’)$
采样一步 $(S, A, R, S’, A’)$


Q-learning
$R + \gamma \max Q(S’, a)$
采样一步 $(S, A, R, S’)$




5. 深度思考：偏差与方差的权衡 (Bias-Variance Trade-off)
为什么我们要发明 TD？为什么不直接用 MC？这就涉及机器学习核心的偏差 (Bias) 与 方差 (Variance) 权衡。
5.1 MC (Monte Carlo)

Target: $G_t$ (真实回报)。
无偏差 (Unbiased): $G_t$ 是 $v_\pi(s)$ 的无偏估计。
高方差 (High Variance): $G_t$ 依赖于从 $t$ 到结束的所有随机转移和奖励。路途越长，随机因素积累越多，波动越大。
后果: 即使数据很多，训练也很震荡，收敛慢。

5.2 TD (Temporal Difference)

Target: $R + \gamma V(S’)$ (估计值)。
有偏差 (Biased): 我们用了估计值 $V(S’)$ 来更新 $V(S)$。如果 $V(S’)$ 估计错了，Target 也就错了。
低方差 (Low Variance): Target 只依赖一步的随机性 $R_{t+1}$ 和 $S_{t+1}$。后面的随机性被 $V(S’)$ 概括了。
后果: 收敛快，但初始阶段可能跑偏（因为信了错误的估计）。

5.3 n-step TD：最佳平衡点
我们可以折中一下，看 $n$ 步再截断：
$$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}) $$

$n=1$: TD(0)，低方差，高偏差。
$n=\infty$: MC，高方差，无偏差。
结论: 调节 $n$ (或者使用 $\lambda$-return) 可以找到方差与偏差的最佳平衡点，通常 $n=3 \sim 5$ 效果最好。


6. 总结
本章我们掌握了 RL 的两把利器：

Sarsa: 稳健，适合在线控制，关心“活在当下”的风险。
Q-learning: 激进，适合模拟训练，直接瞄准“理论最优”。

但目前我们用的表格法 (Tabular Method) 有个致命弱点：状态空间太大存不下怎么办？ (比如像素点组成的图像)。
下一章，我们将引入 函数近似 (Function Approximation)，也就是 Deep RL 的前夜 —— DQN 的基础。

上一章：第6章 - 随机近似 | 下一章：第8章 - 值函数近似 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Python</tag>
        <tag>TD Learning</tag>
        <tag>Sarsa</tag>
        <tag>Q-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第8章：值函数近似 (Value Function Approximation)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/08_Value_Function_Approximation/</url>
    <content><![CDATA[1. 为什么需要函数近似？
在前面的章节中，我们都假设使用表格 (Table) 来存储 Value Function：

$v(s)$ 是一个长度为 $|∘|$ 的向量。
$q(s, a)$ 是一个大小为 $|∘| \times |∘|$ 的矩阵。

致命问题：

状态空间爆炸：围棋的状态数是 $10^{170}$，任何计算机都存不下这个表。
泛化能力差：如果是连续状态（比如机器人的位置坐标），你没见过的坐标就没有 Value，表格法无法处理“相似的状态”。

解决方案：用一个函数 $f(s, w)$ 来近似 $v(s)$。
$$ \hat{v}(s, w) \approx v_\pi(s) $$
其中 $w$ 是参数（比如神经网络的权重）。这样我们只需要存储 $w$，通常 $w$ 的维度远小于状态数。

2. 目标函数与优化
我们希望 $\hat{v}(s, w)$ 越准越好。定义目标函数（损失函数）：
$$ J(w) = \mathbb{E}{\pi} \left[ (v\pi(S) - \hat{v}(S, w))^2 \right] $$
使用 SGD 进行更新：
$$ w \leftarrow w + \alpha [v_\pi(s) - \hat{v}(s, w)] \nabla_w \hat{v}(s, w) $$
但问题是：我们不知道真实的 $v_\pi(s)$。
办法：用 TD Target 代替 $v_\pi(s)$。
$$ w \leftarrow w + \alpha [ \underbrace{(R + \gamma \hat{v}(s’, w))}_{\text{TD Target}} - \hat{v}(s, w) ] \nabla_w \hat{v}(s, w) $$
这叫 Semi-gradient，因为我们在求导时，忽略了 TD Target 对 $w$ 的依赖（把 Target 当常数看）。

3. Deep Q-Network (DQN)
当 $\hat{v}$ 是一个深度神经网络时，这就是 Deep RL。
DQN 是将 Q-learning 与神经网络结合的里程碑算法（DeepMind, Nature 2015）。
3.1 核心创新
为了解决 Deep RL 训练不稳定的问题，DQN 引入了两大通过：

Experience Replay (经验回放)：

把 $(s, a, r, s’)$ 存进一个 Buffer。
训练时随机采样 Batch。这打破了数据的时间相关性（Correlation），让数据分布更像 i.i.d.，稳定神经网络训练。


Target Network (目标网络)：

在计算 TD Target 时，使用一个参数固定的旧网络 $Q(s’, a; w^-)$。
$y = r + \gamma \max_{a’} Q(s’, a’; w^-)$。
这避免了“自己追自己”（Chasing a moving target）的震荡问题。



3.2 Python 代码实战：DQN 核心逻辑
import torchimport torch.nn as nnimport torch.optim as optimimport randomfrom collections import dequeclass QNetwork(nn.Module):    def __init__(self, state_dim, action_dim):        super(QNetwork, self).__init__()        self.fc = nn.Sequential(            nn.Linear(state_dim, 64),            nn.ReLU(),            nn.Linear(64, 64),            nn.ReLU(),            nn.Linear(64, action_dim)        )        def forward(self, x):        return self.fc(x)class DQNAgent:    def __init__(self, state_dim, action_dim):        self.q_net = QNetwork(state_dim, action_dim)        self.target_net = QNetwork(state_dim, action_dim)        self.target_net.load_state_dict(self.q_net.state_dict()) # 初始化相同                self.optimizer = optim.Adam(self.q_net.parameters(), lr=0.001)        self.memory = deque(maxlen=10000) # Replay Buffer        self.batch_size = 32        self.gamma = 0.99            def update(self):        if len(self.memory) &lt; self.batch_size: return                # 1. Experience Replay        batch = random.sample(self.memory, self.batch_size)        states, actions, rewards, next_states, dones = zip(*batch)                states = torch.tensor(states, dtype=torch.float32)        actions = torch.tensor(actions, dtype=torch.long)        rewards = torch.tensor(rewards, dtype=torch.float32)        next_states = torch.tensor(next_states, dtype=torch.float32)        dones = torch.tensor(dones, dtype=torch.float32)                # 2. 计算 Q_current        q_values = self.q_net(states)        q_current = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)                # 3. 计算 Q_target (使用 Target Network)        with torch.no_grad():            next_q_values = self.target_net(next_states)            max_next_q = next_q_values.max(1)[0]            q_target = rewards + (1 - dones) * self.gamma * max_next_q                    # 4. 梯度下降        loss = nn.MSELoss()(q_current, q_target)        self.optimizer.zero_grad()        loss.backward()        self.optimizer.step()            def sync_target(self):        self.target_net.load_state_dict(self.q_net.state_dict())

4. 总结
本章我们迈出了从“玩具问题”到“实际应用”的关键一步。

Function Approximation 让 RL 能处理无限状态。
DQN 通过 Replay Buffer 和 Target Network 解决了非线性近似的不稳定性。

至此，Value-based 方法（学习 $Q$ 值）已经讲完了。但还有一类问题很难处理：连续动作空间（比如机器人关节角度）。这就需要 Policy-based 方法，直接学习策略函数 $\pi(a|s)$。
下一章：策略梯度方法 (Policy Gradient Methods)。

上一章：第7章 - 时序差分方法 | 下一章：第9章 - 策略梯度方法 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Python</tag>
        <tag>Deep Learning</tag>
        <tag>DQN</tag>
        <tag>Function Approximation</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第9章：策略梯度方法 (Policy Gradient Methods)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/09_Policy_Gradient/</url>
    <content><![CDATA[1. 为什么要直接优化策略？
Value-based 方法（如 DQN）虽然强大，但有局限：

无法处理连续动作：如果动作是“方向盘转动角度（0度到360度）”，Q-learning 需要求 $\arg\max_a Q(s, a)$，这在连续空间是很难的。
无法学习随机策略：Value-based 最终给出的通常是确定性策略（选 Q 最大的）。但在石头剪刀布游戏中，最好的策略是随机的。

Policy Gradient (PG) 方法直接对策略 $\pi(a|s, \theta)$ 进行参数化（比如用神经网络），然后通过调整 $\theta$ 来最大化期望回报 $J(\theta)$。

2. 策略梯度定理 (Policy Gradient Theorem)
我们想最大化目标函数 $J(\theta) = \mathbb{E}[G_0]$。
我们需要求梯度 $\nabla J(\theta)$ 来进行梯度上升（Gradient Ascent）。
神奇的 Policy Gradient Theorem 告诉我们：
$$ \nabla J(\theta) \propto \sum_{s} d_\pi(s) \sum_{a} q_\pi(s, a) \nabla_\theta \pi(a|s, \theta) $$
利用 Log-Derivative Trick ($\nabla \pi = \pi \nabla \log \pi$)，可以写成期望形式：
$$ \nabla J(\theta) = \mathbb{E}{\pi} [ \nabla\theta \ln \pi(A_t | S_t, \theta) \cdot Q_\pi(S_t, A_t) ] $$
直观解释：

$\nabla \ln \pi$：告诉我们如何调整参数能让动作 $A_t$ 的概率变大。
$Q(S_t, A_t)$：这个动作好不好？
乘起来：如果动作好（Q大），就多增加它的概率；如果不好（Q小），就少增加甚至减少（如果 Q 是负的）。


3. REINFORCE 算法
REINFORCE 是最基础的 PG 算法。它使用 Monte Carlo 方法来估计 $Q(S_t, A_t)$，即直接用实际回报 $G_t$ 代替 $Q$。
更新公式:
$$ \theta \leftarrow \theta + \alpha \gamma^t G_t \nabla_\theta \ln \pi(A_t | S_t, \theta) $$
3.1 缺点
由于 $G_t$ 是单次采样的回报，方差极大。这导致 REINFORCE 训练非常不稳定。
改进：引入 Baseline，减去一个基线 $b(s)$（通常是 $V(s)$），变成 $G_t - V(S_t)$，这不改变期望但能显著降低方差。

4. PyTorch 代码实战：REINFORCE
我们用 PyTorch 实现一个简单的 REINFORCE 来玩 CartPole。
import torchimport torch.nn as nnimport torch.optim as optimfrom torch.distributions import Categoricalclass PolicyNetwork(nn.Module):    def __init__(self, state_dim, action_dim):        super(PolicyNetwork, self).__init__()        self.fc = nn.Sequential(            nn.Linear(state_dim, 128),            nn.ReLU(),            nn.Linear(128, action_dim),            nn.Softmax(dim=-1) # 输出概率分布        )            def forward(self, x):        return self.fc(x)class REINFORCE:    def __init__(self, state_dim, action_dim):        self.policy = PolicyNetwork(state_dim, action_dim)        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-3)        self.gamma = 0.99        self.log_probs = []        self.rewards = []            def select_action(self, state):        state = torch.from_numpy(state).float().unsqueeze(0)        probs = self.policy(state)        m = Categorical(probs) # 创建分布        action = m.sample()    # 采样动作                # 保存 log_prob 用于后续求导: ln π(a|s)        self.log_probs.append(m.log_prob(action))        return action.item()            def update(self):        R = 0        policy_loss = []        returns = []                # 1. 计算每个时刻的 Return G_t        for r in self.rewards[::-1]:            R = r + self.gamma * R            returns.insert(0, R)                    returns = torch.tensor(returns)        # 归一化 returns 可以稳定训练        returns = (returns - returns.mean()) / (returns.std() + 1e-9)                # 2. 计算损失: - sum( log_prob * G_t )        # 注意: 我们要最大化 J，也就是最小化 -J        for log_prob, R in zip(self.log_probs, returns):            policy_loss.append(-log_prob * R)                    # 3. 梯度下降        self.optimizer.zero_grad()        policy_loss = torch.cat(policy_loss).sum()        policy_loss.backward()        self.optimizer.step()                # 清空数据        self.log_probs = []        self.rewards = []

5. 总结
本章我们解锁了 Policy-based 方法。

Policy Gradient Theorem 是核心基石。
REINFORCE 是最简单的实现，虽然方差大，但它展示了 PG 的基本逻辑。

REINFORCE 的方差问题如何彻底解决？
答案是结合 Value-based 和 Policy-based 的优点，这就引出了下一章的终极形态：Actor-Critic 方法。

上一章：第8章 - 值函数近似 | 下一章：第10章 - Actor-Critic 方法 &gt;&gt;
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>REINFORCE</tag>
        <tag>Policy Gradient</tag>
        <tag>Reinforcement Learning</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习数学原理 - 第10章：Actor-Critic 方法 (Actor-Critic Methods)</title>
    <url>/2026/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/10_Actor_Critic/</url>
    <content><![CDATA[1. 演员与评论家 (Actor and Critic)
在第9章中，我们学习了 Policy Gradient (如 REINFORCE)。它虽然能处理连续动作，但方差极大，因为它用整条 Episode 的回报 $G_t$ 来评估一个动作的好坏。
在第7章中，我们学习了 TD Learning (如 Q-learning)。它方差小（单步更新），但很难处理连续动作。
Actor-Critic (AC) 结合了这两者的优点：

Actor (策略网络)：$\pi(a|s, \theta)$。负责选动作，像个演员。
Critic (价值网络)：$q(s, a, w)$ 或 $v(s, w)$。负责给动作打分，像个评论家。

核心逻辑：
Actor 不再盲目等待环境的随机回报 $G_t$，而是听从 Critic 的评价。Critic 说这个动作好（TD Error &gt; 0），Actor 就增加概率；Critic 说不好，就减少概率。

2. 优势函数 (Advantage Function)
REINFORCE 的更新公式：
$$ \nabla J \approx \mathbb{E} [ \nabla \ln \pi(a|s) \cdot G_t ] $$
Actor-Critic 的更新公式：
$$ \nabla J \approx \mathbb{E} [ \nabla \ln \pi(a|s) \cdot Q_w(s, a) ] $$
为了进一步降低方差，我们引入 Advantage Function $A(s, a) = Q(s, a) - V(s)$。
即：动作 $a$ 的价值，相对于当前状态平均价值 $V(s)$ 的优势。
$$ \nabla J \approx \mathbb{E} [ \nabla \ln \pi(a|s) \cdot (R + \gamma V_w(s’) - V_w(s)) ] $$
这里我们用 TD Error $\delta = R + \gamma V(s’) - V(s)$ 来近似 Advantage。这样我们只需要一个网络估计 $V(s)$ 即可。这就是 A2C (Advantage Actor-Critic)。

3. A2C 算法架构
A2C 通常共享神经网络的前几层（特征提取层），然后分叉出两个头：

Policy Head: 输出动作概率分布（Softmax）。
Value Head: 输出状态价值（Scalar）。

损失函数:

Actor Loss: $-\log \pi(a|s) \cdot \delta$ (最大化 Advantage)。
Critic Loss: $\delta^2 = (R + \gamma V(s’) - V(s))^2$ (最小化 TD Error)。
Entropy Loss: $-\sum \pi \log \pi$ (可选，鼓励探索，防止过早收敛)。

$$ Loss = Loss_{Actor} + 0.5 \cdot Loss_{Critic} - 0.01 \cdot Loss_{Entropy} $$

4. PyTorch 代码实战：A2C
import torchimport torch.nn as nnimport torch.optim as optimfrom torch.distributions import Categoricalclass ActorCritic(nn.Module):    def __init__(self, state_dim, action_dim):        super(ActorCritic, self).__init__()        # 共享特征层        self.affine = nn.Linear(state_dim, 128)                # Actor Head        self.action_layer = nn.Linear(128, action_dim)                # Critic Head        self.value_layer = nn.Linear(128, 1)            def forward(self, x):        x = torch.relu(self.affine(x))                # Actor 输出概率        action_probs = torch.softmax(self.action_layer(x), dim=-1)                # Critic 输出 Value        state_value = self.value_layer(x)                return action_probs, state_valuedef train_a2c(env, model, optimizer, gamma=0.99):    state = env.reset()    log_probs = []    values = []    rewards = []        # 1. 收集轨迹 (这里简化为单步更新或短轨迹)    # ... (省略循环，假设收集了一段数据) ...    # 假设 action_probs, state_value = model(state)        # 2. 计算 Returns 和 Advantages    R = 0    returns = []    for r in rewards[::-1]:        R = r + gamma * R        returns.insert(0, R)    returns = torch.tensor(returns)        # 3. 计算 Loss    # Advantage = Return - Value    # 注意：这里 detach() 很重要，Critic 的目标是逼近 Return，而不是让 Return 迁就 Value    advantages = returns - torch.stack(values).detach()        actor_loss = -(torch.stack(log_probs) * advantages).mean()    critic_loss = nn.MSELoss()(torch.stack(values), returns.unsqueeze(1))        loss = actor_loss + 0.5 * critic_loss        # 4. 更新    optimizer.zero_grad()    loss.backward()    optimizer.step()

5. 进阶：从 A2C 到 PPO
A2C 是现代 Policy-based 方法的基石。但它有个缺点：步长 $\alpha$ 难调。

步长太大：策略更新过猛，导致性能崩塌（Policy Collapse）。
步长太小：训练太慢。

PPO (Proximal Policy Optimization) 通过限制策略更新的幅度（Clip 操作），保证了每次更新都在一个“信任区域”内，是目前最流行的 Deep RL 算法（OpenAI 默认算法）。

6. 全系列总结
恭喜你！你已经走完了《强化学习数学原理》的全部旅程：

MDP: 一切的起点，定义了状态、动作、奖励。
Bellman Eq: 评估策略好坏的标尺。
VI/PI: 有模型时的上帝视角解法。
MC: 无模型，从经验中学习（采样）。
TD (Sarsa/Q-learning): 边走边学，结合了 DP 和 MC。
Function Approx (DQN): 征服无限状态空间。
Policy Gradient: 征服连续动作空间。
Actor-Critic: 集大成者，迈向现代 RL。

强化学习的数学大厦已经建成。接下来，就是去实践，去解决真正的问题！

上一章：第9章 - 策略梯度方法 | 返回首页
]]></content>
      <categories>
        <category>强化学习数学原理</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Python</tag>
        <tag>PyTorch</tag>
        <tag>Actor-Critic</tag>
        <tag>A2C</tag>
      </tags>
  </entry>
</search>
