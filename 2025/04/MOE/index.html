<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>MoE | WPIRONMAN</title><meta name="author" content="WP"><meta name="copyright" content="WP"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Mixtures of Experts 《Adaptive Mixture of Local Experts》  论文链接：https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;jjnh91.pdf 1991年，由 Hinton和 Jordan提出，这是最早的MoE架构。 核心思想：通过多个独立专家网络处理输入数据不同子集，并由门控网络动态选择专家。每个专家接受相同的输"><meta property="og:type" content="article"><meta property="og:title" content="MoE"><meta property="og:url" content="https://wp-a.github.io/2025/04/MOE/index.html"><meta property="og:site_name" content="WPIRONMAN"><meta property="og:description" content="Mixtures of Experts 《Adaptive Mixture of Local Experts》  论文链接：https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;jjnh91.pdf 1991年，由 Hinton和 Jordan提出，这是最早的MoE架构。 核心思想：通过多个独立专家网络处理输入数据不同子集，并由门控网络动态选择专家。每个专家接受相同的输"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502161913940.png"><meta property="article:published_time" content="2025-04-27T12:37:24.000Z"><meta property="article:modified_time" content="2026-02-26T01:43:08.433Z"><meta property="article:author" content="WP"><meta property="article:tag" content="论文精读"><meta property="article:tag" content="混合专家系统"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502161913940.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MoE",
  "url": "https://wp-a.github.io/2025/04/MOE/",
  "image": "https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502161913940.png",
  "datePublished": "2025-04-27T12:37:24.000Z",
  "dateModified": "2026-02-26T01:43:08.433Z",
  "author": [
    {
      "@type": "Person",
      "name": "WP",
      "url": "https://wp-a.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="https://wpironman.oss-cn-qingdao.aliyuncs.com/favicon.png"><link rel="canonical" href="https://wp-a.github.io/2025/04/MOE/index.html"><link rel="preconnect" href="//cdnjs.cloudflare.com"><link rel="manifest" href="/null"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css"><script>(()=>{var e={set:(e,t,a)=>{a&&(a=Date.now()+864e5*a,localStorage.setItem(e,JSON.stringify({value:t,expiry:a})))},get:e=>{var t=localStorage.getItem(e);if(t){var{value:t,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return t;localStorage.removeItem(e)}}},t=(window.btf={saveToLocal:e,getScript:(o,n={})=>new Promise((e,t)=>{let a=document.createElement("script");a.src=o,a.async=!0,Object.entries(n).forEach(([e,t])=>a.setAttribute(e,t)),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),getCSS:(o,n)=>new Promise((e,t)=>{let a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),addGlobalFn:(e,t,a=!1,o=window)=>{var n=o.globalFn||{};n[e]=n[e]||{},n[e][a||Object.keys(n[e]).length]=t,o.globalFn=n}},()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")}),a=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")},o=(btf.activateDarkMode=t,btf.activateLightMode=a,e.get("theme")),t=("dark"===o?t():"light"===o&&a(),e.get("aside-status"));void 0!==t&&document.documentElement.classList.toggle("hide-aside","hide"===t);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Sans+SC:wght@400;500;600&amp;family=JetBrains+Mono:wght@400;500&amp;family=Roboto+Slab:wght@400;600;700&amp;display=swap" media="print" onload='this.media="all"'><script>let GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdnjs.cloudflare.com/ajax/libs/egjs-infinitegrid/4.12.0/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyloadPlugin:!1,isAnchor:!0,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"MoE",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="/css/_custom/category/categories.css"><link rel="preconnect" href="https://fonts.loli.net" crossorigin><link rel="stylesheet" href="/css/categories.css"><link rel="stylesheet" href="/css/valine.css"><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/medium-style.css"><script>window.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("my-pv");e&&fetch("https://wpironman.top/pv").then(t=>t.json()).then(t=>{e.textContent=t.pv}).catch(()=>{e.textContent="获取失败"})})</script><link rel="preload" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" as="style" onload='this.rel="stylesheet"'><link href="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" as="image" crossorigin="anonymous"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image:url(https://wpironman.oss-cn-qingdao.aliyuncs.com/10year.webp)"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/head.png" onerror='this.onerror=null,this.src="https://wpironman.oss-cn-qingdao.aliyuncs.com/head.gif"' alt="avatar" loading='lazy'></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">168</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/construction-detection/"><i class="fa-fw fas fa-hard-hat"></i><span> 工地检测</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-user-group"></i><span> 友链</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fa-solid fa-user-tie"></i><span> 本站友链</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i><span> 随机开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="fa-fw fa fa-taxi"></i><span> 异次元之旅</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/academic/"><i class="fa-fw fas fa-graduation-cap"></i><span> 学术主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502161913940.png)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/favicon.png" alt="Logo" loading='lazy'><span class="site-name">WPIRONMAN</span></a><a class="nav-page-title" href="/"><span class="site-name">MoE</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/construction-detection/"><i class="fa-fw fas fa-hard-hat"></i><span> 工地检测</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-user-group"></i><span> 友链</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fa-solid fa-user-tie"></i><span> 本站友链</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i><span> 随机开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="fa-fw fa fa-taxi"></i><span> 异次元之旅</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/academic/"><i class="fa-fw fas fa-graduation-cap"></i><span> 学术主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">MoE<a class="post-edit-link" href="null_posts/MOE.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-04-27T12:37:24.000Z" title="发表于 2025-04-27 20:37:24">2025-04-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">论文精读</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>Mixtures of Experts</h1><h2 id="《Adaptive-Mixture-of-Local-Experts》">《Adaptive Mixture of Local Experts》</h2><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501100302213.png" style="zoom:50%" loading='lazy'><p>论文链接：<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf">https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf</a></p><p>1991年，由 Hinton和 Jordan提出，这是最早的MoE架构。</p><p>核心思想：通过多个独立专家网络处理输入数据不同子集，并由门控网络动态选择专家。<strong>每个专家接受相同的输入数据</strong>，但通过门控网络的动态分配，专家会专注于处理输入空间的特定区域。</p><h3 id="基础架构">基础架构</h3><p>如图，一个由专家网络和门控网络组成的系统。每个专家是一个前馈网络，所有专家接收相同的输入，并具有相同数量的输出。门控网络也是一个前馈网络，通常接收与专家网络相同的输入。它的输出是归一化的 $ p_j = \exp(r_j) / \sum_i \exp(r_i) $，其中 $ r_j $是门控网络输出单元 $j$ 接收的总加权输入。选择器（selector）类似于一个多输入单输出的随机开关；开关选择来自专家 $ j $ 的输出的概率为 $p_j$ 。每个专家通常只会被分配到可能输入向量空间的一个小区域内。</p><p>系统由多个专家网络和一个门控网络组成。每个专家是一个前馈网络，处理特定子任务；门控网络根据输入决定每个专家的混合比例（概率）。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501113347385.png" style="zoom:50%" loading='lazy'><p>通过重新定义误差函数，鼓励专家竞争而非协作，确保每个专家专注于特定子任务。传统误差函数（如均方误差）会导致专家协作，增加耦合；论文提出优比损失（基于高斯混合模型的负对数概率），使专家独立学习，减少干扰。改进后的误差函数使门控网络倾向于选择最适合的专家，加快收敛。</p><h3 id="性能比较">性能比较</h3><p>元音辨别：区分多说话者元音区分（识别元音 [i], [I], [a], [A]）。</p><p><strong>数据集</strong>：来自 75 个说话者的共振峰数据（Peterson 和 Barney, 1952），前 50 个用于训练，后 25 个用于测试。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501110027054.png" style="zoom:60%" loading='lazy'><p>模型能自动分解任务，专注于不同类别对（如 [i]/[I] 和 [a]/[A]），仅 2-3 个专家在最终混合中活跃。</p><p>混合专家模型达到误差标准明显快于反向传播网络，平均只需要大约一半的周期数。混合模型的学习时间也随着专家数量的增加而很好地扩展。混合专家模型具有较小但统计上显著的平均周期数优势。</p><h3 id="总结">总结</h3><p>这篇论文针对的问题是在不同场合执行不同任务会产生干扰，导致训练速度慢和泛化性能差。MoE 的核心思想是让专家专注于适合的子任务。这意味着某些专家的利用率较低（论文中提到最终只有 2-3 个专家活跃），后续的论文大多都是在解决这个问题。</p><p>论文展示了 MoE 模型的任务分解过程：初始阶段，门控网络给所有专家分配相等的混合比例，导致每个专家处理的案例数量大致相等，决策线趋向于处理所有案例的平均最优解。随着训练进行，竞争机制使专家分化，专注于特定子任务，从而形成更符合数据分布的最优决策面。这种机制显著减少了干扰，训练速度比反向传播网络快约 50%，体现了 MoE 架构在训练时间上的优势。</p><h2 id="《Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer》">《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》</h2><p>超大型神经网络：稀疏门控混合专家层</p><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06538">https://arxiv.org/abs/1701.06538</a></p><p>2017年，合作作者中还有Hinton和 Jordan，在LSTM层之间应用MoE卷积。仅以微小的计算效率损失就取得了超过 1000 倍的模型容量提升，并在公共语言建模和翻译数据集上显著提升了最先进的结果。</p><p>上一篇强调的是减小时间成本，这一篇是减小计算成本。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501141207768.png" style="zoom:40%" loading='lazy'><h3 id="基础架构-2">基础架构</h3><h4 id="稀疏门控专家混合层（MoE）">稀疏门控专家混合层（MoE）</h4><p>MoE 由很多专家组成，每个专家相当于一个前馈神经网络。以及一个可训练的门控网络，该网络选择专家的稀疏组合来处理每个输入。网络的所有部分都通过反向传播进行联合训练。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501165106468.png" style="zoom:30%" loading='lazy'><p>设 $ G(x) $和 $ E_i(x) $ 分别为给定输入 $ x $ 的门控网络输出和第 $ i $ 个专家网络的输出。MoE 模块的输出 $ y $ 可表示为：<br>$$<br>y = \sum_{i=1}^{n} G(x)_i E_i(x)<br>$$</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501192052114.webp" style="zoom:38%" loading='lazy'><h3 id="均衡负载">均衡负载</h3><p>门控网络总是倾向于收敛到一种状态，在这种状态下，它总是对同一批专家产生较大的权重，使得被偏好的专家训练得更快，更容易被门控网络选中。希望在训练和推理过程中，专家的重要性相等，称之为<strong>负载平衡</strong> 。某种程度上，这是为了防止对同一个专家过度拟合。</p><h4 id="Keep-Top-K">Keep Top-K</h4><p><strong>Noisy Top-K Gating</strong></p><p>添加了两个组件：<strong>稀疏性和噪音</strong>。在应用 Softmax 函数之前，添加可调的高斯噪声，然后仅保留前 $ k $ 个值，将其余值设为 $-\infty$（这会导致对应的门控值为 0）。每部分的噪声量由可训练权重矩阵 $ W_{\text{noise}} $ 控制。通过简单的反向传播训练门控网络。<strong>稀疏性用于节省计算，噪声项有助于负载均衡。</strong></p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250501192435690.webp" style="zoom:33%" loading='lazy'><h5 id="Noise">Noise</h5><p>使用Noisy Top-K Gating方法改进MoE层，引入可训练的 Gaussian 噪声防止总是选择相同的专家。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502135639110.webp" style="zoom:50%" loading='lazy'> $$ H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i) $$<h5 id="Sparse">Sparse</h5><p>除了想要激活的前 k 名专家之外，其他所有专家的权重都将设置为 $-\infty$ 。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502140128737.png" style="zoom:50%" loading='lazy'> $$ \text{KeepTopK}(v, k)_i = \begin{cases} v_i & \text{if } v_i \text{ is in the top } k \text{ elements of } v, \\\ -\infty & \text{otherwise}. \end{cases} $$ 通过将这些权重设置为 $-\infty$ ，SoftMax 在这些权重上的输出将产生概率 0 ： <img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502140429369.webp" style="zoom:50%" loading='lazy'> $$ G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k)) $$<p>KeepTopK 策略将每个 token 路由给几个选定的专家。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502140803358.webp" style="zoom:50%" loading='lazy'><h4 id="Auxiliary-Loss">Auxiliary Loss</h4><p>为了在训练过程中获得更均匀的专家分布，辅助损失（也称为<em>负载平衡损失</em> ）被添加到网络的常规损失中。它增加了一个约束，迫使专家具有同等重要性。</p><p>该辅助损失的第一个组成部分是将整个批次中每个专家的门控值相加。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502133715717.png" style="zoom:50%" loading='lazy'> $$ \text{Importance}(X) = \sum_{x \in X} G(x) $$ 这为我们提供了每个专家的重要性分数 ，该分数表示无论输入如何，选择特定专家的可能性。可以用它来计算变异系数 （ CV ），它告诉我们专家之间重要性得分的差异有多大。利用这个 CV 分数，我们可以在训练期间更新辅助损失 ，以尽可能降低 CV 分数（ 从而给予每个专家同等的重要性 ）。 <img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502134023343.webp" style="zoom:50%" loading='lazy'><p>如果重要性分数存在很大差异，则 CV 会很高，相反，如果所有专家的重要性得分都相似，那么 CV 就会较低</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502134637067.webp" style="zoom:50%" loading='lazy'> $$ L_{\text{importance}}(X) = w_{\text{importance}} \cdot CV(\text{Importance}(X))^2 $$ 最后，将辅助损失作为单独的损失添加，以便在训练期间进行优化。<p>虽然这种损失函数可以确保平衡的重要性，但专家仍然可能收到数量非常不同的样本。为了解决这一问题，论文引入了 $ L_{\text{load}} $ 损失，专门用于平衡专家接收的样本数量（即负载均衡），与 $ L_{\text{importance}} $ 损失（平衡门控权重总和）配合使用。</p><h4 id="Load-Balancing-Loss">Load-Balancing Loss</h4><p>专家接收的样本数量是一个离散值，无法用于反向传播。所以这里定义了一个平滑估计器 $Load(X)$，用于估计每个专家在输入批次 $X$ 中分配到的示例数量，通过概率计算来近似样本分配。平滑性使得可以通过估计器反向传播梯度。这是门控函数中噪声项的目的。</p><p>对于一个输入批次 $ X $，第 $ i $ 个专家的负载定义为：<br>$$ {Load}(X)_i = \sum_{x \in X} P(x, i) $$<br>其中 $ P(x, i) $ 是给定输入 $ x $ 时第 $ i $ 个专家被选中的概率，它描述了第 $ i$ 个专家的“带噪声得分”大于某个阈值的概率。论文通过噪声 Top-K 门控的特性计算 $ P(x, i) $：<br>$$ P(x, i) = \Pr\left( (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i) > k{th\_excluding}(H(x), k, i) \right) $$<br><strong>$ (x \cdot W_g)_i $</strong>：</p><ul><li>$x $ 是输入向量（例如来自上一层的 LSTM 输出）。</li><li>$ W_g $ 是门控网络的可训练权重矩阵。</li><li>$ (x \cdot W_g)_i $ 是第 $ i $ 个专家的原始得分（未添加噪声），表示门控网络对第 $ i $ 个专家的“偏好”。</li></ul><p><strong>$ StandardNormal()⋅Softplus((x⋅Wnoise)i) $</strong>：</p><ul><li><strong>$ \text{StandardNormal}() $</strong>：表示从均值为 0、标准差为 1 的标准正态分布中采样一个随机数。</li><li><strong>$ (x \cdot W_{\text{noise}})_i $</strong>：通过另一个可训练权重矩阵$ W_{\text{noise}} $ 计算的噪声控制项。</li><li><strong>$ \text{Softplus}((x \cdot W_{\text{noise}})_i) $</strong>：这部分计算噪声的标准差。它通过将输入 $x$ 与另一个可训练的权重矩阵 $W_{noise}$ 相乘，然后应用 Softplus 函数 $ \text{Softplus}(z) = \log(1 + e^z) $ 来确保标准差为正值。这个标准差是可调节的，并且依赖于输入 $x$ 。</li><li>这部分是加到原始得分上的高斯噪声，表示一个高斯噪声项，均值为 0，方差由 $ \text{Softplus}((x \cdot W_{\text{noise}})_i) $决定。噪声的引入有助于负载均衡，避免门控网络总是选择固定的专家。</li></ul><p>$$ H(x)_ i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i $$</p><p>$H(x)_i$代表了第$ i $个专家的最终“带噪声得分”。</p><p><strong>$k\text{th_excluding}(H(x), k, i)$</strong>: 这是决定专家 i 是否被选中的阈值。它的含义是：在向量 $H(x)$（包含了所有专家的带噪声得分）中，排除掉第 i 个专家自身的分数后，找到剩下 n−1 个分数中第 k 大的分数 。</p><p>$Pr(⋯&gt;…): $ 公式计算 $ H(x)_ i $（重新采样噪声后）大于 $k\text{th_excluding}(H(x), k, i)$ 的概率。在噪声 Top-K 门控中，第 $𝑖$ 个专家被选中当且仅当 $ H(x)_ i$ 是 $𝐻 ( 𝑥 )$ 中前 $𝑘$ 大的值。<br>$$ P(x, i) = \Phi\left( \frac{(x \cdot W_g)_i - k{th\_ excluding}(H(x), k, i)}{\text{Softplus}((x \cdot W_{\text{noise}})_i)} \right) $$<br>Φ 表示标准正态分布的累积分布函数（CDF）。利用正态分布的 CDF 给出了计算这个概率的具体数学表达式，方便进行计算和反向传播（因为 Φ 是可微的）。这整个机制是为了在选择专家时引入随机性（有助于负载均衡 ）并估算每个专家被选中的概率，进而定义$L_{load}$损失。<br>$$ L_{\text{load}}(X) = w_{\text{load}} \cdot CV(\text{Load}(X))^2 $$<br>初始负载不平衡：为了避免内存溢出错误，需要在近似相等的专家负载状态下初始化网络（因为软约束需要一些时间才能发挥作用）。为了实现这一点，将矩阵 $W_g$ 和 $W_{noise}$ 初始化为全零，这样就不会产生信号，而只有一些噪声。</p><h4 id="Hierarchical-Mixture-of-Experts">Hierarchical Mixture-of-Experts</h4><p>层次 MoE 是一种分层结构的 MoE，如果专家数量非常庞大，可以使用两层分层MoE来降低分支因子。在一个分层MoE中，一个主门控网络选择一个稀疏加权组合的“专家”，每个“专家”本身就是一个具有自己门控网络的二级混合专家。</p><p><strong>第一级（主门控网络）</strong>：主门控网络 $ G_{\text{primary}} $ 负责选择一组“专家组”（groups of experts）。</p><p><strong>第二级（次级门控网络）</strong>：每个专家组内有一个次级门控网络 $G_i $，负责在该组内选择具体的专家。</p><p><strong>专家网络</strong>：最终的专家网络 $E_{i,j} $，其中 $ i $ 表示组索引，$j $ 表示组内的专家索引。<br>$$ y_H = \sum_{i=1}^{a} \sum_{j=1}^{b} G_{\text{primary}}(x)_i \cdot G_i(x)_j \cdot E_{i,j}(x) $$<br>$Gprimary(x)i$：主门控网络对第 $ i $ 个组的权重。</p><p>$G_i(x)_j $：第 $ i $ 个组的次级门控网络对组内第 $j $ 个专家的权重。</p><p>$ E_{i,j}(x) $：第 $ i $ 个组中第 $j $ 个专家的输出。</p><p>对专家利用率的衡量指标将更改为以下内容：<br>$$ \text{Importance}_H(X)_{i,j} = \sum_{x \in X} G_{\text{primary}}(x)_i \cdot G_i(x)_j $$</p><p>$$ \text{Load}_H(X)_{i,j} = \frac{\text{Load}_{\text{primary}}(X)_i \cdot \text{Load}_i(X^{(i)})_j}{|X^{(i)}|} $$</p><p>$Load_{primary}$ 和 $Load_i$ 分别表示主门控网络和 $i^{th}$ 次级门控网络的加载函数。 $ X^{(i)}$表示 X 中满足 $ G_{primary}(x)_i &gt; 0 $ 的子集。</p><h2 id="《GShard-Scaling-Giant-Models-with-Conditional-Computation-and-Automatic-Sharding》">《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》</h2><p>基于条件计算和自动分片的巨型模型扩展。</p><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.16668">https://arxiv.org/abs/2006.16668</a></p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502231628278.png" style="zoom:30%" loading='lazy'><h3 id="创新点">创新点</h3><h4 id="Expert-Capacity">Expert Capacity</h4><p>专家容量</p><p>不平衡不仅存在于所选专家中，还存在于发送给专家的token分配中。如果输入 token 不成比例地发送给一个专家而不是另一个专家，那么也可能导致训练不足。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502134823180.webp" style="zoom:50%" loading='lazy'><p>解决这个问题的一个方法是限制每个专家可以处理的 token 数量，即专家容量 。当专家达到容量上限时，产生的 token 将被发送给下一个专家：</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502134935242.webp" style="zoom:50%" loading='lazy'><p>如果两个专家都已达到其容量上限，则 token 将不会被任何专家处理，而是被发送到下一层。这称为 token 溢出 。这些token的表示通过残差连接传递到下一层。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502135022720.webp" style="zoom:50%" loading='lazy'><h4 id="Auxiliary-loss">Auxiliary loss</h4><p>门控函数不应总是选择相同的一小部分专家，因为这会导致一小部分专家容量溢出，而其余专家则被闲置。定义了一个辅助损失项 ℓaux 来强制执行此约束。 它被添加到模型的总损失函数中 L = ℓnll + k ∗ ℓaux，其中 k 是一个常数乘数。</p><h4 id="Random-routing">Random routing</h4><p>随机路由机制</p><p>在 top-2 设计中，始终选择表现最优的专家，但第二选择的专家则根据其权重以一定概率被选中。</p><h2 id="《Switch-Transformers-Scaling-to-Trillion-Parameter-Models-with-Simple-and-Eﬃcient-Sparsity》">《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Eﬃcient Sparsity》</h2><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a></p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250505161645576.png" style="zoom:35%" loading='lazy'><p>简化了MoE路由算法，并设计了直观的改进模型，从而降低了通信和计算成本。提出的训练技术减轻了不稳定性，并且证明了可以使用较低精度（bfloat16）格式首次训练大型稀疏模型。将 Switch 层添加到 Transformer 的自注意力层中。</p><h3 id="Switch-Routing">Switch Routing</h3><p>（1）仅将token路由到单个专家，因此减少了路由器计算。</p><p>（2）由于每个token仅被路由到单个专家，因此每个专家的批次大小（专家容量）可以至少减半。</p><p>（3）路由实现得到简化，并且通信成本降低。</p><h2 id="《DeepSeekMoE-Towards-Ultimate-Expert-Specialization-in-Mixture-of-Experts-Language-Models》">《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》</h2><h3 id="现有MoE架构问题">现有MoE架构问题</h3><p>（1）知识混合性：现有的MoE实践通常采用数量有限的专家（例如，8个或16个），因此分配给特定专家的token可能涵盖各种知识。因此，指定的专家将倾向于在其参数中组合截然不同的知识类型，而这些知识很难同时利用。</p><p>（2）知识冗余：分配给不同专家的token可能需要共同的知识。因此，多个专家可能会收敛于在其各自的参数中获取共享知识，从而导致专家参数的冗余。</p><p>这些问题共同阻碍了现有MoE实践中的专家专业化，使其无法达到MoE模型的理论性能上限。</p><p>我个人感觉第一个问题和第一篇论文提出的目的应该是一致的，一个模型在不同场合执行不同任务会产生干扰，然后这里的专家又出现了这个问题，使得专家的专业化程度较低。</p><h3 id="解决问题的主要策略">解决问题的主要策略</h3><p>（1）细粒度专家分割：在保持参数数量不变的同时，我们通过分割 FFN 中间隐藏维度将专家分割成更细的粒度。相应地，在保持恒定计算成本的同时，我们也激活更多细粒度的专家，以实现激活专家的更灵活和适应性更强的组合。细粒度的专家分割使得不同的知识能够被更精细地分解，并被更精确地学习到不同的专家中，其中每个专家将保持更高的专业化水平。此外，激活专家组合的灵活性增加也有助于更准确和有针对性的知识获取。</p><p>（2）共享专家隔离：我们隔离某些专家作为始终激活的共享专家，旨在捕获和巩固不同上下文中的通用知识。通过将通用知识压缩到这些共享专家中，其他路由专家之间的冗余将被减轻。这可以提高参数效率，并确保每个路由专家通过专注于独特的方面来保持专业性。</p><p>DeepSeekMoE 中的这些架构创新为训练参数高效的 MoE 语言模型提供了机会，其中每个专家都高度专业化。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502145719363.png" style="zoom:40%" loading='lazy'><p>DeepSeekMoE 的示意图。子图 (a) 展示了具有传统 top-2 路由策略的 MoE 层。子图 (b) 说明了细粒度专家分割策略。子图 © 展示了共享专家隔离策略的集成，构成了完整的 DeepSeekMoE 架构。值得注意的是，在这三种架构中，专家参数的数量和计算成本保持不变。</p><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502161913940.png" style="zoom:40%" loading='lazy'><h3 id="DeepSeekMoE-架构">DeepSeekMoE 架构</h3><h4 id="细粒度专家分割">细粒度专家分割</h4><p>通过将FFN中间隐藏维度降低到原始大小的𝑚分之一，将每个专家FFN分割成𝑚个更小的专家。由于每个专家变得更小，作为回应，还将激活专家的数量增加到𝑚倍，以保持相同的计算成本。通过细粒度的专家分割，MoE层的输出可以表示为：<br>$$ \mathbf{h}_t^l = \sum_{i=1}^{mN} (g_{i,t} \cdot \text{FFN}_i(\mathbf{u}_t^l)) + \mathbf{u}_t^l $$</p><p>$$<br>g_{i,t} = \begin{cases}<br>s_{i,t}, &amp; s_{i,t} \in \text{Topk}({s_{j,t}} \mid 1 \leq j \leq mN), mK), \<br>0, &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>$$<br>s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^{l^T} \mathbf{e}_i^l)<br>$$</p><p>其中，专家参数的总数等于 𝑁 乘以标准 FFN 中的参数数量，mN 表示细粒度专家的总数。采用细粒度专家分割策略后，非零门控的数量也将增加到 mK。</p><p>组合灵活性的激增增强了实现更准确和更有针对性的知识获取的潜力。</p><h4 id="共享专家隔离">共享专家隔离</h4><p>采用传统的路由策略，分配给不同专家的令牌可能需要一些共同的知识或信息。因此，多个专家可能会趋同于在其各自的参数中获取共享知识，从而导致专家参数的冗余。然而，如果存在专门用于捕获和整合不同上下文中的共同知识的共享专家，则可以减轻其他路由专家之间的参数冗余。这种冗余的减轻将有助于构建一个参数效率更高、专家更专业的模型。</p><p>隔离𝐾𝑠个专家作为共享专家。无论路由器模块如何，每个token都将被确定性地分配给这些共享专家。为了保持恒定的计算成本，其他路由专家中激活的专家数量将减少𝐾𝑠。通过集成共享专家隔离策略，完整DeepSeekMoE架构中的MoE层可以表述如下：<br>$$ \mathbf{h}_t^l = \sum_{i=1}^{K_s} \text{FFN}_i(\mathbf{u}_t^l) + \sum_{i=K_s+1}^{mN} (g_{i,t} \cdot \text{FFN}_i(\mathbf{u}_t^l)) + \mathbf{u}_t^l $$</p><p>$$<br>g_{i,t} = \begin{cases}<br>s_{i,t}, &amp; s_{i,t} \in \text{Topk}({s_{j,t} \mid K_s + 1 \leq j \leq mN }), mK - K_s), \<br>0, &amp; \text{otherwise}<br>\end{cases}<br>$$</p><p>$$<br>s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^{l^T} \mathbf{e}_i)<br>$$</p><p>最后，在 DeepSeekMoE 中，共享专家的数量为 𝐾𝑠，路由专家的总数为 𝑚𝑁 − 𝐾𝑠，非零门的数量为 𝑚𝐾 − 𝐾𝑠。</p><h4 id="负载均衡问题">负载均衡问题</h4><p>自动学习的路由策略可能会遇到负载不均衡的问题，这表现出两个显著的缺陷。首先，存在路由崩溃的风险，即模型总是只选择少数几个专家，导致其他专家无法得到充分的训练。其次，如果专家分布在多个设备上，负载不均衡会加剧计算瓶颈。</p><h5 id="Expert-Level-Balance-Loss">Expert-Level Balance Loss</h5><p>专家级平衡损失。为了降低路由崩溃的风险，平衡损失的计算如下：<br>$$ \mathcal{L}_{\text{ExpBal}} = \alpha_1 \sum_{i=1}^{N'} f_i p_i $$</p><p>$$<br>f_i = \frac{N’}{K’ T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ selects Expert } i)<br>$$</p><p>$$<br>p_i = \frac{1}{T} \sum_{t=1}^{T} s_{i,t}<br>$$</p><p>其中𝛼1是一个被称为专家级别平衡因子的超参数，为了简洁起见，𝑁′等于(𝑚𝑁 − 𝐾𝑠)，𝐾′等于(𝑚𝐾 − 𝐾𝑠)。1(·)表示指示函数。</p><h5 id="Device-Level-Balance-Loss">Device-Level Balance Loss</h5><p>将所有路由的专家划分为 𝐷 组 {E1, E2, . . ., E𝐷}，并将每组部署在单个设备上，则设备级别平衡损失的计算方式如下：<br>$$ \mathcal{L}_{\text{DevBal}} = \alpha_2 \sum_{i=1}^{D} f'_i p'_i $$</p><p>$$ f'_i = \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i} f_j $$</p><p>$$ p'_i = \sum_{j \in \mathcal{E}_i} p_j $$</p><p>其中𝛼2是一个被称为设备级别平衡因子的超参数。设置一个较小的专家级别平衡因子以降低路由崩溃的风险，同时设置一个较大的设备级别平衡因子以促进设备间的均衡计算。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://wp-a.github.io">WP</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://wp-a.github.io/2025/04/MOE/">https://wp-a.github.io/2025/04/MOE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://wp-a.github.io" target="_blank">WPIRONMAN</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">论文精读</a><a class="post-meta__tags" href="/tags/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E7%B3%BB%E7%BB%9F/">混合专家系统</a></div><div class="post-share"><div class="social-share" data-image="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250502161913940.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/123.JPG" target="_blank"><img class="post-qr-code-img" src="/img/123.JPG" alt="微信" loading='lazy'></a><div class="post-qr-code-desc">微信</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/MoCo/" title="MoCo"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250425170023257.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post" loading='lazy'><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">MoCo</div></div><div class="info-2"><div class="info-item-1">MoCo Momentum Contrast for Unsupervised Visual Representation Learning (cvpr2020) 论文地址：https://arxiv.org/pdf/1911.05722 代码地址：https://github.com/facebookresearch/moco 概述 MoCo 将对比学习看作是一个字典查找任务 ：一个编码后的查询（query）应该与其匹配的键（正样本）相似，而与其他所有的键（负样本）不相似 。 对比学习的核心思想是训练一个编码器，使其能够区分相似（正样本）和不相似（负样本）的样本 。 传统方法 VS MoCo 端到端（End-to-end）方法（SimCLR，Inva Spread）：将当前 mini-batch 内的样本作为字典 。这种方法的优点是字典中的键编码是一致的（由同一个编码器生成），但缺点是字典的大小受限于 mini-batch 的大小，而 mini-batch 大小又受限于 GPU 内存 。过大的 mini-batch 也会带来优化难题 。 Memory Bank...</div></div></div></a><a class="pagination-related" href="/2025/06/%E6%89%8B%E6%92%95%20Vision%20Transformer/" title="手撕 Vision Transformer"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250601121856303.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post" loading='lazy'><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">手撕 Vision Transformer</div></div><div class="info-2"><div class="info-item-1">手撕 Vision...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/BERT/" title="BERT详解 - 双向编码器表示模型精读"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/202511102140124.png" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-09</div><div class="info-item-2">BERT详解 - 双向编码器表示模型精读</div></div><div class="info-2"><div class="info-item-1">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 引言 BERT (Bidirectional Encoder Representations from Transformers) 是Google在2018年提出的革命性自然语言处理模型，它通过在无标注文本上进行预训练，学习深层的双向语言表示，在下游任务上取得了突破性的成果。 BERT的核心创新在于双向上下文编码，与之前的ELMo（浅层双向）和GPT（单向）不同，BERT使用Transformer编码器同时利用上下文信息，彻底改变了NLP领域的预训练范式。 背景知识 预训练语言模型的发展 在BERT之前，主流的预训练方法存在以下局限性： 单向语言模型（如GPT）：只能从左到右或从右到左进行编码，无法同时利用双向上下文 浅层双向模型（如ELMo）：虽然考虑了双向信息，但只是简单拼接左右向表示，而非深度双向，网络架构比较老，使用的RNN 为什么需要双向编码？ 语言的理解往往需要同时考虑前后文信息。例如： “银行”...</div></div></div></a><a class="pagination-related" href="/2025/04/CSRMS/" title="CSRMS"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250418223753935.png" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-18</div><div class="info-item-2">CSRMS</div></div><div class="info-2"><div class="info-item-1">CSRMS 用于视觉表征学习的类级结构化关系建模与平滑 (MM2023) 论文地址：https://ercdm.sdu.edu.cn/__local/7/AC/70/7E4948C4761839F62E3958CE772_043AE854_2B459A.pdf 代码地址：https://github.com/czt117/CSRMS 个人理解这个像是一个知识总结的过程。首先通过特征提取获得特征图，这个过程可以类比我从书本上学习知识的过程，提取出有用的知识，然后通过聚类算法对特征图进行分簇，就相当于把学到的知识进行总结的过程，但是总会有一些比较相近的知识容易被搞混，这个就是类间相似性和类内多样性，再着重对这一块进行处理，使得对知识的掌握更加透彻。 名词解释： 课程构建（Curriculum...</div></div></div></a><a class="pagination-related" href="/2025/11/MAE/" title="MAE详解 - Masked Autoencoders精读"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/202511102142380.png" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-09</div><div class="info-item-2">MAE详解 - Masked Autoencoders精读</div></div><div class="info-2"><div class="info-item-1">Masked Autoencoders Are Scalable Vision Learners 引言 MAE (Masked Autoencoders) 由He Kaiming团队在2021年提出，为视觉自监督学习带来了新的范式。论文标题“Masked Autoencoders Are Scalable Vision Learners”凸显了其两大特性：一是基于掩码的自重构任务；二是能在大规模数据和模型上稳定扩展。和SimCLR、MoCo等对比学习方法相比，MAE丢弃了昂贵的负样本构造环节，通过简单的遮挡-重建目标即可学习高质量的视觉特征。 在图像理解任务中，过去的自监督方法往往依赖对比学习或生成式建模。MAE将NLP中成熟的Masked Language Modeling理念迁移到视觉领域，将图片切分为patch token，然后随机遮挡大部分token，让模型仅凭剩余少量可见token推断出被遮挡的像素，从而学到上下文结构。 背景知识 自监督视觉预训练的演进 预文本任务 (Pretext...</div></div></div></a><a class="pagination-related" href="/2025/11/Mamba/" title="Mamba详解 - 选择性状态空间模型精读"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/202511111045897.png" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-09</div><div class="info-item-2">Mamba详解 - 选择性状态空间模型精读</div></div><div class="info-2"><div class="info-item-1">Mamba: Linear-Time Sequence Modeling with Selective SSMs 论文地址：https://arxiv.org/pdf/2312.00752 代码地址：https://github.com/state-spaces/mamba 引言 Mamba 是一种基于状态空间模型（State Space Model, SSM）的高效序列建模框架，旨在在保持强表达能力的同时，将计算与内存复杂度降至与序列长度线性相关。与Transformer的二次复杂度相比，Mamba在超长序列、低时延和内存受限场景中具有显著优势。 Mamba的核心在于“选择性扫描（Selective Scan）”与“输入依赖的状态转移”，通过对经典S4（Structured State Space Sequence Model）的工程化与理论改进，实现端到端可训练、GPU友好、且具有SOTA性能的线性时间序列模型。 背景知识：状态空间模型（SSM）与S4 连续与离散SSM 连续时间SSM： $$ \dot x(t) = A x(t) + B u(t), \quad y(t)...</div></div></div></a><a class="pagination-related" href="/2025/09/MambaOut/" title="MambaOut"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/202509111759034.png" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-10</div><div class="info-item-2">MambaOut</div></div><div class="info-2"><div class="info-item-1">MambaOut MambaOut: Do We Really Need Mamba for Vision? (CVPR...</div></div></div></a><a class="pagination-related" href="/2025/04/MoCo/" title="MoCo"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/20250425170023257.png" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-24</div><div class="info-item-2">MoCo</div></div><div class="info-2"><div class="info-item-1">MoCo Momentum Contrast for Unsupervised Visual Representation Learning (cvpr2020) 论文地址：https://arxiv.org/pdf/1911.05722 代码地址：https://github.com/facebookresearch/moco 概述 MoCo 将对比学习看作是一个字典查找任务 ：一个编码后的查询（query）应该与其匹配的键（正样本）相似，而与其他所有的键（负样本）不相似 。 对比学习的核心思想是训练一个编码器，使其能够区分相似（正样本）和不相似（负样本）的样本 。 传统方法 VS MoCo 端到端（End-to-end）方法（SimCLR，Inva Spread）：将当前 mini-batch 内的样本作为字典 。这种方法的优点是字典中的键编码是一致的（由同一个编码器生成），但缺点是字典的大小受限于 mini-batch 的大小，而 mini-batch 大小又受限于 GPU 内存 。过大的 mini-batch 也会带来优化难题 。 Memory Bank...</div></div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div><h1 id="site-title" fetchpriority="high" style="font-display:swap">MoE</h1></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Mixtures of Experts</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%8AAdaptive-Mixture-of-Local-Experts%E3%80%8B"><span class="toc-number">1.1.</span> <span class="toc-text">《Adaptive Mixture of Local Experts》</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.1.</span> <span class="toc-text">基础架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83"><span class="toc-number">1.1.2.</span> <span class="toc-text">性能比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.1.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%8AOutrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer%E3%80%8B"><span class="toc-number">1.2.</span> <span class="toc-text">《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84-2"><span class="toc-number">1.2.1.</span> <span class="toc-text">基础架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E9%97%A8%E6%8E%A7%E4%B8%93%E5%AE%B6%E6%B7%B7%E5%90%88%E5%B1%82%EF%BC%88MoE%EF%BC%89"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">稀疏门控专家混合层（MoE）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E8%A1%A1%E8%B4%9F%E8%BD%BD"><span class="toc-number">1.2.2.</span> <span class="toc-text">均衡负载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Keep-Top-K"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Keep Top-K</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Noise"><span class="toc-number">1.2.2.1.1.</span> <span class="toc-text">Noise</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Sparse"><span class="toc-number">1.2.2.1.2.</span> <span class="toc-text">Sparse</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Auxiliary-Loss"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Auxiliary Loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Load-Balancing-Loss"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">Load-Balancing Loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hierarchical-Mixture-of-Experts"><span class="toc-number">1.2.2.4.</span> <span class="toc-text">Hierarchical Mixture-of-Experts</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%8AGShard-Scaling-Giant-Models-with-Conditional-Computation-and-Automatic-Sharding%E3%80%8B"><span class="toc-number">1.3.</span> <span class="toc-text">《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-number">1.3.1.</span> <span class="toc-text">创新点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Expert-Capacity"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Expert Capacity</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Auxiliary-loss"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Auxiliary loss</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Random-routing"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Random routing</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%8ASwitch-Transformers-Scaling-to-Trillion-Parameter-Models-with-Simple-and-E%EF%AC%83cient-Sparsity%E3%80%8B"><span class="toc-number">1.4.</span> <span class="toc-text">《Switch Transformers: Scaling to Trillion Parameter Models with Simple and Eﬃcient Sparsity》</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Switch-Routing"><span class="toc-number">1.4.1.</span> <span class="toc-text">Switch Routing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%8ADeepSeekMoE-Towards-Ultimate-Expert-Specialization-in-Mixture-of-Experts-Language-Models%E3%80%8B"><span class="toc-number">1.5.</span> <span class="toc-text">《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%B0%E6%9C%89MoE%E6%9E%B6%E6%9E%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.5.1.</span> <span class="toc-text">现有MoE架构问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%BB%E8%A6%81%E7%AD%96%E7%95%A5"><span class="toc-number">1.5.2.</span> <span class="toc-text">解决问题的主要策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DeepSeekMoE-%E6%9E%B6%E6%9E%84"><span class="toc-number">1.5.3.</span> <span class="toc-text">DeepSeekMoE 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%86%E7%B2%92%E5%BA%A6%E4%B8%93%E5%AE%B6%E5%88%86%E5%89%B2"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">细粒度专家分割</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B1%E4%BA%AB%E4%B8%93%E5%AE%B6%E9%9A%94%E7%A6%BB"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">共享专家隔离</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98"><span class="toc-number">1.5.3.3.</span> <span class="toc-text">负载均衡问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Expert-Level-Balance-Loss"><span class="toc-number">1.5.3.3.1.</span> <span class="toc-text">Expert-Level Balance Loss</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Device-Level-Balance-Loss"><span class="toc-number">1.5.3.3.2.</span> <span class="toc-text">Device-Level Balance Loss</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 - 2026 By WP</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js" defer></script><script src="/js/main.js" defer></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(()=>{var t=()=>{var t;window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise()):(window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],tags:"none"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(var e of document.querySelectorAll('script[type^="math/tex"]')){var a=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],a),n=document.createTextNode("");e.parentNode.replaceChild(n,e),a.start={node:n,delim:"",n:0},a.end={node:n,delim:"",n:0},t.math.push(a)}},""]}}},(t=document.createElement("script")).src="https://cdn.staticfile.org/mathjax/3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t))};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{var e=()=>{var e;0!==(e=document.querySelectorAll("pre > code.mermaid")).length&&e.forEach(e=>{var t=document.createElement("pre"),a=(t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent,document.createElement("div"));a.className="mermaid-wrap",a.appendChild(t),e.parentNode.replaceWith(a)});let t=document.querySelectorAll("#article-container .mermaid-wrap");0!==t.length&&(e=()=>(e=>{window.loadMermaid=!0;let n="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,t)=>{let a=e.firstElementChild;e=`%%{init:{ 'theme':'${n}'}}%%
`+a.textContent,t=mermaid.render("mermaid-"+t,e);let d=e=>{a.insertAdjacentHTML("afterend",e)};"string"==typeof t?d(t):t.then(({svg:e})=>d(e))})})(t),btf.addGlobalFn("themeChange",e,"mermaid"),window.loadMermaid?e():btf.getScript("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.1/mermaid.min.js").then(e))};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{let n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,o=null,a=(t,e)=>{n&&(window.shuoshuoComment.destroyValine=()=>{t.children.length&&(t.innerHTML="",t.classList.add("no-comment"))});e={el:"#vcomment",appId:"FXG14lTbR0Yj3W2kb3tkAt4L-gzGzoHsz",appKey:"hohJIUW6lOhfboJzq5FvG8z7",avatar:"monsterid",serverURLs:"https://fxg14ltb.lc-cn-n1-shared.com",emojiMaps:"",visitor:!1,...o,path:n?e:o&&o.path||window.location.pathname};new Valine(e)};var t=async(t,e)=>{"function"==typeof Valine||await btf.getScript("https://unpkg.com/valine@1.5.1/dist/Valine.min.js"),a(t,e)};n?window.shuoshuoComment={loadComment:t}:setTimeout(t,0)})()</script></div><script type="text/javascript" src="/js/reward.js" defer></script><script src="/js/fix-avatar.js" defer></script><script src="/js/blog-cool-features.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"]):not([href="/gallery/"]):not([href="/about/"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});let t=e=>{e&&Object.values(e).forEach(e=>e())};document.addEventListener("pjax:send",()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");var e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),t(window.globalFn.pjaxSend)}),document.addEventListener("pjax:complete",()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach(e=>{let t=document.createElement("script");var a=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(a)),e.parentNode.replaceChild(t,e)}),t(window.globalFn.pjaxComplete)}),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js" defer></script></div></div><script data-pjax src="/js/githubcalendar-loader.js?v=20251124"></script><script data-pjax>function GithubCalendarConfig(){var t=document.getElementById("recent-posts");t&&"/"==location.pathname&&(console.log("已挂载hexo-github-calendar https://github.com/Barry-Flynn/hexo-github-calendar"),t.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>')),GithubCalendar("https://github-calendar-api.meta-code.top/api?user=wp-a",["#ebedf0","#a2f7af","#6ce480","#54ad63","#469252","#31753c","#1f5f2a","#13531f","#084111","#032b09","#000000"],"wp-a")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:0}}</style><style></style></body></html>