<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节） | WPIRONMAN</title><meta name="author" content="WP"><meta name="copyright" content="WP"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这一篇对应视频 04：“REINFORCE 4 LLM，设计 reward，如何维护和计算 PG&#x2F;KL loss，KL loss 计算细节”（BV1Ya1LB1EDM）。 它的价值不在于“给你一个能把大模型训得多强的框架”，而在于把 RL4LLM 里最容易被 verl&#x2F;openrlhf&#x2F;... 这些工程封装遮住的核心逻辑，拆成一个可手写、可调试、可逐行对齐公式的最小闭环：  LLM 作为 poli"><meta property="og:type" content="article"><meta property="og:title" content="Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）"><meta property="og:url" content="https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AREINFORCE%204%20LLM%EF%BC%88Reward%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%20PG-KL%20Loss%20%E7%BB%86%E8%8A%82%EF%BC%89/index.html"><meta property="og:site_name" content="WPIRONMAN"><meta property="og:description" content="这一篇对应视频 04：“REINFORCE 4 LLM，设计 reward，如何维护和计算 PG&#x2F;KL loss，KL loss 计算细节”（BV1Ya1LB1EDM）。 它的价值不在于“给你一个能把大模型训得多强的框架”，而在于把 RL4LLM 里最容易被 verl&#x2F;openrlhf&#x2F;... 这些工程封装遮住的核心逻辑，拆成一个可手写、可调试、可逐行对齐公式的最小闭环：  LLM 作为 poli"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp"><meta property="article:published_time" content="2026-02-09T11:40:00.000Z"><meta property="article:modified_time" content="2026-02-10T04:40:02.776Z"><meta property="article:author" content="WP"><meta property="article:tag" content="Agentic RL"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RLHF"><meta property="article:tag" content="REINFORCE"><meta property="article:tag" content="KL"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）",
  "url": "https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AREINFORCE%204%20LLM%EF%BC%88Reward%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%20PG-KL%20Loss%20%E7%BB%86%E8%8A%82%EF%BC%89/",
  "image": "https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp",
  "datePublished": "2026-02-09T11:40:00.000Z",
  "dateModified": "2026-02-10T04:40:02.776Z",
  "author": [
    {
      "@type": "Person",
      "name": "WP",
      "url": "https://wp-a.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="https://wpironman.oss-cn-qingdao.aliyuncs.com/favicon.png"><link rel="canonical" href="https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AREINFORCE%204%20LLM%EF%BC%88Reward%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%20PG-KL%20Loss%20%E7%BB%86%E8%8A%82%EF%BC%89/index.html"><link rel="preconnect" href="//cdnjs.cloudflare.com"><link rel="manifest" href="/null"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css"><script>(()=>{var e={set:(e,t,a)=>{a&&(a=Date.now()+864e5*a,localStorage.setItem(e,JSON.stringify({value:t,expiry:a})))},get:e=>{var t=localStorage.getItem(e);if(t){var{value:t,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return t;localStorage.removeItem(e)}}},t=(window.btf={saveToLocal:e,getScript:(o,n={})=>new Promise((e,t)=>{let a=document.createElement("script");a.src=o,a.async=!0,Object.entries(n).forEach(([e,t])=>a.setAttribute(e,t)),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),getCSS:(o,n)=>new Promise((e,t)=>{let a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),addGlobalFn:(e,t,a=!1,o=window)=>{var n=o.globalFn||{};n[e]=n[e]||{},n[e][a||Object.keys(n[e]).length]=t,o.globalFn=n}},()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")}),a=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")},o=(btf.activateDarkMode=t,btf.activateLightMode=a,e.get("theme")),t=("dark"===o?t():"light"===o&&a(),e.get("aside-status"));void 0!==t&&document.documentElement.classList.toggle("hide-aside","hide"===t);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Sans+SC:wght@400;500;600&amp;family=JetBrains+Mono:wght@400;500&amp;family=Roboto+Slab:wght@400;600;700&amp;display=swap" media="print" onload='this.media="all"'><script>let GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdnjs.cloudflare.com/ajax/libs/egjs-infinitegrid/4.12.0/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyloadPlugin:!1,isAnchor:!0,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="/css/_custom/category/categories.css"><link rel="preconnect" href="https://fonts.loli.net" crossorigin><link rel="stylesheet" href="/css/categories.css"><link rel="stylesheet" href="/css/valine.css"><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/medium-style.css"><script>window.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("my-pv");e&&fetch("https://wpironman.top/pv").then(t=>t.json()).then(t=>{e.textContent=t.pv}).catch(()=>{e.textContent="获取失败"})})</script><link rel="preload" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" as="style" onload='this.rel="stylesheet"'><link href="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" as="image" crossorigin="anonymous"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image:url(https://wpironman.oss-cn-qingdao.aliyuncs.com/10year.webp)"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/head.png" onerror='this.onerror=null,this.src="https://wpironman.oss-cn-qingdao.aliyuncs.com/head.gif"' alt="avatar" loading='lazy'></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">168</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/construction-detection/"><i class="fa-fw fas fa-hard-hat"></i><span> 工地检测</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-user-group"></i><span> 友链</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fa-solid fa-user-tie"></i><span> 本站友链</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i><span> 随机开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="fa-fw fa fa-taxi"></i><span> 异次元之旅</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/academic/"><i class="fa-fw fas fa-graduation-cap"></i><span> 学术主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/favicon.png" alt="Logo" loading='lazy'><span class="site-name">WPIRONMAN</span></a><a class="nav-page-title" href="/"><span class="site-name">Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/construction-detection/"><i class="fa-fw fas fa-hard-hat"></i><span> 工地检测</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-user-group"></i><span> 友链</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fa-solid fa-user-tie"></i><span> 本站友链</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i><span> 随机开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="fa-fw fa fa-taxi"></i><span> 异次元之旅</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/academic/"><i class="fa-fw fas fa-graduation-cap"></i><span> 学术主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）<a class="post-edit-link" href="null_posts/Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG-KL Loss 细节）.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2026-02-09T11:40:00.000Z" title="发表于 2026-02-09 19:40:00">2026-02-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/">算法解析</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>这一篇对应视频 04：<strong>“REINFORCE 4 LLM，设计 reward，如何维护和计算 PG/KL loss，KL loss 计算细节”</strong>（BV1Ya1LB1EDM）。</p><p>它的价值不在于“给你一个能把大模型训得多强的框架”，而在于把 RL4LLM 里最容易被 <code>verl/openrlhf/...</code> 这些工程封装遮住的核心逻辑，拆成一个<strong>可手写、可调试、可逐行对齐公式</strong>的最小闭环：</p><ul><li>LLM 作为 policy：state/action/trajectory 怎么映射</li><li>REINFORCE/PG loss 在 LLM 上怎么落地：<strong>联合 logprob 怎么维护</strong></li><li>KL penalty 为什么是“语言能力 vs reward-max”的拨杆</li><li>KL 计算最容易踩坑在哪里：<strong>sum vs mean / fKL vs rKL / mask 与聚合</strong></li></ul><p>系列导航：</p><ul><li><a href="/2026/02/Agentic%20RL%EF%BC%9A%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA%EF%BC%88PG%20Loss%E3%80%81TRPO%E3%80%81PPO-Clip%EF%BC%89/" title="Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）">Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）</a></li></ul><p>建议阅读顺序（我按“先理解再工程”的顺序排）：</p><ol><li><a href="/2026/02/Agentic%20RL%EF%BC%9A%E4%BB%8E%20PG%20%E5%88%B0%20TRPO%20%E5%88%B0%20PPO-Clip%EF%BC%88%E6%8E%A8%E5%AF%BC%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AF%B9%E9%BD%90%EF%BC%89/" title="Agentic RL：从 PG 到 TRPO 到 PPO-Clip（推导与代码对齐）">02&amp;03：PG→TRPO→PPO + GRPO&#x2F;On-policy 补充（推导与代码对齐）</a></li><li><a href="/2026/02/Agentic%20RL%EF%BC%9APG%20Loss%20%E7%BB%84%E4%BB%B6%E8%AF%A6%E8%A7%A3/" title="Agentic RL：PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）">01：PG loss 组件详解（PPO-clip &#x2F; Dual-clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）</a></li><li>你正在读的这一篇（04：REINFORCE + reward + KL 的最小闭环）</li><li><a href="/2026/02/Agentic%20RL%EF%BC%9AReward%20Model%20Insights%EF%BC%88Bradley-Terry%E3%80%81MLE%20%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89/" title="Agentic RL：Reward Model Insights（Bradley-Terry、MLE 与深度学习）">09：Reward Model Insights（Bradley-Terry &#x2F; MLE &#x2F; 深度学习）</a></li></ol><hr><h2 id="0-资料对齐（视频-GitHub）">0. 资料对齐（视频 + GitHub）</h2><ul><li>视频 04：<code>BV1Ya1LB1EDM</code><ul><li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ya1LB1EDM/">https://www.bilibili.com/video/BV1Ya1LB1EDM/</a></li></ul></li><li>配套仓库（你本地已下载）：<code>wdkns/modern_genai_bilibili</code><ul><li>GitHub：<a target="_blank" rel="noopener" href="https://github.com/wdkns/modern_genai_bilibili/tree/main/agentic_rl">https://github.com/wdkns/modern_genai_bilibili/tree/main/agentic_rl</a></li></ul></li><li>对应 Notebook（讲思路 + 展示现象）：<ul><li><code>agentic_rl/deep_RL/align/align_demo.ipynb</code></li></ul></li><li>对应训练脚本（本文主线）：<ul><li><code>agentic_rl/deep_RL/scripts/reinforce_align.py</code></li></ul></li><li>KL 方向（fKL/rKL）补充材料：<ul><li><code>agentic_rl/sft_rl_fkl_rkl.ipynb</code></li></ul></li></ul><p>本文会以 <code>reinforce_align.py</code> 为主线，把视频里强调的关键点“落回代码”，同时把最常见的 KL 实现坑一次讲清楚。</p><h2 id="0-1-这个-demo-刻意简化了什么（你迁移到真实任务必须补上什么）">0.1 这个 demo 刻意简化了什么（你迁移到真实任务必须补上什么）</h2><p>先把边界讲清楚：<code>reinforce_align.py</code> 的定位是 <strong>“把 RL4LLM 的骨架拆开给你看”</strong>，而不是“可直接拿去训强模型的配方”。它刻意简化/省略了很多真实训练里决定成败的东西。</p><p>你把它迁移到真实任务（尤其是 agentic RL / deep research）时，至少要补齐这些点，否则你会遇到“曲线很好看但行为变坏/训练不稳定/指标不可解释”：</p><ol><li><strong>方差控制（baseline / advantage / whiten）</strong><ul><li>REINFORCE 直接用 reward 当权重，方差极大；现实里要么用 critic/GAE（PPO），要么至少做 per-prompt/per-group 标准化（GRPO）。</li></ul></li><li><strong>变长序列的 mask 与归一</strong><ul><li>必须明确 EOS 后还算不算 KL / logprob；长度归一按“有效 token 数”做，而不是除以固定常数。</li></ul></li><li><strong>训练集 vs 评测集（防 reward hacking）</strong><ul><li>reward model/verifier 很容易被 hack。你需要 holdout prompts + 定期采样输出做 case analysis。</li></ul></li><li><strong>KL 的“种类”别混用</strong><ul><li><code>KL(π_new || π_ref)</code>（anchor 到参考模型） 和 <code>KL(π_new || π_old)</code>（trust region）是两类东西；日志里 <code>approx_kl</code> 往往又是 sampled proxy。</li></ul></li><li><strong>奖励的尺度与分布</strong><ul><li>reward 的均值/方差、clip、归一、温度/采样策略都会直接改变梯度尺度；你需要把 reward 的统计量记录进日志（mean/std/quantile）。</li></ul></li><li><strong>系统层面的吞吐与成本</strong><ul><li>对 LLM 来说 rollout 才是大头。你需要推理引擎（vLLM）和 batching 逻辑，否则训练速度会慢到不可迭代。</li></ul></li></ol><p>把这些补齐之后，你才算真的进入“可复现、可解释、可对比”的 RL4LLM 训练范畴。</p><hr><h2 id="1-把-LLM-看成-policy：state-action-trajectory-是什么">1. 把 LLM 看成 policy：state/action/trajectory 是什么</h2><p>把自回归生成写成 RL 形式，你可以用下面这个映射来理解：</p><ul><li><strong>状态</strong> $s_t$：prompt + 目前已经生成的 token 前缀（history）</li><li><strong>动作</strong> $a_t$：下一个 token（离散动作空间大小约等于 vocab size）</li><li><strong>策略</strong> $\pi_\theta(a_t|s_t)$：LM 在当前上下文下的 next-token distribution</li><li><strong>轨迹</strong> $\tau$：整段生成序列（从 prompt 开始，到 EOS/长度上限结束）</li></ul><p>很多 RL4LLM demo（包括本期）更像 contextual bandit：reward 常常是<strong>整句/整段生成结束后</strong>一次性给出（outcome reward）。</p><hr><h2 id="2-REINFORCE-PG-的核心：你不是在最小化“监督学习-loss”，而是在构造正确梯度">2. REINFORCE/PG 的核心：你不是在最小化“监督学习 loss”，而是在构造正确梯度</h2><p>先把目标写清楚。对一条生成序列 $y$（包含 token 序列）定义 reward $R(y)$，我们想最大化：</p><p>$$J(\theta)=\mathbb{E}_{y\sim\pi_\theta}[R(y)]$$</p><p>麻烦点在于：期望是对“由 $\pi_\theta$ 采样出来的分布”取的，属于对分布求导。REINFORCE 用 log-trick 把它改写成对 logprob 求导：</p><p>$$\nabla_\theta J(\theta)=\mathbb{E}_{y\sim\pi_\theta}\big[R(y)\cdot\nabla_\theta \log \pi_\theta(y)\big]$$</p><p>工程里我们习惯写成“最小化 loss”，所以常见的 PG loss（负号）是：</p><p>$$\mathcal{L}_{PG}(\theta)=-\mathbb{E}_{y\sim\pi_\theta}\big[R(y)\cdot \log \pi_\theta(y)\big]$$</p><p>关键点：这里的 $\mathcal{L}_{PG}$ 不是什么“预测误差”，它是一个为了得到正确梯度而构造的 surrogate objective。你日志里看到它上下波动很正常，真正像“性能曲线”的通常是 reward/任务指标。</p><hr><h2 id="3-LLM-的联合概率：为什么要维护的是整段输出的-logprob（而不是某个-token-的概率）">3. LLM 的联合概率：为什么要维护的是整段输出的 logprob（而不是某个 token 的概率）</h2><p>LLM 的序列概率是 token 条件概率的连乘。对数空间就是累加：</p><p>$$\log \pi_\theta(y)=\sum_{t=1}^{T}\log \pi_\theta(a_t|s_t)$$</p><p>这就是脚本里下面这行在做的事（逐 token 累加）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log_probs_accumulated[active_mask] += dist.log_prob(next_tokens).unsqueeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>注意：视频里反复强调的是“联合概率”。你维护联合 logprob，才能把 rollout 的整段输出直接带回 REINFORCE 公式里。</p><p>一个工程细节：脚本里把联合 logprob 再除以 <code>NUM_TOKENS</code> 做长度归一：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">normalized_log_probs = log_probs_accumulated / NUM_TOKENS</span><br></pre></td></tr></table></figure><p>它的直觉是：否则长句天然更负（logprob 连加得更多），梯度尺度会随长度漂移，训练会很难调。</p><p>更严谨的版本通常会按“有效生成长度”归一（EOS 后不再计入），而不是除以固定常数 <code>NUM_TOKENS</code>。这属于 demo 简化，你在做真实任务时建议修正。</p><h3 id="3-1-长度控制的三种办法：别把“归一”当成唯一解">3.1 长度控制的三种办法：别把“归一”当成唯一解</h3><p>很多初学者会把“长度归一”当成万能药，但它本质只是<strong>在改梯度尺度</strong>。更系统的做法是把“长度”当成你要显式管理的变量，常见有三条路：</p><ol><li><strong>只做尺度归一（不改最优点）</strong><ul><li>用 <code>logprob_sum / effective_len</code> 主要是让不同长度的样本梯度量级可比，方便调参。</li><li>但你要意识到：它也在引入长度相关的 reweighting，可能会轻微偏好短序列。</li></ul></li><li><strong>在 reward 里显式惩罚/奖励长度（改最优点）</strong><ul><li>例如 <code>R_total = R_task - λ * length</code> 或者把 reward 写成 “每 token 的平均质量”。</li><li>这更可控：你明确告诉策略“长一点/短一点”的代价，而不是通过一个隐式归一去偷改梯度。</li></ul></li><li><strong>用任务协议把长度关掉</strong><ul><li>对 research agent 来说，经常可以通过输出格式约束（必须包含哪些段落/必须引用几篇/最大字数）把长度问题转化为“格式是否合规”的 verifier 问题。</li></ul></li></ol><p>工程里最推荐的顺序是：先用协议/格式约束把长度控制住；必要时再做尺度归一；最后才考虑把长度写进 reward（因为它最容易引入副作用）。</p><hr><h2 id="4-这个-demo-的-reward：embedding-cosine-similarity（好用但危险）">4. 这个 demo 的 reward：embedding cosine similarity（好用但危险）</h2><p>视频用一个非常直观的目标来做“RL4LLM 祛魅”：</p><blockquote><p>不管 prompt 是什么，都让模型倾向输出“和猫相关”的语义。</p></blockquote><p>reward 的定义很简单：用一个冻结的 sentence-transformer 把生成序列编码成 embedding，再和 <code>cat</code> 的 embedding 做 cosine similarity：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sequence_embeddings = embedding_model.encode(sequences, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line">reference_embedding = embedding_model.encode(<span class="string">&quot;cat&quot;</span>, convert_to_tensor=<span class="literal">True</span>)</span><br><span class="line">rewards = util.pytorch_cos_sim(reference_embedding.unsqueeze(<span class="number">0</span>), sequence_embeddings).squeeze()</span><br></pre></td></tr></table></figure><p>优点：</p><ul><li>密集、稳定、实现快</li><li>很适合用来观察“policy 在 reward 驱动下会怎么变形”</li></ul><p>风险（几乎必然出现 reward hacking，视频里也观察到了）：</p><ul><li>反复刷 “cat/cat/cat…”</li><li>输出结构与可读性变差</li><li>分布漂移后语言能力下降（不加 KL 时尤其明显）</li></ul><h3 id="4-1-奖励函数的“最小单元测试”：别等-RL-把你训歪了才发现">4.1 奖励函数的“最小单元测试”：别等 RL 把你训歪了才发现</h3><p>不管你用 RM、verifier 还是 embedding 相似度，只要它进了训练闭环，你就应该把它当成一个“会被攻击的系统组件”。最小的工程防线是做 reward 的单元测试：</p><ol><li><strong>极值样本检查</strong>：对同一批 prompt 采样一堆输出，把 reward 排序，人工看 Top/Bottom 各 20 条，问一句：高分真的更好吗？有没有明显投机模式？</li><li><strong>不变性测试</strong>：同一语义、不同表述（同义改写/打乱格式/加冗余词）reward 是否稳定？如果 reward 对“表面模式”过敏，RL 一定会放大它。</li><li><strong>长度敏感性</strong>：reward 和长度是否强相关？如果相关性很强，策略会先学会控长度而不是学会做任务。</li><li><strong>跨 prompt 泛化</strong>：把一部分 prompts 留出来不参与训练，只做周期性评测。reward 提升不代表泛化，可能只是对训练 prompts 的模式记忆。</li></ol><p>这几个检查成本很低，但能提前把 80% 的“训练后行为变怪”在训练前暴露出来。</p><p>所以你应该把它当成“看清机制的玩具环境”，不要把这种 reward 直接当 deep research 的终极评价函数。真实的 research agent reward 里至少还需要：</p><ul><li>结构化程度（是否按要求分段、是否有引用）</li><li>事实一致性（citation-grounded / verifier）</li><li>覆盖度（是否检索到关键文献、是否覆盖关键观点）</li><li>成本与时延（token/工具调用预算）</li><li>以及更可靠的自动评测闭环（否则 RL 只是在“钻漏洞”）</li></ul><hr><h2 id="5-从-REINFORCE-demo-到-PPO-RLHF：为什么-PPO-会把-KL-变成“每步的稠密代价”">5. 从 REINFORCE demo 到 PPO-RLHF：为什么 PPO 会把 KL 变成“每步的稠密代价”</h2><p>视频里顺便对比了 PPO 用在 LLM-RL/RLHF 时 reward 的一个典型写法（你在很多框架里都会看到）：</p><ul><li>终点 reward 来自 reward model：只在最后一步加上</li><li>中间每一步都加一个 token-level 的 KL 代价（把“偏离参考策略”当成稠密惩罚）</li></ul><p>一个常见的 token-level shaping 形式是：</p><p>$$r_t = -\beta\cdot\big(\log\pi_\theta(a_t|s_t)-\log\pi_{\text{ref}}(a_t|s_t)\big)$$</p><p>终点再加上 RM 的序列级 reward（简写成 $r_T \mathrel{+}=R_{\text{RM}}$）。</p><p>然后你会看到：</p><ul><li>用这些 $r_t$ 算 return-to-go：$G_t=\sum_{k\ge t}\gamma^{k-t}r_k$</li><li>再用 critic/value baseline 得到 advantage：$A_t=G_t - V(s_t)$</li><li>最后 actor 用 PPO-clip / GRPO 的目标更新</li></ul><p>这就把“KL 约束”从一个抽象正则，变成了“每一步都能提供训练信号的稠密 reward shaping”。</p><hr><h2 id="6-KL-penalty：它到底在约束什么">6. KL penalty：它到底在约束什么</h2><p>你可以把 KL penalty 理解成一个 prior / anchor：</p><ul><li><strong>不加 KL</strong>：策略会为了 reward 最大化而狂奔，快速漂移到奇怪分布（语言能力掉得也快）</li><li><strong>加 KL</strong>：允许为了 reward 改变，但要求“别离参考策略太远”，从而得到一个 trade-off</li></ul><p>在“显式 loss”写法下，经常写成：</p><p>$$\mathcal{L}(\theta)=\mathcal{L}_{PG}(\theta)+\beta\cdot \mathrm{KL}(\pi_\theta|\pi_{\text{ref}})$$</p><p>其中 $\beta$ 就是拨杆：</p><ul><li>$\beta$ 小：更激进追 reward，更容易 drift</li><li>$\beta$ 大：更像在原模型附近做 steering，稳但 reward 上升慢</li></ul><hr><h2 id="7-KL-计算细节：最常见的坑（视频-04-的重点）">7. KL 计算细节：最常见的坑（视频 04 的重点）</h2><p>视频里点名了两个实现层面的坑：</p><ol><li><strong>聚合方式</strong>：KL 是对类别分布的求和，不能在 vocab 维度做 mean</li><li><strong>方向</strong>：你到底在算 forward KL 还是 reverse KL，梯度性质完全不同</li></ol><h3 id="7-1-KL-的定义：sum-over-vocab，不是-mean">7.1 KL 的定义：sum over vocab，不是 mean</h3><p>对离散分布 $p,q$：</p><p>$$\mathrm{KL}(p|q)=\sum_i p_i(\log p_i-\log q_i)$$</p><p>注意是 <strong>sum over vocab</strong>。如果你写成 <code>mean(dim=-1)</code>，你相当于把 KL 除以了 vocab size（例如 50k），数值会小几个数量级，迫使你把 <code>kl_coef</code> 人为调得很大，训练动态会非常别扭。</p><p>这也是视频里提到的现象：当前实现的 KL 数值很小，乘以 vocab size 才接近“正常量级”。</p><h3 id="7-2-forward-vs-reverse：你算的是哪个-KL">7.2 forward vs reverse：你算的是哪个 KL</h3><p>先给你一个最不容易搞混的写法（建议你在代码里直接手算，而不是依赖 <code>F.kl_div</code> 的 input/target 语义）。</p><p>记：</p><ul><li><code>logp = log_softmax(logits_cur)</code>，<code>p = exp(logp)</code></li><li><code>logq = log_softmax(logits_ref)</code>，<code>q = exp(logq)</code></li></ul><p>forward KL（常见“约束新策略别偏离 ref”）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kl_fwd = (p * (logp - logq)).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)  <span class="comment"># KL(cur || ref)</span></span><br></pre></td></tr></table></figure><p>reverse KL：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kl_rev = (q * (logq - logp)).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)  <span class="comment"># KL(ref || cur)</span></span><br></pre></td></tr></table></figure><p>两者的梯度性质不同（直觉层面：fKL 更“cover”，rKL 更“mode-seeking”）。你到底该用哪一个，取决于你希望它把策略往哪里推。</p><p>视频里还提到一个工程现实：很多框架并不是算 full-distribution KL，而是用 sampled action 的 logprob 差来近似（更便宜），例如：</p><p>$$\widehat{\mathrm{KL}} \approx \mathbb{E}\big[\log\pi_\theta(a_t|s_t)-\log\pi_{\text{ref}}(a_t|s_t)\big]$$</p><p>注意：这是一个常用近似，但它并不等价于 full-distribution KL；你要知道自己在用哪一种。</p><h3 id="7-3-reinforce-align-py-里的具体问题（你在读代码时要意识到）">7.3 <code>reinforce_align.py</code> 里的具体问题（你在读代码时要意识到）</h3><p>脚本当前实现（第 100 行附近）是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kl_div = F.kl_div(</span><br><span class="line">    F.log_softmax(logits_active, dim=-<span class="number">1</span>),</span><br><span class="line">    F.log_softmax(ref_logits_active, dim=-<span class="number">1</span>),</span><br><span class="line">    reduction=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">    log_target=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">kl_div_accumulated[active_mask] += kl_div.mean(dim=-<span class="number">1</span>).unsqueeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里至少有两个你需要“显式做出选择”的点：</p><ol><li><code>mean(dim=-1)</code>：如上所述，这不符合 KL 的定义，通常应该改成 <code>sum(dim=-1)</code>，然后重新调 <code>KL_FACTOR</code>。</li><li><code>F.kl_div(input=logp_cur, target=logp_ref, log_target=True)</code> 实际算出来更接近 <code>KL(ref || cur)</code>（reverse KL）。如果你本意是 <code>KL(cur || ref)</code>，就不要用这段，直接手算最清晰。</li></ol><h3 id="7-4-为什么大家更常用-KL，而不是-JS-散度">7.4 为什么大家更常用 KL，而不是 JS 散度</h3><p>你之前问过一个很好的问题：训练里做约束/正则，为啥常用 KL，而不是 JS？</p><p>工程上最常见的原因通常有四个：</p><ol><li><strong>计算方便</strong>：KL 能直接用 <code>log_softmax</code> + <code>exp</code> 在一个 forward 里算出来；JS 需要构造混合分布 $m=\frac12(p+q)$，等价要算两次 KL 并且要显式拿到 $m$。</li><li><strong>和 trust region/自然梯度的关系更直接</strong>：TRPO 的约束项就是 KL；PPO/LLM-RL 生态沿用了“用 KL 控制更新幅度”这条主线。</li><li><strong>小步近似下足够用</strong>：你把策略变化限制在小范围内时，KL 作为局部度量已经能很好地起到“别跑太远”的效果（并且你只需要一个可调强度的惩罚项）。</li><li><strong>梯度与惩罚强度更“硬”</strong>：JS 是有界的（上限是 $\log 2$），当两个分布差得很远时，它的惩罚/梯度容易变得不够敏感；而 KL 是无界的，能在策略明显漂移时给出更强的约束信号（当然代价是它的非对称性需要你显式选方向）。</li></ol><p>不是说 JS 不能用，而是 KL 在“好算 + 好调 + 和已有理论/工程惯例对齐 + 在漂移时惩罚更硬”这几件事上更划算。</p><p>如果你担心 KL 的非对称性带来“mode-seeking/分布塌缩”（例如 rKL 更容易把概率压到少数模式），工程上更常见的解法是：</p><ol><li>用 entropy/采样温度保留探索；</li><li>监控并约束 <code>clip_fraction/approx_kl</code>，别让更新太激进；</li><li>必要时在 reward 侧引入多样性/覆盖度指标，而不是直接换成 JS。</li></ol><hr><h2 id="8-一个更“对定义”的-KL-实现（可直接替换脚本核心片段）">8. 一个更“对定义”的 KL 实现（可直接替换脚本核心片段）</h2><p>下面这段给的是 token-level <strong>forward KL</strong>（cur||ref），并且是 sum over vocab：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">logp = F.log_softmax(logits_active, dim=-<span class="number">1</span>)      <span class="comment"># [B_active, V]</span></span><br><span class="line">logq = F.log_softmax(ref_logits_active, dim=-<span class="number">1</span>)  <span class="comment"># [B_active, V]</span></span><br><span class="line">p = logp.exp()</span><br><span class="line">kl_fwd = (p * (logp - logq)).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)         <span class="comment"># [B_active]</span></span><br><span class="line"></span><br><span class="line">kl_div_accumulated[active_mask] += kl_fwd.unsqueeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如果你想要 reverse KL，把 $p$ 换成 $q=\exp(\text{logq})$，并把括号里的差翻过来即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">q = logq.exp()</span><br><span class="line">kl_rev = (q * (logq - logp)).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)         <span class="comment"># [B_active]</span></span><br></pre></td></tr></table></figure><p>做完这件事，你会发现：</p><ul><li>KL 数值会上一个量级（不再需要“乘 vocab size 才像样”）</li><li>你的 <code>KL_FACTOR</code> 需要重调（通常会变小很多）</li><li>KL 曲线会更可解释（不会出现“系数很大但 KL 还几乎为 0”的错觉）</li></ul><hr><h2 id="9-维护-PG-KL-loss-的工程-checklist（照着做基本不会炸）">9. 维护 PG/KL loss 的工程 checklist（照着做基本不会炸）</h2><p>结合视频 04 + 脚本经验，总结成一个很实用的 checklist：</p><ol><li>rollout 生成时至少维护：<ul><li><code>output_ids</code>（当前生成的 token 序列）</li><li><code>logp_cur(a_t|s_t)</code> 的累加（联合 logprob）</li><li><code>logits_cur</code> 与 <code>logits_ref</code>（如果你做 full-distribution KL）</li><li><code>active_mask</code>（EOS 后别继续采样/别继续累计 KL）</li></ul></li><li>reward 计算必须 <code>torch.no_grad()</code>，并且最好和 policy 网络解耦（否则梯度会串）</li><li>KL 必须明确：<ul><li>方向（fKL / rKL / sampled 近似）</li><li>聚合（vocab 维 sum；再按 token/seq 平均）</li></ul></li><li>训练日志优先看：<ul><li>reward/成功率（主）</li><li>KL（主）</li><li>输出样例（主，防止 reward hacking）</li><li>policy_loss 数值（辅助，不要迷信它单调）</li></ul></li><li>容易被忽略但很关键：<ul><li>tokenizer/model 要对齐（否则 decode 和 eos 行为会怪）</li><li>长度归一最好按有效 token 数（EOS 后别算）</li></ul></li></ol><hr><h2 id="10-你下一步怎么把它迁移到-Agentic-RL-Deep-Research">10. 你下一步怎么把它迁移到 Agentic RL / Deep Research</h2><p>这个 REINFORCE demo 的意义不是让你“真的用它训一个大模型”，而是让你能：</p><ul><li>从零写出 RL4LLM 的最小闭环（rollout -&gt; reward -&gt; PG -&gt; KL）</li><li>看懂 PPO/GRPO/verl 里 loss 组件到底在对应哪条公式</li><li>更重要的是：知道 reward/KL/聚合这些细节会怎样影响训练动态</li></ul><p>如果你要做 deep research agentic RL，我建议把精力优先投到：</p><ol><li><strong>评测闭环</strong>：reward 设计 + 自动评测的可信度（否则 RL 只是“骗指标”）</li><li><strong>action space 设计</strong>：对 research agent 来说，动作往往是“检索/阅读/引用/工具调用/计划”的离散决策，而不是直接训每个 token</li><li>再决定要不要训 token policy（LLM 本体）。很多实际系统完全不微调 LLM，而是把 RL 用在上层决策与工具策略上</li></ol><p>后续如果你希望我把这个 demo 按“更接近 PPO-RLHF 的形态”（dense reward、critic、advantage、clip objective、KL 监控）继续扩展成可跑的 notebook，我也可以在这个系列里把它一步步补齐。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://wp-a.github.io">WP</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AREINFORCE%204%20LLM%EF%BC%88Reward%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%20PG-KL%20Loss%20%E7%BB%86%E8%8A%82%EF%BC%89/">https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AREINFORCE%204%20LLM%EF%BC%88Reward%20%E8%AE%BE%E8%AE%A1%E4%B8%8E%20PG-KL%20Loss%20%E7%BB%86%E8%8A%82%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://wp-a.github.io" target="_blank">WPIRONMAN</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Agentic-RL/">Agentic RL</a><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/RLHF/">RLHF</a><a class="post-meta__tags" href="/tags/REINFORCE/">REINFORCE</a><a class="post-meta__tags" href="/tags/KL/">KL</a></div><div class="post-share"><div class="social-share" data-image="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/123.JPG" target="_blank"><img class="post-qr-code-img" src="/img/123.JPG" alt="微信" loading='lazy'></a><div class="post-qr-code-desc">微信</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9A%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA%EF%BC%88PG%20Loss%E3%80%81TRPO%E3%80%81PPO-Clip%EF%BC%89/" title="Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post" loading='lazy'><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）</div></div><div class="info-2"><div class="info-item-1">这一页是 Agentic RL 系列的导航与阅读路线图。定位是：用最少的前置，把你带到“看懂 PPO/PG loss 的公式，能对齐到代码实现，并能读懂训练日志与稳定性问题”。 配套视频与资料： 视频 01（工程组件）：BV1KFpczQEkA 视频 02（推导主线）：BV12NHJzTEoP 视频 03（PG 补充 / GRPO / 优势标准化 / On-policy）：BV18hsFzbEKJ 视频 04（REINFORCE 4 LLM / reward 设计 / PG+KL loss 细节）：BV1Ya1LB1EDM 视频 05（vLLM 推理参数 / 显存分析 / max_num_batched_tokens）：BV1QnSFBkEZU 视频 06（DeepSeekMath-v2 自我验证 / verifier + meta-verifier / 迭代验证与微调）：BV1AaSTBEEeS 视频 07（RLVR 的边界 / Base vs RL / pass@k / PPL / vLLM 评测细节）：BV1pWSvBtEAk veRL（verl 框架 /...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AvLLM%20%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E3%80%81%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%88max_num_batched_tokens%EF%BC%89/" title="Agentic RL：vLLM 参数配置、显存分析与性能调优（max_num_batched_tokens）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post" loading='lazy'><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Agentic RL：vLLM 参数配置、显存分析与性能调优（max_num_batched_tokens）</div></div><div class="info-2"><div class="info-item-1">这一篇对应视频 05：“vLLM 参数配置、显存分析与性能调优 max_num_batched_tokens”（BV1QnSFBkEZU）。 这期的核心不是“教你把服务跑起来”，而是给你一个可以复用的调参心智模型： vLLM 的显存到底被谁吃掉（权重 / KV cache / peak activation / CUDA Graph / 杂项）。 max_model_len、max_num_seqs、max_num_batched_tokens 之间到底是谁在限制并发与吞吐。 为什么 max_num_batched_tokens 既影响“吞吐”，又会反过来影响“能留给 KV cache 的空间”（因为它参与了 profile 的 peak activation 测量）。 系列导航： Agentic RL：系列导航（PG...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9APG%20Loss%20%E7%BB%84%E4%BB%B6%E8%AF%A6%E8%A7%A3/" title="Agentic RL：PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-09</div><div class="info-item-2">Agentic RL：PG Loss 组件详解（PPO-clip &#x2F; Dual-Clip &#x2F; Entropy &#x2F; KL &#x2F; 聚合）</div></div><div class="info-2"><div class="info-item-1">这篇文章把「Agentic RL / LLM-RL 训练里常见的 Policy Gradient (PG) loss 到底由哪些组件拼起来」讲清楚，重点解释： PPO-clip：为什么要 clip、clip 住了哪些情况、什么时候梯度为 0 Dual-clip：它在 PPO-clip 基础上到底“多 clip 了什么”，解决什么不稳定 Entropy / KL：为什么要加正则、权重怎么理解 聚合（aggregate）：token/sequence/group 维度的 sum/mean 会如何改变梯度尺度 推导补全（视频 02）：从 REINFORCE / PG 到 TRPO，再到 PPO-clip，解释 log pi、ratio、以及“为什么 PPO 公式里看起来没有 log” 本文偏“工程视角”：你看完应该能把这些项在代码里正确实现出来，并能解释训练曲线为什么会那样。 补充说明：下面不少“实现细节/监控指标/聚合模式”的表述，我会刻意对齐 verl 生态里常见写法。 我不会把任何视频/博客里的“经验区间”当成硬规则。对 LLM-RL...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9A%E5%88%86%E5%B8%83%E8%A7%86%E8%A7%92%E7%90%86%E8%A7%A3%20SFT%20%E4%B8%8E%20RL%EF%BC%88Forward-Reverse%20KL%E3%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E5%A5%96%E5%8A%B1%EF%BC%89/" title="Agentic RL：分布视角理解 SFT 与 RL（Forward&#x2F;Reverse KL、分布与奖励）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">Agentic RL：分布视角理解 SFT 与 RL（Forward&#x2F;Reverse KL、分布与奖励）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：【[Agentic RL] 10 分布的视角理解 LLM 的 SFT 训练和 RL 训练，Forward/Reverse KL，分布与奖励】（BV1WvrGBGEbf）。 我会按“分布匹配（distribution matching）”的视角，把 SFT 与 RL（RLHF/RLVR/RFT/GRPO/DAPO…）放到同一个坐标系里解释清楚： SFT 本质是 forward KL：用数据分布 $P_{\text{data}}$ 去“覆盖”模型分布 $\pi_\theta$（mode-covering）。 带 KL 正则的 RL 本质是 reverse KL：用模型分布 $\pi_\theta$ 去“追逐”一个由 reward 定义的目标分布 $P^*$（mode-seeking）。 你在工程里看到的 KL、entropy、以及“reward in loss vs in reward”，都只是这套分布视角的不同落地方式。 系列导航： Agentic RL：系列导航（PG...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9A%E9%87%8D%E6%96%B0%E7%90%86%E8%A7%A3%20DPO%EF%BC%88KL%20%E6%AD%A3%E5%88%99%20RL%E3%80%81%E9%9A%90%E5%BC%8F%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BC%BA%E9%99%B7%EF%BC%89/" title="Agentic RL：重新理解 DPO（KL 正则 RL、隐式奖励模型与缺陷）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">Agentic RL：重新理解 DPO（KL 正则 RL、隐式奖励模型与缺陷）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：【[Agentic RL] 11 重新理解 DPO，带 KL 正则的 RL 目标函数推导，隐式的奖励模型，DPO 可能的缺陷与不足】（BV1N16ZBuERA）。 我不会把它写成“逐句视频笔记”，而是把 DPO 放回一个更稳的框架里：KL 正则的 RL 目标函数与概率建模（Bradley-Terry）。你看完应该能回答这 3 个问题： DPO 到底在优化什么，它和 “reward - β KL” 的 RL 目标是什么关系？ 为什么说 DPO 有一个“隐式的奖励模型”（implicit RM），它是什么形式？ DPO 为什么会出现一些看起来反直觉的问题（reward hacking / 过拟合 / 生成质量掉 / 长度偏置），以及你在工程里怎么防。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 关联阅读（建议顺序）： 先把 KL-constrained RL 的分布视角打通（π* 与 P*） BT&#x2F;MLE 的概率建模视角（你会更容易理解 DPO 的 logistic...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AReward%20Model%20Insights%EF%BC%88Bradley-Terry%E3%80%81MLE%20%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89/" title="Agentic RL：Reward Model Insights（Bradley-Terry、MLE 与深度学习）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">Agentic RL：Reward Model Insights（Bradley-Terry、MLE 与深度学习）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：【[Agentic RL] [RM] 09 Reward Model insights，理解概率建模（Bradley-Terry）、MLE、深度学习的关系】（BV1z4vkBBEgD）。 我不会把它写成“逐句笔记”，而是把 RM 的核心拆成一个你能反复复用的框架： 最顶层是 MLE（决定 loss），中间层是 Bradley-Terry（赋予概率意义），最底层是 Deep Learning（提供函数逼近与表征能力）。 如果你把 Reward Model 当成“又一个神经网络”，你很容易在训练/调参/诊断上迷路；但只要把它当成一个概率模型的参数化实现，很多现象会瞬间可解释（包括：为什么 RM 会偏向“拉平”、为什么 KL 约束几乎是必需、为什么 intransitive 数据会让 RM 变钝）。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 关联阅读（建议顺序）： RL4LLM 最小闭环：reward + PG&#x2F;KL 的工程细节 PPO&#x2F;GRPO 中 KL...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20%E6%A0%B8%E5%BF%83%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88GRPO%E3%80%81RLOO%E3%80%81REINFORCE++%EF%BC%89%E4%B8%8E%20Baseline%20%E8%AE%BE%E8%AE%A1/" title="Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-09</div><div class="info-item-2">Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：“[veRL] 核心强化学习算法，GRPO、RLOO、REINFORCE++、REINFORCE++ baseline”（BV1d4Yvz4EXA）。 但我不会把它写成“视频逐句笔记”，而是把 veRL 里这些算法放到同一个坐标系里讲清楚： 它们本质上都在解同一个问题：不用训练 critic，也能把 outcome reward 变成稳定的更新信号。 它们真正的差异，主要集中在：baseline 怎么选、advantage 怎么归一、KL 怎么放。 你应该怎么选：不是看算法名字，而是看你的任务 reward 形态（稀疏/密集、噪声/可验证、是否能 group sampling）。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 建议先读（打通基础概念与日志诊断）： PG→TRPO→PPO：推导与代码对齐 PG loss 组件详解（PPO-clip &#x2F; KL &#x2F; Entropy &#x2F; 聚合） veRL...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&%20Entropy%EF%BC%89/" title="Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-09</div><div class="info-item-2">Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）</div></div><div class="info-2"><div class="info-item-1">这一篇对应 veRL 视频：“从原理层面理解训练参数，PPO &amp; GRPO，batch size，kl &amp; entropy”（BV1DZL1zNEN2）。 写这篇的目标不是“照着参数表翻译”，而是把你在 verl（veRL）里最常改、最容易踩坑的配置项，和它背后的算法对象一一对齐：你改的到底是“采样分布/有效 batch/更新步长/探索强度/保守性”，还是只是在调一个看起来像超参的数字。 我不会把视频内容当成“圣经”。这类讲解里最容易出错的两件事是： 把经验区间说成普适真理（例如某个指标“应该在 0.1-0.4”）。 把实现细节当成算法本身（例如 KL 的某种近似估计，被误当成 KL 的定义）。 所以本文会更强调“你应该如何用日志闭环验证”，而不是“照着配方抄参数”。一句话：参数之间强耦合，你改一个数字，往往同时改了“有效学习率、更新步长、数据复用程度和探索强度”。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 延伸阅读（更偏算法本体而不是参数）： veRL 核心算法：GRPO &#x2F; RLOO...</div></div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div><h1 id="site-title" fetchpriority="high" style="font-display:swap">Agentic RL：REINFORCE 4 LLM（Reward 设计与 PG+KL Loss 细节）</h1></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E8%B5%84%E6%96%99%E5%AF%B9%E9%BD%90%EF%BC%88%E8%A7%86%E9%A2%91-GitHub%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">0. 资料对齐（视频 + GitHub）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-1-%E8%BF%99%E4%B8%AA-demo-%E5%88%BB%E6%84%8F%E7%AE%80%E5%8C%96%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%88%E4%BD%A0%E8%BF%81%E7%A7%BB%E5%88%B0%E7%9C%9F%E5%AE%9E%E4%BB%BB%E5%8A%A1%E5%BF%85%E9%A1%BB%E8%A1%A5%E4%B8%8A%E4%BB%80%E4%B9%88%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">0.1 这个 demo 刻意简化了什么（你迁移到真实任务必须补上什么）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%8A%8A-LLM-%E7%9C%8B%E6%88%90-policy%EF%BC%9Astate-action-trajectory-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">3.</span> <span class="toc-text">1. 把 LLM 看成 policy：state&#x2F;action&#x2F;trajectory 是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-REINFORCE-PG-%E7%9A%84%E6%A0%B8%E5%BF%83%EF%BC%9A%E4%BD%A0%E4%B8%8D%E6%98%AF%E5%9C%A8%E6%9C%80%E5%B0%8F%E5%8C%96%E2%80%9C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-loss%E2%80%9D%EF%BC%8C%E8%80%8C%E6%98%AF%E5%9C%A8%E6%9E%84%E9%80%A0%E6%AD%A3%E7%A1%AE%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.</span> <span class="toc-text">2. REINFORCE&#x2F;PG 的核心：你不是在最小化“监督学习 loss”，而是在构造正确梯度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-LLM-%E7%9A%84%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%BB%B4%E6%8A%A4%E7%9A%84%E6%98%AF%E6%95%B4%E6%AE%B5%E8%BE%93%E5%87%BA%E7%9A%84-logprob%EF%BC%88%E8%80%8C%E4%B8%8D%E6%98%AF%E6%9F%90%E4%B8%AA-token-%E7%9A%84%E6%A6%82%E7%8E%87%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">3. LLM 的联合概率：为什么要维护的是整段输出的 logprob（而不是某个 token 的概率）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E9%95%BF%E5%BA%A6%E6%8E%A7%E5%88%B6%E7%9A%84%E4%B8%89%E7%A7%8D%E5%8A%9E%E6%B3%95%EF%BC%9A%E5%88%AB%E6%8A%8A%E2%80%9C%E5%BD%92%E4%B8%80%E2%80%9D%E5%BD%93%E6%88%90%E5%94%AF%E4%B8%80%E8%A7%A3"><span class="toc-number">5.1.</span> <span class="toc-text">3.1 长度控制的三种办法：别把“归一”当成唯一解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%BF%99%E4%B8%AA-demo-%E7%9A%84-reward%EF%BC%9Aembedding-cosine-similarity%EF%BC%88%E5%A5%BD%E7%94%A8%E4%BD%86%E5%8D%B1%E9%99%A9%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">4. 这个 demo 的 reward：embedding cosine similarity（好用但危险）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0%E7%9A%84%E2%80%9C%E6%9C%80%E5%B0%8F%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E2%80%9D%EF%BC%9A%E5%88%AB%E7%AD%89-RL-%E6%8A%8A%E4%BD%A0%E8%AE%AD%E6%AD%AA%E4%BA%86%E6%89%8D%E5%8F%91%E7%8E%B0"><span class="toc-number">6.1.</span> <span class="toc-text">4.1 奖励函数的“最小单元测试”：别等 RL 把你训歪了才发现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BB%8E-REINFORCE-demo-%E5%88%B0-PPO-RLHF%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88-PPO-%E4%BC%9A%E6%8A%8A-KL-%E5%8F%98%E6%88%90%E2%80%9C%E6%AF%8F%E6%AD%A5%E7%9A%84%E7%A8%A0%E5%AF%86%E4%BB%A3%E4%BB%B7%E2%80%9D"><span class="toc-number">7.</span> <span class="toc-text">5. 从 REINFORCE demo 到 PPO-RLHF：为什么 PPO 会把 KL 变成“每步的稠密代价”</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-KL-penalty%EF%BC%9A%E5%AE%83%E5%88%B0%E5%BA%95%E5%9C%A8%E7%BA%A6%E6%9D%9F%E4%BB%80%E4%B9%88"><span class="toc-number">8.</span> <span class="toc-text">6. KL penalty：它到底在约束什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-KL-%E8%AE%A1%E7%AE%97%E7%BB%86%E8%8A%82%EF%BC%9A%E6%9C%80%E5%B8%B8%E8%A7%81%E7%9A%84%E5%9D%91%EF%BC%88%E8%A7%86%E9%A2%91-04-%E7%9A%84%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">9.</span> <span class="toc-text">7. KL 计算细节：最常见的坑（视频 04 的重点）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-KL-%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%9Asum-over-vocab%EF%BC%8C%E4%B8%8D%E6%98%AF-mean"><span class="toc-number">9.1.</span> <span class="toc-text">7.1 KL 的定义：sum over vocab，不是 mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-forward-vs-reverse%EF%BC%9A%E4%BD%A0%E7%AE%97%E7%9A%84%E6%98%AF%E5%93%AA%E4%B8%AA-KL"><span class="toc-number">9.2.</span> <span class="toc-text">7.2 forward vs reverse：你算的是哪个 KL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-reinforce-align-py-%E9%87%8C%E7%9A%84%E5%85%B7%E4%BD%93%E9%97%AE%E9%A2%98%EF%BC%88%E4%BD%A0%E5%9C%A8%E8%AF%BB%E4%BB%A3%E7%A0%81%E6%97%B6%E8%A6%81%E6%84%8F%E8%AF%86%E5%88%B0%EF%BC%89"><span class="toc-number">9.3.</span> <span class="toc-text">7.3 reinforce_align.py 里的具体问题（你在读代码时要意识到）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A4%A7%E5%AE%B6%E6%9B%B4%E5%B8%B8%E7%94%A8-KL%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF-JS-%E6%95%A3%E5%BA%A6"><span class="toc-number">9.4.</span> <span class="toc-text">7.4 为什么大家更常用 KL，而不是 JS 散度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%B8%80%E4%B8%AA%E6%9B%B4%E2%80%9C%E5%AF%B9%E5%AE%9A%E4%B9%89%E2%80%9D%E7%9A%84-KL-%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%8F%AF%E7%9B%B4%E6%8E%A5%E6%9B%BF%E6%8D%A2%E8%84%9A%E6%9C%AC%E6%A0%B8%E5%BF%83%E7%89%87%E6%AE%B5%EF%BC%89"><span class="toc-number">10.</span> <span class="toc-text">8. 一个更“对定义”的 KL 实现（可直接替换脚本核心片段）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E7%BB%B4%E6%8A%A4-PG-KL-loss-%E7%9A%84%E5%B7%A5%E7%A8%8B-checklist%EF%BC%88%E7%85%A7%E7%9D%80%E5%81%9A%E5%9F%BA%E6%9C%AC%E4%B8%8D%E4%BC%9A%E7%82%B8%EF%BC%89"><span class="toc-number">11.</span> <span class="toc-text">9. 维护 PG&#x2F;KL loss 的工程 checklist（照着做基本不会炸）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E4%BD%A0%E4%B8%8B%E4%B8%80%E6%AD%A5%E6%80%8E%E4%B9%88%E6%8A%8A%E5%AE%83%E8%BF%81%E7%A7%BB%E5%88%B0-Agentic-RL-Deep-Research"><span class="toc-number">12.</span> <span class="toc-text">10. 你下一步怎么把它迁移到 Agentic RL &#x2F; Deep Research</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 - 2026 By WP</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js" defer></script><script src="/js/main.js" defer></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(()=>{var t=()=>{var t;window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise()):(window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],tags:"none"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(var e of document.querySelectorAll('script[type^="math/tex"]')){var a=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],a),n=document.createTextNode("");e.parentNode.replaceChild(n,e),a.start={node:n,delim:"",n:0},a.end={node:n,delim:"",n:0},t.math.push(a)}},""]}}},(t=document.createElement("script")).src="https://cdn.staticfile.org/mathjax/3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t))};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{var e=()=>{var e;0!==(e=document.querySelectorAll("pre > code.mermaid")).length&&e.forEach(e=>{var t=document.createElement("pre"),a=(t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent,document.createElement("div"));a.className="mermaid-wrap",a.appendChild(t),e.parentNode.replaceWith(a)});let t=document.querySelectorAll("#article-container .mermaid-wrap");0!==t.length&&(e=()=>(e=>{window.loadMermaid=!0;let n="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,t)=>{let a=e.firstElementChild;e=`%%{init:{ 'theme':'${n}'}}%%
`+a.textContent,t=mermaid.render("mermaid-"+t,e);let d=e=>{a.insertAdjacentHTML("afterend",e)};"string"==typeof t?d(t):t.then(({svg:e})=>d(e))})})(t),btf.addGlobalFn("themeChange",e,"mermaid"),window.loadMermaid?e():btf.getScript("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.1/mermaid.min.js").then(e))};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{let n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,o=null,a=(t,e)=>{n&&(window.shuoshuoComment.destroyValine=()=>{t.children.length&&(t.innerHTML="",t.classList.add("no-comment"))});e={el:"#vcomment",appId:"FXG14lTbR0Yj3W2kb3tkAt4L-gzGzoHsz",appKey:"hohJIUW6lOhfboJzq5FvG8z7",avatar:"monsterid",serverURLs:"https://fxg14ltb.lc-cn-n1-shared.com",emojiMaps:"",visitor:!1,...o,path:n?e:o&&o.path||window.location.pathname};new Valine(e)};var t=async(t,e)=>{"function"==typeof Valine||await btf.getScript("https://unpkg.com/valine@1.5.1/dist/Valine.min.js"),a(t,e)};n?window.shuoshuoComment={loadComment:t}:setTimeout(t,0)})()</script></div><script type="text/javascript" src="/js/reward.js" defer></script><script src="/js/fix-avatar.js" defer></script><script src="/js/blog-cool-features.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"]):not([href="/gallery/"]):not([href="/about/"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});let t=e=>{e&&Object.values(e).forEach(e=>e())};document.addEventListener("pjax:send",()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");var e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),t(window.globalFn.pjaxSend)}),document.addEventListener("pjax:complete",()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach(e=>{let t=document.createElement("script");var a=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(a)),e.parentNode.replaceChild(t,e)}),t(window.globalFn.pjaxComplete)}),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js" defer></script></div></div><script data-pjax src="/js/githubcalendar-loader.js?v=20251124"></script><script data-pjax>function GithubCalendarConfig(){var t=document.getElementById("recent-posts");t&&"/"==location.pathname&&(console.log("已挂载hexo-github-calendar https://github.com/Barry-Flynn/hexo-github-calendar"),t.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>')),GithubCalendar("https://github-calendar-api.meta-code.top/api?user=wp-a",["#ebedf0","#a2f7af","#6ce480","#54ad63","#469252","#31753c","#1f5f2a","#13531f","#084111","#032b09","#000000"],"wp-a")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:0}}</style><style></style></body></html>