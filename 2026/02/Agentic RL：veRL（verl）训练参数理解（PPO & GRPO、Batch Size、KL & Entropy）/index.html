<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy） | WPIRONMAN</title><meta name="author" content="WP"><meta name="copyright" content="WP"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这一篇对应 veRL 视频：“从原理层面理解训练参数，PPO &amp; GRPO，batch size，kl &amp; entropy”（BV1DZL1zNEN2）。 写这篇的目标不是“照着参数表翻译”，而是把你在 verl（veRL）里最常改、最容易踩坑的配置项，和它背后的算法对象一一对齐：你改的到底是“采样分布&#x2F;有效 batch&#x2F;更新步长&#x2F;探索强度&#x2F;保守性”，还是只是在调一个看起来像超参的"><meta property="og:type" content="article"><meta property="og:title" content="Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）"><meta property="og:url" content="https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&%20Entropy%EF%BC%89/index.html"><meta property="og:site_name" content="WPIRONMAN"><meta property="og:description" content="这一篇对应 veRL 视频：“从原理层面理解训练参数，PPO &amp; GRPO，batch size，kl &amp; entropy”（BV1DZL1zNEN2）。 写这篇的目标不是“照着参数表翻译”，而是把你在 verl（veRL）里最常改、最容易踩坑的配置项，和它背后的算法对象一一对齐：你改的到底是“采样分布&#x2F;有效 batch&#x2F;更新步长&#x2F;探索强度&#x2F;保守性”，还是只是在调一个看起来像超参的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp"><meta property="article:published_time" content="2026-02-09T13:40:00.000Z"><meta property="article:modified_time" content="2026-02-10T04:40:02.777Z"><meta property="article:author" content="WP"><meta property="article:tag" content="Agentic RL"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="PPO"><meta property="article:tag" content="KL"><meta property="article:tag" content="veRL"><meta property="article:tag" content="verl"><meta property="article:tag" content="GRPO"><meta property="article:tag" content="Batch Size"><meta property="article:tag" content="Entropy"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Agentic RL：veRL（verl）训练参数理解（PPO & GRPO、Batch Size、KL & Entropy）",
  "url": "https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&%20Entropy%EF%BC%89/",
  "image": "https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp",
  "datePublished": "2026-02-09T13:40:00.000Z",
  "dateModified": "2026-02-10T04:40:02.777Z",
  "author": [
    {
      "@type": "Person",
      "name": "WP",
      "url": "https://wp-a.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="https://wpironman.oss-cn-qingdao.aliyuncs.com/favicon.png"><link rel="canonical" href="https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&amp;%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&amp;%20Entropy%EF%BC%89/index.html"><link rel="preconnect" href="//cdnjs.cloudflare.com"><link rel="manifest" href="/null"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css"><script>(()=>{var e={set:(e,t,a)=>{a&&(a=Date.now()+864e5*a,localStorage.setItem(e,JSON.stringify({value:t,expiry:a})))},get:e=>{var t=localStorage.getItem(e);if(t){var{value:t,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return t;localStorage.removeItem(e)}}},t=(window.btf={saveToLocal:e,getScript:(o,n={})=>new Promise((e,t)=>{let a=document.createElement("script");a.src=o,a.async=!0,Object.entries(n).forEach(([e,t])=>a.setAttribute(e,t)),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),getCSS:(o,n)=>new Promise((e,t)=>{let a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),addGlobalFn:(e,t,a=!1,o=window)=>{var n=o.globalFn||{};n[e]=n[e]||{},n[e][a||Object.keys(n[e]).length]=t,o.globalFn=n}},()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")}),a=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")},o=(btf.activateDarkMode=t,btf.activateLightMode=a,e.get("theme")),t=("dark"===o?t():"light"===o&&a(),e.get("aside-status"));void 0!==t&&document.documentElement.classList.toggle("hide-aside","hide"===t);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Sans+SC:wght@400;500;600&amp;family=JetBrains+Mono:wght@400;500&amp;family=Roboto+Slab:wght@400;600;700&amp;display=swap" media="print" onload='this.media="all"'><script>let GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdnjs.cloudflare.com/ajax/libs/egjs-infinitegrid/4.12.0/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!0,islazyloadPlugin:!1,isAnchor:!0,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Agentic RL：veRL（verl）训练参数理解（PPO & GRPO、Batch Size、KL & Entropy）",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="/css/_custom/category/categories.css"><link rel="preconnect" href="https://fonts.loli.net" crossorigin><link rel="stylesheet" href="/css/categories.css"><link rel="stylesheet" href="/css/valine.css"><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/medium-style.css"><script>window.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("my-pv");e&&fetch("https://wpironman.top/pv").then(t=>t.json()).then(t=>{e.textContent=t.pv}).catch(()=>{e.textContent="获取失败"})})</script><link rel="preload" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" as="style" onload='this.rel="stylesheet"'><link href="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" as="image" crossorigin="anonymous"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image:url(https://wpironman.oss-cn-qingdao.aliyuncs.com/10year.webp)"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://wpironman.oss-cn-qingdao.aliyuncs.com/head.png" onerror='this.onerror=null,this.src="https://wpironman.oss-cn-qingdao.aliyuncs.com/head.gif"' alt="avatar" loading='lazy'></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">168</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/construction-detection/"><i class="fa-fw fas fa-hard-hat"></i><span> 工地检测</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-user-group"></i><span> 友链</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fa-solid fa-user-tie"></i><span> 本站友链</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i><span> 随机开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="fa-fw fa fa-taxi"></i><span> 异次元之旅</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/academic/"><i class="fa-fw fas fa-graduation-cap"></i><span> 学术主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/favicon.png" alt="Logo" loading='lazy'><span class="site-name">WPIRONMAN</span></a><a class="nav-page-title" href="/"><span class="site-name">Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-tools"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/construction-detection/"><i class="fa-fw fas fa-hard-hat"></i><span> 工地检测</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-user-group"></i><span> 友链</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fa-solid fa-user-tie"></i><span> 本站友链</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fa fa-subway"></i><span> 随机开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://travel.moe/go.html?travel=on"><i class="fa-fw fa fa-taxi"></i><span> 异次元之旅</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/academic/"><i class="fa-fw fas fa-graduation-cap"></i><span> 学术主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）<a class="post-edit-link" href="null_posts/Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）.md" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2026-02-09T13:40:00.000Z" title="发表于 2026-02-09 21:40:00">2026-02-09</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/">算法解析</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/">工程实践</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">5.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>这一篇对应 veRL 视频：<strong>“从原理层面理解训练参数，PPO &amp; GRPO，batch size，kl &amp; entropy”</strong>（BV1DZL1zNEN2）。</p><p>写这篇的目标不是“照着参数表翻译”，而是把你在 verl（veRL）里最常改、最容易踩坑的配置项，和它背后的算法对象一一对齐：你改的到底是“采样分布/有效 batch/更新步长/探索强度/保守性”，还是只是在调一个看起来像超参的数字。</p><p>我不会把视频内容当成“圣经”。这类讲解里最容易出错的两件事是：</p><ol><li><strong>把经验区间说成普适真理</strong>（例如某个指标“应该在 0.1-0.4”）。</li><li><strong>把实现细节当成算法本身</strong>（例如 KL 的某种近似估计，被误当成 KL 的定义）。</li></ol><p>所以本文会更强调“你应该如何用日志闭环验证”，而不是“照着配方抄参数”。一句话：<strong>参数之间强耦合</strong>，你改一个数字，往往同时改了“有效学习率、更新步长、数据复用程度和探索强度”。</p><p>系列导航：</p><ul><li><a href="/2026/02/Agentic%20RL%EF%BC%9A%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA%EF%BC%88PG%20Loss%E3%80%81TRPO%E3%80%81PPO-Clip%EF%BC%89/" title="Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）">Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip）</a></li></ul><p>延伸阅读（更偏算法本体而不是参数）：</p><ul><li><a href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20%E6%A0%B8%E5%BF%83%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88GRPO%E3%80%81RLOO%E3%80%81REINFORCE++%EF%BC%89%E4%B8%8E%20Baseline%20%E8%AE%BE%E8%AE%A1/" title="Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计">veRL 核心算法：GRPO &#x2F; RLOO &#x2F; REINFORCE++ 与 Baseline 设计</a></li></ul><hr><h2 id="0-资料对齐（视频-本地仓库）">0. 资料对齐（视频 + 本地仓库）</h2><ul><li>视频：<code>BV1DZL1zNEN2</code><ul><li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1DZL1zNEN2/">https://www.bilibili.com/video/BV1DZL1zNEN2/</a></li></ul></li><li>配套仓库（你本地路径）：<ul><li><code>/Users/wangpeng/Downloads/modern_genai_bilibili-main/agentic_rl/verl</code></li></ul></li><li>本文主要引用的笔记（按重要程度）：<ul><li><code>agentic_rl/verl/objectives/objectives_loss.ipynb</code>：PPO / KL / entropy / clip fraction / 训练时该看什么曲线</li><li><code>agentic_rl/verl/训练及调参经验/config-perf-tuning.ipynb</code>：mini/micro batch 切分、dynamic batch、loss scale</li><li><code>agentic_rl/verl/objectives/agg_loss.ipynb</code>：<code>loss_agg_mode</code> 为什么会改变“有效 batch”与长度偏置</li><li><code>agentic_rl/verl/objectives/review_grpo.ipynb</code>：PPO vs GRPO 直觉与 DeepSeek 风格细节（adv 标准化 / off-policy masking）</li><li><code>agentic_rl/verl/objectives/grpo_gspo.ipynb</code>：GRPO/GSPO 的“序列 vs token”缩放因子（更偏进阶，可选）</li><li><code>agentic_rl/verl/训练及调参经验/sft.ipynb</code>：<code>micro_batch_size_per_gpu</code> 才是显存真开关、长序列的 sequence parallel</li></ul></li></ul><p>verl 配置文档入口（笔记里引用的链接）：</p><ul><li><a target="_blank" rel="noopener" href="https://verl.readthedocs.io/en/latest/examples/config.html">https://verl.readthedocs.io/en/latest/examples/config.html</a></li></ul><hr><h2 id="0-1-先建立一个“不会被参数名带偏”的心智模型">0.1 先建立一个“不会被参数名带偏”的心智模型</h2><p>verl 的参数很多，但绝大多数可以映射回 4 个“你真正关心的杠杆”。我建议你以后看任何 RLHF/RLVR 框架，都先按这个分类理解：</p><ol><li><strong>采样分布（Sampling Distribution）</strong>：你让旧策略怎么生成数据。</li><li><strong>数据复用强度（Data Reuse / Off-policy Drift）</strong>：同一批 rollout 你要训练几轮，以及每轮切多细。</li><li><strong>更新幅度（Update Magnitude / Trust Region）</strong>：你允许策略一步走多远。</li><li><strong>探索强度（Exploration / Diversity）</strong>：你是否允许模型持续探索，而不是快速塌缩到单一模板。</li></ol><p>同一个“症状”（比如 reward 不涨）可以来自这四类中的任意一种原因。把它们混在一起调，最常见的结果是：短期变好，长期崩；或者“看起来更稳了”，其实只是探索被掐死。</p><h2 id="1-你在-verl-里训练-PPO-GRPO，本质做的是哪-4-件事">1. 你在 verl 里训练 PPO/GRPO，本质做的是哪 4 件事</h2><p>把实现细节都抹掉，verl 的 RL4LLM（PPO/GRPO）训练闭环可以压缩成 4 步：</p><ol><li><strong>Rollout（采样）</strong>：用旧策略 $\pi_{\theta_{\text{old}}}$ 生成 response（或者每个 prompt 生成一组 response）。</li><li><strong>打分（reward / verifier）</strong>：得到每条轨迹的 reward（verifiable reward / RM / rubric / verifier 都属于这一步）。</li><li><strong>信用分配（advantages）</strong>：<ul><li>PPO：靠 critic/GAE 把序列回报分解成 token 级 advantage；</li><li>GRPO：不用 critic，直接在“同一 prompt 的一组样本”内部做 baseline（group mean / std）。</li></ul></li><li><strong>优化（update）</strong>：对同一批 rollout 数据做若干个 epoch，按 mini-batch/micro-batch 切分，梯度累积，然后更新 actor（以及 PPO 的 critic）。</li></ol><p>所以当你看到“训练不稳定/显存爆/速度慢/指标不涨”，不要先猜玄学超参。先问自己它属于哪一步：</p><ul><li>rollout 太慢：推理引擎与 batching 问题；</li><li>reward 噪声大：评测/奖励设计问题；</li><li>advantage 质量差：baseline/标准化/critic 学不动；</li><li>update 太激进：batch/epoch/LR/clip/KL/entropy 的耦合问题。</li></ul><hr><h2 id="1-1-“On-policy”-在-LLM-RL-里经常是一个误导词">1.1 “On-policy” 在 LLM-RL 里经常是一个误导词</h2><p>很多人听到 PPO/GRPO 是 on-policy，就默认“只要是 on-policy 就稳”。但在工程里你几乎必然会做两件事，它们会把你推向 <strong>轻度 off-policy</strong>：</p><ol><li>一次 rollout 很贵，所以你会对同一批数据做多个 epoch 更新（<code>ppo_epochs &gt; 1</code>）。</li><li>一次 rollout 的 batch 很大，所以你会切成很多 mini/micro 去更新。</li></ol><p>当你跑到第 2/3 个 epoch 时，当前策略已经不是产生数据的那一个策略了。于是你会看到经典组合：</p><ul><li><code>clip fraction</code> 上升（大量 ratio 被裁到边界）；</li><li><code>approx_kl(old,new)</code> 上升（更新跨得更大）；</li><li>reward 未必更好（甚至变差），因为你可能在反复榨同一批数据，过拟合到了某些 reward hacking 模式。</li></ul><p>所以我更推荐你把 <code>ppo_epochs</code> 当成“数据复用强度旋钮”，而不是“训练更充分”的旋钮。更稳的调参顺序是：</p><ol><li>先用较小 <code>ppo_epochs</code> 跑通（例如 1-2），把 KL/entropy/clipfrac 的形态看明白。</li><li>只有当你确认更新非常保守（KL 很低、clipfrac 接近 0、reward 上不去）且 rollout 成本极高时，再考虑增加 epoch。</li></ol><h2 id="2-Batch-size：为什么你觉得改了-batch，结果像改了-learning-rate">2. Batch size：为什么你觉得改了 batch，结果像改了 learning rate</h2><h3 id="2-1-三个名字：Global-Mini-Micro（以及-GA）">2.1 三个名字：Global / Mini / Micro（以及 GA）</h3><p>从“优化器每次 step 用了多少样本”这个角度，你只需要记住三层：</p><ul><li><strong>Global batch（全局 batch）</strong>：一次参数更新（optimizer step）看到的样本总量。它决定梯度噪声大小与“每步更新有多稳”。</li><li><strong>Micro-batch（微 batch）</strong>：单卡一次 forward/backward 放得下的样本量。它几乎直接决定峰值显存。</li><li><strong>Gradient Accumulation（梯度累积，GA）</strong>：当 global &gt; micro_total 时，用多次 micro forward/backward 累积梯度，再做一次 optimizer step。</li></ul><p>在 verl 的笔记里也强调过一句很工程的话：</p><blockquote><p>真正影响显存的常常是 <code>micro_batch_size_per_gpu</code>，而不是 <code>train_batch_size</code>。</p></blockquote><p>（见 <code>agentic_rl/verl/训练及调参经验/sft.ipynb</code>）</p><h3 id="2-2-PPO-update-阶段：mini-batch-和-micro-batch-怎么对应">2.2 PPO update 阶段：mini-batch 和 micro-batch 怎么对应</h3><p><code>config-perf-tuning.ipynb</code> 把 actor 更新阶段的逻辑写得很直白：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.config.ppo_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(mini_batches):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_dynamic_bsz:</span><br><span class="line">            micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.gradient_accumulation = <span class="variable language_">self</span>.config.ppo_mini_batch_size // <span class="variable language_">self</span>.config.ppo_micro_batch_size_per_gpu</span><br><span class="line">            micro_batches = mini_batch.split(<span class="variable language_">self</span>.config.ppo_micro_batch_size_per_gpu)</span><br></pre></td></tr></table></figure><p>你可以直接把它翻译成一句话：</p><ul><li><code>ppo_mini_batch_size</code> 决定“这一次 update 的统计单位有多大”（更像优化器视角的 batch）。</li><li><code>ppo_micro_batch_size_per_gpu</code> 决定“单卡一次能塞多少”（更像显存视角的 batch）。</li><li>两者的比值决定 GA 次数：<code>mini // micro</code>。</li></ul><p>这也是为什么你会感觉“改 batch 像改了学习率”：</p><ul><li>在很多实现里，loss 默认是 mean reduction；</li><li>当你改变 micro/mini 的切分方式时，<strong>梯度尺度</strong>往往也被改变了；</li><li>于是相当于你隐式改变了“每次 step 的有效步长”。</li></ul><p>这里补一个更“从原理出发”的说法：</p><blockquote><p>在 SGD 视角下，batch size 决定的是梯度噪声大小与统计稳定性。你一旦改变 loss 的归一化方式（mean/sum、token/seq 聚合），batch size 的含义就会变形，进而等价成“有效学习率”的变化。</p></blockquote><h3 id="2-3-Dynamic-batch：按-token-budget-切分，比按样本数切分更靠谱">2.3 Dynamic batch：按 token budget 切分，比按样本数切分更靠谱</h3><p>LLM-RL 最大的工程痛点是序列长度差异极大：</p><ul><li>固定样本数切 micro-batch：长序列 batch OOM，短序列 batch 显存空闲，吞吐不稳；</li><li>所以 verl 提供 <code>use_dynamic_bsz=True</code>：不是“每个 micro-batch 固定 K 条样本”，而是“每个 micro-batch 固定 token 上限”。</li></ul><p>在笔记里，这个 token 上限被写成：</p><ul><li><code>ppo_max_token_len_per_gpu * ulysses_sequence_parallel_size</code></li></ul><p>这意味着：序列并行（SP）更多是为了<strong>省显存</strong>，不是为了加速；它把“每卡 token budget”摊到更多并行里。</p><h3 id="2-4-Dynamic-batch-必须配套一个东西：loss-scale（否则梯度有偏）">2.4 Dynamic batch 必须配套一个东西：loss scale（否则梯度有偏）</h3><p>Dynamic batch 里每个 micro-batch 的样本数 $|b_j|$ 不再相同。为了让最终梯度等价于“对整个 mini-batch 的平均”，verl 会对每次 micro 的 loss 乘一个 scale factor（笔记里给了对应代码解释）：</p><ul><li><code>loss_scale_factor = response_mask.shape[0] / self.config.ppo_mini_batch_size</code></li></ul><p>直觉上它做的是：这个 micro-batch 占整个 mini-batch 的比例是多少，就让它贡献相应比例的梯度。</p><p>如果你只开了 dynamic batch，但忽略了 scale，你训练出来的结果很可能“看似能跑，指标却怪”，因为长短样本被系统性重加权了。</p><h3 id="2-5-你真正的-compute-单位是-token，而不是-sample">2.5 你真正的 compute 单位是 token，而不是 sample</h3><p>在 LLM-RL 场景，“同样的 32 条样本”可能是完全不同的算力消耗与优化难度：长 CoT 的 token 数、激活保存、通信量都更大，也更容易触发 OOM 或梯度不稳定。</p><p>所以我建议你在日志/实验记录里把 batch 写成两行：</p><ol><li><code>num_samples</code>（样本条数）</li><li><code>num_tokens</code>（响应 token 总数，或平均 token）</li></ol><p>你会发现很多“看似随机”的不稳定，其实和 token 分布的长尾强相关。</p><hr><h2 id="3-PPO-vs-GRPO：同样是“policy-gradient”，参数含义完全不同">3. PPO vs GRPO：同样是“policy gradient”，参数含义完全不同</h2><h3 id="3-1-PPO：更精细，但你必须养得起-critic">3.1 PPO：更精细，但你必须养得起 critic</h3><p>PPO 的核心对象是 token 级别的 advantage $\hat A_t$，常见形式是 GAE：</p><p>$$\hat A_t = \delta_t + (\gamma\lambda)\delta_{t+1} + (\gamma\lambda)^2\delta_{t+2} + \cdots$$</p><p>所以 PPO 的“稳定”来自两个东西：</p><ul><li>critic 给 baseline：降低方差；</li><li>clip / KL trust region：限制一步更新幅度。</li></ul><p>代价也很明显：</p><ul><li>你要训练/维护 critic（更多显存、更多 compute、更多不稳定来源）。</li></ul><p>我个人判断 “PPO 值不值得上” 的标准是：你是否真的需要 token-level credit assignment。</p><ul><li>如果你的 reward 基本只在序列末端出现（典型 RLHF/RLVR），critic 很容易学成“差不多的常数”，advantage 质量不高，PPO 的复杂度未必值得。</li><li>如果你的 reward 可以拆成比较细的过程信号（例如 tool 选择、检索覆盖、证据一致性等中间指标），token-level baseline 才更可能带来稳定收益。</li></ul><h3 id="3-2-GRPO：没有-critic，你把-baseline-放进“同组样本”">3.2 GRPO：没有 critic，你把 baseline 放进“同组样本”</h3><p>GRPO 的典型设定是：对同一个 prompt 采样一组 completion（组大小记作 $G$），得到 reward ${R_{i,1}, \dots, R_{i,G}}$，然后用组内统计量构造 advantage。</p><p>你可以把最常见的版本记成：</p><p>$$\hat A_{i,j} = \frac{R_{i,j} - \text{mean}(\mathbf R_i)}{\text{std}(\mathbf R_i) + \epsilon}$$</p><p>笔记里也提到 DeepSeek 风格的一种简化：不除以 std，只做中心化：</p><p>$$\hat A_{i,j} = R_{i,j} - \text{mean}(\mathbf R_i)$$</p><p>所以 GRPO 里你改的“batch size”经常有两层含义：</p><ul><li>你每个 prompt 采样的组大小 $G$（决定 baseline 质量与方差）；</li><li>你一次 update 看到多少个 prompt（决定全局梯度噪声）。</li></ul><p>如果你把 PPO 的直觉（“batch 越大越稳”）直接搬到 GRPO 上，经常会误判，因为 GRPO 的关键瓶颈是“组内差异”是否足够大、baseline 是否有意义。</p><p>这里我补一个常被忽略的现实：<strong>组内差异很多时候来自采样温度，而不是来自模型能力</strong>。</p><p>如果你把 temperature/top_p 调得很保守，组内样本高度同质，你就会看到：</p><ul><li>reward std 很小；</li><li>advantage std 很小（甚至接近 0）；</li><li>训练看起来“很稳”，但 reward 也不涨，因为你几乎没探索到新的轨迹。</li></ul><p>反过来，如果你把采样调得太散，组内差异很大但正确率很低，你会得到高方差 advantage，训练会非常抖。</p><p>所以 GRPO 最核心的调参，往往不是“更大 batch”，而是 <strong>“合理的 group size + 合理的采样多样性”</strong>。</p><h3 id="3-3-一个很容易被忽略的参数：loss-聚合方式（loss-agg-mode）">3.3 一个很容易被忽略的参数：loss 聚合方式（<code>loss_agg_mode</code>）</h3><p><code>agg_loss.ipynb</code> 讲了一个非常重要但常被忽略的问题：<strong>你怎么把 token loss 聚合成一个标量</strong>，会系统性改变训练偏好。</p><p>例如 sample-level 的 <code>seq-mean-token-mean</code>，会把每个样本先除以序列长度；这在长 CoT 场景可能导致：</p><ul><li>长而正确的解被“平均”掉，梯度缩小；</li><li>短而凑巧的解反而更占优势（长度偏置）。</li></ul><p>这不是“理论洁癖”，而是会直接体现在你训练出来的输出风格上（短/长、是否愿意展开推理）。</p><p>verl 文档也明确提到：</p><ul><li>原始 GRPO 常用 sample-level；</li><li>DrGRPO / DAPO 等会改成更 token-level 的聚合来提升稳定性与减少长度偏置。</li></ul><p>我建议你把 <code>loss_agg_mode</code> 当成“优化目标定义的一部分”，而不是“实现细节”：</p><ul><li>你的 reward 如果本质是 sequence-level（例如 pass/fail），你用 per-token mean 去归一化，就相当于把目标改成了“单位 token 的平均收益最大化”，这会系统性偏向短输出。</li><li>如果你希望鼓励长 CoT 并且 reward 更像“总回报”，<code>token-mean</code> 往往更贴近直觉。</li></ul><p>这个问题没有绝对对错，只有“你想要的行为”。</p><hr><h2 id="4-KL-与-Entropy：一个管“别飘”，一个管“别死”">4. KL 与 Entropy：一个管“别飘”，一个管“别死”</h2><h3 id="4-1-KL：你需要先分清两种-KL">4.1 KL：你需要先分清两种 KL</h3><p>在 LLM-RL 里常出现两类 KL（含义完全不同）：</p><ol><li><strong>PPO 的 KL（new vs old）</strong>：衡量你这次更新跨得有多大，常配合 <code>clip fraction</code> 一起看，属于稳定性监控。</li><li><strong>RLHF/RLVR 的 KL（policy vs ref）</strong>：把策略约束在 reference（通常是 SFT/base）附近，防止语言漂移、reward hacking、分布坍缩。</li></ol><p>为了避免“大家都叫 KL，但每个人脑子里不是同一个东西”，我更喜欢把它们写成公式（LLM 里 action 就是 token）：</p><ul><li><strong>PPO 的 ratio 与 KL（old vs new）</strong>：<ul><li>$$r_t(\theta)=\exp(\log \pi_\theta(a_t|s_t) - \log \pi_{\theta_{\text{old}}}(a_t|s_t))$$</li><li>$$\widehat{\mathrm{KL}}(\pi_{\theta_{\text{old}}}|\pi_\theta)\approx \mathbb{E}_{a\sim \pi_{\theta_{\text{old}}}}[\log\pi_{\theta_{\text{old}}}(a|s)-\log\pi_\theta(a|s)]$$</li></ul></li><li><strong>对 reference 的 KL（policy vs ref）</strong>：<ul><li>$$\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})=\mathbb{E}_{a\sim \pi_\theta}[\log\pi_\theta(a|s)-\log\pi_{\text{ref}}(a|s)]$$</li></ul></li></ul><p>一个很重要但常被忽略的点：<strong>KL 的方向会改变偏好</strong>。在 RLHF/RLVR 里常用的 $\mathrm{KL}(\pi_\theta|\pi_{\text{ref}})$ 更像“mode-seeking”的约束，它天然会惩罚跑到 ref 低概率区域的行为，所以你在很多任务里会看到分布被削尖、输出多样性下降。这不是“实现问题”，而是 KL 方向本身带来的效应。</p><p><code>objectives_loss.ipynb</code> 里给的实现提示很直接：最终 policy loss 往往是“PG surrogate + KL loss”：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">policy_loss = policy_loss + kl_loss * <span class="variable language_">self</span>.config.kl_loss_coef</span><br></pre></td></tr></table></figure><p>你调 <code>kl_loss_coef</code> 的直觉可以粗暴理解为：</p><ul><li>越大：越保守，越像 SFT，越不容易 reward hacking，但也越难把 reward 推上去；</li><li>越小：越激进，更容易“学会取悦 reward”，同时也更容易跑飞。</li></ul><h3 id="4-2-为什么“KL-entropy”经常要一起调">4.2 为什么“KL + entropy”经常要一起调</h3><p><code>objectives_loss.ipynb</code> 里还强调了 entropy（探索强度）：</p><ul><li><code>entropy_coeff</code> 默认可能是 0；</li><li>但一旦 reward 非常稀疏或你的策略容易塌缩（只会输出一种模板），entropy 可能是救命的。</li></ul><p>entropy 的计算在笔记里也给了很清楚的 token 级公式（这里用直觉版）：</p><p>$$H(\pi(\cdot)) = -\sum_v p(v)\log p(v)$$</p><p>判读上你只需要记住：</p><ul><li>entropy 低到贴地：模型输出非常确定，探索不足，容易 mode collapse；</li><li>entropy 持续很高：策略没收敛，可能 reward 信号太弱或 update 被各种约束压没。</li></ul><h3 id="4-3-1-entropy-bonus-不是“补丁”，更像“探索预算”">4.3.1 entropy bonus 不是“补丁”，更像“探索预算”</h3><p>一个常见误区是：模型塌缩了，就把 <code>entropy_coeff</code> 加大。</p><p>entropy bonus 的副作用很明确：它会持续奖励“分布更平”。如果你的 reward 信号本身很弱，entropy 可能会把你推向“随机但多样”的区域，reward 更难学。</p><p>更好的策略通常是：</p><ol><li>先把 reward 设计到足够可学习（区分度、噪声、可验证性）。</li><li>再用 entropy 做“探索预算”，并考虑 schedule（前期大、后期小），而不是一把梭。</li></ol><h3 id="4-3-训练时你该盯的-4-条曲线（比“loss-变没变小”重要）">4.3 训练时你该盯的 4 条曲线（比“loss 变没变小”重要）</h3><p>结合 <code>objectives_loss.ipynb</code> 的建议，训练中优先看这几类：</p><ol><li><strong>reward / task metric curve</strong>（主指标）：别盯 policy loss 的绝对值。</li><li><strong>KL（对 ref + 对 old）</strong>：是否偏离过大、是否长期贴 0（步子太小）。</li><li><strong>clip fraction</strong>：它是个很敏感的“步子是否过猛”的指标，但没有普适黄金区间。长期过高通常意味着更新过猛（LR 大、epoch 多、优势尺度过大），长期贴 0 通常意味着更新过保守或 advantage 接近 0。</li><li><strong>actor entropy</strong>：探索是否快速坍缩。</li></ol><p>如果你愿意再加一条，盯 <code>adv/std</code> 或者 GRPO 的 reward std，能很快看出“baseline 是否还有意义”。</p><h3 id="4-4-别把-KL-系数当成常数“背配方”，更建议用闭环控制思路">4.4 别把 KL 系数当成常数“背配方”，更建议用闭环控制思路</h3><p>不同实现里 KL 的估计方式（token/seq 聚合、近似形式）并不完全一致，所以你不能把某个固定系数当成可迁移经验。</p><p>比起死守一个系数，我更建议你用控制思路：</p><ol><li>设一个你能接受的 <strong>KL 目标区间</strong>（对 ref 或对 old，看你要控制什么）。</li><li>观察 KL 与 reward 的耦合：KL 上去 reward 不涨，通常是更新在学坏；KL 很低 reward 不涨，通常是更新太保守或 reward 太弱。</li><li>让 <code>kl_loss_coef</code> 变成“为了把 KL 拉回目标区间”的旋钮，而不是“一个固定常数”。</li></ol><h3 id="4-5-采样温度-vs-entropy-bonus：别把同一件事调两次">4.5 采样温度 vs entropy bonus：别把同一件事调两次</h3><p>temperature/top_p 是 <strong>采样分布层面的探索</strong>；entropy bonus 是 <strong>优化目标层面的探索</strong>。它们经常被同时调，导致你以为自己在“调探索”，实际上是在两处重复施力。</p><p>一个更可诊断的做法是：</p><ol><li>先固定采样温度，用 entropy bonus 调出一个不会塌缩的训练。</li><li>再把 entropy bonus 固定住，用采样温度去控制“你愿意花多少算力去探索”。</li></ol><h3 id="4-6-两个常见误读（我建议你别照搬任何人的结论）">4.6 两个常见误读（我建议你别照搬任何人的结论）</h3><ol><li><strong>“clip fraction 0.1-0.4 是黄金区间”</strong>：这只是常见经验，不是定理。clipfrac 的合理范围依赖于 clip epsilon、advantage 尺度、epoch 数、以及你到底在算 token-level 还是 seq-level。</li><li><strong>“policy_loss 的正负代表训练好坏”</strong>：policy loss 是 surrogate，且数据分布在变。你更应该看 reward、KL、entropy、ratio 分布形态，以及“指标是否可复现”。</li></ol><hr><h2 id="5-你要把-verl-用在-agentic-RL-deep-research，上来就该怎么改参数">5. 你要把 verl 用在 agentic RL / deep research，上来就该怎么改参数</h2><p>把“深研究 agent”抽象成一个 RL 任务，最常见的失败模式是：reward 难、噪声大、探索空间巨大。</p><p>因此一个更稳的起手式通常是：</p><ol><li>先把 rollout 推理打通并做吞吐优化（你已经有 vLLM 那篇）。</li><li>在 update 侧优先保证“不会跑飞”：KL/clip/epoch 不要太激进。</li><li>在 reward/advantage 侧优先保证“有有效差异”：GRPO 的组内方差、或者 verifier 的区分能力。</li></ol><p>这里我再补一个更“像研究”的建议：<strong>用最小实验矩阵把耦合拆开</strong>。例如固定三样只动一样：</p><ul><li>固定 sampling（temperature/group size），只调 update（epochs/clip/KL）；</li><li>固定 update，只调 sampling；</li><li>固定两者，只改 reward/advantage（baseline/标准化/长度归一化）。</li></ul><p>这样你才知道“系统到底在优化什么”，也更容易判断视频里的说法是否适用于你的任务。</p><hr><h2 id="6-verl-veRL-框架层面的关键点（比单个超参更重要）">6. verl/veRL 框架层面的关键点（比单个超参更重要）</h2><p>这一节不是“照视频”，而是我认为 verl 真正强的地方：它把 rollout 的系统问题（长尾、并发、训练-推理资源争用、tokenization 一致性）当成一等公民处理。对 agentic RL 来说，这往往比你选 PPO 还是 GRPO 更关键。</p><h3 id="6-1-为什么-agent-rollout-倾向-async：长尾（straggler）会吃掉你的-GPU">6.1 为什么 agent rollout 倾向 async：长尾（straggler）会吃掉你的 GPU</h3><p>agent 任务里不同样本完成 rollout 的时间差距极大：简单样本一轮对话结束，复杂样本可能多轮推理 + 多次工具调用 + 报错重试。如果用同步 batching，整批会被最慢样本拖死，GPU 大量空转。</p><p>verl 的 AgentLoop 体系用 async 并发把“快任务”和“慢任务”交织起来，本质是在解决这个系统瓶颈（见 <code>agentic_rl/verl/agent/agent_loop_details.ipynb</code>）。</p><h3 id="6-2-token-in-token-out：训练-推理一致性不是细节，是收敛性问题">6.2 token-in/token-out：训练-推理一致性不是细节，是收敛性问题</h3><p><code>agentic_rl/verl/tokenizer/encode-decode.ipynb</code> 里强调：<code>encode(messages)</code> 并不等价于 <code>prompt_ids ⊕ response_ids</code>，decode 再 encode 也可能不可逆。在 RL 训练里，这会导致 trajectory 偏离策略分布，严重时 PPO 甚至不收敛。</p><p>因此在多轮 agent rollout 里，verl 更倾向 token-in/token-out：用 token ids 作为接口，避免 chat template 反复 encode/decode 的不一致。</p><h3 id="6-3-tool-返回的-token-不参与-policy-gradient（response-mask）">6.3 tool 返回的 token 不参与 policy gradient（response_mask）</h3><p>AgentLoop 里常见做法是维护 <code>response_mask</code>：</p><ul><li>LLM 生成的 token：mask=1（参与 loss）</li><li>Tool 返回的 token：mask=0（不参与 loss）</li></ul><p>这不是“实现偏好”，而是你必须明确：你究竟在优化 LLM 的什么行为。把工具输出当作 action token 来回传梯度，会把“环境反应”错当成“策略行为”，梯度变得没有意义。</p><h3 id="6-4-Hybrid-模式：同一批-GPU-在训练和推理之间分时复用">6.4 Hybrid 模式：同一批 GPU 在训练和推理之间分时复用</h3><p>verl 里有 <code>wake_up/sleep</code> 机制，用于在 rollout（vLLM/TP，占 KV cache）和 train（FSDP，占梯度/优化器状态）之间切换资源。这类机制是否好用，取决于你的集群形态：</p><ul><li>推理训练分离（standalone）：架构更简单；</li><li>同卡复用（hybrid）：吞吐可能更高，但工程复杂度与系统抖动也更高，调参必须把“系统因素”算进去。</li></ul><hr><h2 id="7-一个更靠谱的“调参诊断表”：用现象定位到旋钮">7. 一个更靠谱的“调参诊断表”：用现象定位到旋钮</h2><p>下面这张表不是配方，是把“症状 -&gt; 可能原因 -&gt; 优先改什么”串起来。你不需要完全照做，但建议你每次改参数都能回答：我是在动哪类旋钮（采样/复用/更新/探索），我预期哪个日志指标会怎么变。</p><ol><li><strong>reward 完全不涨，KL(old,new) 很低，clipfrac 接近 0，entropy 也不高</strong><br>可能原因：更新太保守或 advantage 接近 0（组内样本太同质 / reward 无区分度）。<br>优先动作：降低 <code>kl_loss_coef</code> 或稍增 LR/clip epsilon；提高采样多样性（temperature/top_p 或 group size）；检查 reward std / adv std。</li><li><strong>reward 短期上涨后崩，KL(ref) 快速上升，entropy 下降很快</strong><br>可能原因：在学 reward hacking 或走出 ref 分布太快。<br>优先动作：增大 <code>kl_loss_coef</code>；减少 <code>ppo_epochs</code>；降低 LR；检查 reward 设计是否过度偏向格式/捷径。</li><li><strong>clipfrac 很高但 KL(old,new) 不高</strong><br>可能原因：同一批数据被多 epoch 复用，ratio 被推到 clip 边界，但整体 KL 还没很大（或被聚合方式稀释）。<br>优先动作：先减 <code>ppo_epochs</code>；再看是否需要增大 <code>ppo_mini_batch_size</code> 或调整 <code>loss_agg_mode</code>；检查 advantage 尺度是否过大（reward scale / 标准化）。</li><li><strong>GRPO 训练“很稳但没学到”，reward std 很小</strong><br>可能原因：采样太确定，组内没有有效对比；或者 reward 太粗。<br>优先动作：提高采样温度或 group size；把 reward 拆成更可学习的分项（哪怕是 proxy）；必要时引入 verifier/自验证提升区分度。</li><li><strong>一切看起来都正常，但换成多轮 agent 就完全不收敛</strong><br>可能原因：tokenization/trajectory 不一致（chat template encode/decode），或者 tool 输出误入梯度。<br>优先动作：确保 token-in/token-out；确认 <code>response_mask</code> 把 tool token 排除；优先把单轮收敛后再扩到多轮。</li></ol><p>如果你后续希望我把“verl 的 agent loop + rollout + reward manager + objective”按代码路径拆成能复现的工程笔记，我建议下一篇从这些 notebook 开始：</p><ul><li><code>agentic_rl/verl/agent/agent_loop_arch.ipynb</code></li><li><code>agentic_rl/verl/agent/agent_loop_details.ipynb</code></li></ul><p>它们会把“训练循环怎么把 agent 的 tool 交互排除在梯度外、怎么做 async rollout、怎么做 continuous batching”讲得更像一套系统，而不是零散参数。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://wp-a.github.io">WP</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&amp;%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&amp;%20Entropy%EF%BC%89/">https://wp-a.github.io/2026/02/Agentic%20RL%EF%BC%9AveRL%EF%BC%88verl%EF%BC%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%EF%BC%88PPO%20&amp;%20GRPO%E3%80%81Batch%20Size%E3%80%81KL%20&amp;%20Entropy%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://wp-a.github.io" target="_blank">WPIRONMAN</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Agentic-RL/">Agentic RL</a><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="post-meta__tags" href="/tags/PPO/">PPO</a><a class="post-meta__tags" href="/tags/KL/">KL</a><a class="post-meta__tags" href="/tags/veRL/">veRL</a><a class="post-meta__tags" href="/tags/verl/">verl</a><a class="post-meta__tags" href="/tags/GRPO/">GRPO</a><a class="post-meta__tags" href="/tags/Batch-Size/">Batch Size</a><a class="post-meta__tags" href="/tags/Entropy/">Entropy</a></div><div class="post-share"><div class="social-share" data-image="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/123.JPG" target="_blank"><img class="post-qr-code-img" src="/img/123.JPG" alt="微信" loading='lazy'></a><div class="post-qr-code-desc">微信</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9ARLVR%20%E7%9A%84%E8%BE%B9%E7%95%8C%EF%BC%88Base%20vs%20RL%E3%80%81pass@k%E3%80%81PPL%20%E4%B8%8E%20vLLM%20%E8%AF%84%E6%B5%8B%E7%BB%86%E8%8A%82%EF%BC%89/" title="Agentic RL：RLVR 的边界（Base vs RL、pass@k、PPL 与 vLLM 评测细节）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post" loading='lazy'><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Agentic RL：RLVR 的边界（Base vs RL、pass@k、PPL 与 vLLM 评测细节）</div></div><div class="info-2"><div class="info-item-1">这一篇对应视频 07：“limits of RLVR，base vs. RL, pass@k, ppl 基于 vLLM 计算细节以及采样效率”（BV1pWSvBtEAk）。 我把它拆成三条主线： Base vs RL 的“能力”到底在对比什么：RLVR 更像分布削尖（distribution sharpening）还是能力外推（capability uplift）？ 为什么一定要看 pass@k 而不只看 pass@1：以及怎么低方差地估算整条 pass@k 曲线。 怎么用 vLLM 可靠地算 PPL / entropy（评测细节）：不踩坑地得到能解释现象的指标。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 配套资料（你本地已有）： 视频 07：BV1pWSvBtEAk https://www.bilibili.com/video/BV1pWSvBtEAk/ 代码/笔记仓库（对应你本地下载的...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20%E6%A0%B8%E5%BF%83%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88GRPO%E3%80%81RLOO%E3%80%81REINFORCE++%EF%BC%89%E4%B8%8E%20Baseline%20%E8%AE%BE%E8%AE%A1/" title="Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post" loading='lazy'><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：“[veRL] 核心强化学习算法，GRPO、RLOO、REINFORCE++、REINFORCE++ baseline”（BV1d4Yvz4EXA）。 但我不会把它写成“视频逐句笔记”，而是把 veRL 里这些算法放到同一个坐标系里讲清楚： 它们本质上都在解同一个问题：不用训练 critic，也能把 outcome reward 变成稳定的更新信号。 它们真正的差异，主要集中在：baseline 怎么选、advantage 怎么归一、KL 怎么放。 你应该怎么选：不是看算法名字，而是看你的任务 reward 形态（稀疏/密集、噪声/可验证、是否能 group sampling）。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 建议先读（打通基础概念与日志诊断）： PG→TRPO→PPO：推导与代码对齐 PG loss 组件详解（PPO-clip &#x2F; KL &#x2F; Entropy &#x2F; 聚合） veRL...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20AgentLoop%20%E5%85%A8%E6%B5%81%E7%A8%8B%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%BB%86%E8%8A%82%EF%BC%88Async%20Rollout%E3%80%81%E7%8A%B6%E6%80%81%E6%9C%BA%E3%80%81Tool-Interaction%EF%BC%89/" title="Agentic RL：veRL AgentLoop 全流程与计算细节（Async Rollout、状态机、Tool-Interaction）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">Agentic RL：veRL AgentLoop 全流程与计算细节（Async Rollout、状态机、Tool-Interaction）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：【[Agentic RL] 14 verl AgentLoop 全流程与计算细节，async rollout 实现，状态机，tool / interaction】（BV18d6sBpEZq）。 我会用“系统 + 数据契约”的方式把 AgentLoop 讲清楚：不是复述视频，而是把你真正会卡住的点拆开，直到你能做到： 读 AgentLoop 相关代码时，能快速定位“当前在数据流的哪一段”。 你能解释清楚：为什么 agent loop 输出必须是交错轨迹（LLM token + tool obs token），以及为什么必须带 response_mask。 你能把“tool / interaction / termination / reward attach”这几个最容易写乱的逻辑写成一套可 debug 的状态机。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 建议先读（否则你会觉得本文有点“infra 细节过密”）： 12 先建立 AgentLoop 的架构直觉（async &#x2F; sticky...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20Infra%20AgentLoop%EF%BC%88AgentLoopManager%E3%80%81Async%20Rollout%20%E4%B8%8E%20Hybrid%20%E6%8E%A8%E8%AE%AD%EF%BC%89/" title="Agentic RL：veRL Infra AgentLoop（AgentLoopManager、Async Rollout 与 Hybrid 推训）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">Agentic RL：veRL Infra AgentLoop（AgentLoopManager、Async Rollout 与 Hybrid 推训）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：【[Agentic RL] 12 verl infra AgentLoop 基本概念及流程，AgentLoopManager，hybrid训练与推理】（BV135zrBaEEU）。 如果你已经看完我在上一篇里写的 “Agent Loop 为什么需要 async rollout” 与 “response_mask 基本概念”，那么这篇就是 infra 深挖版：把 verl 的 AgentLoop 体系从“能用”讲到“你能改、能调、能排障”。 你看完应该能回答这些工程问题： AgentLoopManager / Worker / AgentLoop / AsyncLLMServerManager 各自负责什么，边界怎么划？ 为什么 async rollout 不是优化项，而是 multi-turn tool use 的必要条件？它和 vLLM 的 continuous batching 怎么配合？ sticky session 为什么必须有？它和 prefix cache、load balancing 是什么关系？ “hybrid...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20%E6%A0%B8%E5%BF%83%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88GRPO%E3%80%81RLOO%E3%80%81REINFORCE++%EF%BC%89%E4%B8%8E%20Baseline%20%E8%AE%BE%E8%AE%A1/" title="Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-09</div><div class="info-item-2">Agentic RL：veRL 核心强化学习算法（GRPO、RLOO、REINFORCE++）与 Baseline 设计</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：“[veRL] 核心强化学习算法，GRPO、RLOO、REINFORCE++、REINFORCE++ baseline”（BV1d4Yvz4EXA）。 但我不会把它写成“视频逐句笔记”，而是把 veRL 里这些算法放到同一个坐标系里讲清楚： 它们本质上都在解同一个问题：不用训练 critic，也能把 outcome reward 变成稳定的更新信号。 它们真正的差异，主要集中在：baseline 怎么选、advantage 怎么归一、KL 怎么放。 你应该怎么选：不是看算法名字，而是看你的任务 reward 形态（稀疏/密集、噪声/可验证、是否能 group sampling）。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 建议先读（打通基础概念与日志诊断）： PG→TRPO→PPO：推导与代码对齐 PG loss 组件详解（PPO-clip &#x2F; KL &#x2F; Entropy &#x2F; 聚合） veRL...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9ATokenizer%20%E7%BC%96%E8%A7%A3%E7%A0%81%E9%9D%9E%E5%AF%B9%E7%A7%B0%E6%80%A7%E4%B8%8E%20Token-in-Token-out%EF%BC%88RL%20%E8%AE%AD%E7%BB%83%E5%B4%A9%E6%BA%83%E7%9A%84%E6%A0%B9%E5%9B%A0%EF%BC%89/" title="Agentic RL：Tokenizer 编解码非对称性与 Token-in-Token-out（RL 训练崩溃的根因）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-10</div><div class="info-item-2">Agentic RL：Tokenizer 编解码非对称性与 Token-in-Token-out（RL 训练崩溃的根因）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：【[veRL] tokenizer 编解码的非对称性，RL 训练崩溃到 Agent loop 中的 token in token out】（BV1b2pDzYEY2）。 我不会把它写成“视频复述”，而是把它抽象成一个你做 RL4LLM / Agentic RL / Multi-turn Tool Use 一定会遇到的工程定律： 在 RL 训练里，token_ids 才是“行为（action）”本体；把它 decode 成文本、再 encode 回去，往往已经不是同一个行为了。 一旦你在 rollout 的链路里出现 decode → encode（尤其是 multi-turn），你就可能让 PPO/GRPO 训练变成“在错误分布上算 logprob”，表现为： approx_kl/clipfrac/loss 统计异常 reward curve 不上升，甚至彻底不收敛 multi-turn agent loop 越跑越乱（历史拼接后 token 逐步漂移） 系列导航： Agentic RL：系列导航（PG...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20FSDP%20SFT%20Trainer%20%E8%A1%A5%E5%85%85%EF%BC%88Teacher%20Forcing%E3%80%81Shift%20Labels-Logits%E3%80%81Loss%20Mask%EF%BC%89/" title="Agentic RL：veRL FSDP SFT Trainer 补充（Teacher Forcing、Shift Labels&#x2F;Logits、Loss Mask）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-09</div><div class="info-item-2">Agentic RL：veRL FSDP SFT Trainer 补充（Teacher Forcing、Shift Labels&#x2F;Logits、Loss Mask）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：“[veRL] fsdp sft trainer 补充，teacher forcing、shift labels shift logits、loss mask”（BV1eWjtzbEdP）。 它是上一篇 SFT trainer 文章的“补充篇”，专门把三个最容易写错、但一错就会把模型训歪的细节讲透： Teacher forcing：SFT 到底在“喂什么”给模型，喂错会导致什么偏差。 Shift labels / shift logits：为什么 causal LM 的 CE loss 天生存在“错一位”，实现里你必须显式对齐。 Loss mask：multi-turn + tool-use 数据里，你到底要监督哪些 token；mask 在 shift 前后怎么对齐。 系列导航： Agentic RL：系列导航（PG Loss、TRPO、PPO-Clip） 关联阅读（建议先看主篇再看补充）： veRL：FSDP SFT Trainer 主篇（交叉熵 &#x2F; loss mask &#x2F;...</div></div></div></a><a class="pagination-related" href="/2026/02/Agentic%20RL%EF%BC%9AveRL%20FSDP%20SFT%20Trainer%EF%BC%88SFT%20vs%20RL%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E3%80%81Loss%20Mask%E3%80%81LR%20Scheduler%EF%BC%89/" title="Agentic RL：veRL FSDP SFT Trainer（SFT vs RL、交叉熵损失、Loss Mask、LR Scheduler）"><img class="cover" src="https://wpironman.oss-cn-qingdao.aliyuncs.com/1.webp" alt="cover" loading='lazy'><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-09</div><div class="info-item-2">Agentic RL：veRL FSDP SFT Trainer（SFT vs RL、交叉熵损失、Loss Mask、LR Scheduler）</div></div><div class="info-2"><div class="info-item-1">这篇文章对应视频：“[veRL] FSDP SFT trainer，SFT vs. RL，交叉熵损失 | loss mask | learning rate scheduler”（BV1CkJgzAEAG）。 补充篇（更聚焦 teacher forcing / shift labels-logits / loss mask 对齐）： veRL：FSDP SFT Trainer 补充（Teacher Forcing &#x2F; Shift &#x2F; Loss Mask） 进一步把 SFT 接到 tool-use agent 的 cold start（MultiTurn Tool Use / Coding Agent）： veRL：MultiTurn Tool Use &#x2F; Coding Agent SFT（Cold Start for RL） 但我会把它写成一份“可落地的工程读物”，而不是视频逐句复刻。你看完应该能回答这些问题： 为什么做 agentic RL / RLHF 之前，SFT 反而是你最不该糊弄的一步？ causal LM...</div></div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div><h1 id="site-title" fetchpriority="high" style="font-display:swap">Agentic RL：veRL（verl）训练参数理解（PPO &amp; GRPO、Batch Size、KL &amp; Entropy）</h1></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E8%B5%84%E6%96%99%E5%AF%B9%E9%BD%90%EF%BC%88%E8%A7%86%E9%A2%91-%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">0. 资料对齐（视频 + 本地仓库）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-1-%E5%85%88%E5%BB%BA%E7%AB%8B%E4%B8%80%E4%B8%AA%E2%80%9C%E4%B8%8D%E4%BC%9A%E8%A2%AB%E5%8F%82%E6%95%B0%E5%90%8D%E5%B8%A6%E5%81%8F%E2%80%9D%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">0.1 先建立一个“不会被参数名带偏”的心智模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BD%A0%E5%9C%A8-verl-%E9%87%8C%E8%AE%AD%E7%BB%83-PPO-GRPO%EF%BC%8C%E6%9C%AC%E8%B4%A8%E5%81%9A%E7%9A%84%E6%98%AF%E5%93%AA-4-%E4%BB%B6%E4%BA%8B"><span class="toc-number">3.</span> <span class="toc-text">1. 你在 verl 里训练 PPO&#x2F;GRPO，本质做的是哪 4 件事</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E2%80%9COn-policy%E2%80%9D-%E5%9C%A8-LLM-RL-%E9%87%8C%E7%BB%8F%E5%B8%B8%E6%98%AF%E4%B8%80%E4%B8%AA%E8%AF%AF%E5%AF%BC%E8%AF%8D"><span class="toc-number">4.</span> <span class="toc-text">1.1 “On-policy” 在 LLM-RL 里经常是一个误导词</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Batch-size%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E8%A7%89%E5%BE%97%E6%94%B9%E4%BA%86-batch%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%83%8F%E6%94%B9%E4%BA%86-learning-rate"><span class="toc-number">5.</span> <span class="toc-text">2. Batch size：为什么你觉得改了 batch，结果像改了 learning rate</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E4%B8%89%E4%B8%AA%E5%90%8D%E5%AD%97%EF%BC%9AGlobal-Mini-Micro%EF%BC%88%E4%BB%A5%E5%8F%8A-GA%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">2.1 三个名字：Global &#x2F; Mini &#x2F; Micro（以及 GA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-PPO-update-%E9%98%B6%E6%AE%B5%EF%BC%9Amini-batch-%E5%92%8C-micro-batch-%E6%80%8E%E4%B9%88%E5%AF%B9%E5%BA%94"><span class="toc-number">5.2.</span> <span class="toc-text">2.2 PPO update 阶段：mini-batch 和 micro-batch 怎么对应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Dynamic-batch%EF%BC%9A%E6%8C%89-token-budget-%E5%88%87%E5%88%86%EF%BC%8C%E6%AF%94%E6%8C%89%E6%A0%B7%E6%9C%AC%E6%95%B0%E5%88%87%E5%88%86%E6%9B%B4%E9%9D%A0%E8%B0%B1"><span class="toc-number">5.3.</span> <span class="toc-text">2.3 Dynamic batch：按 token budget 切分，比按样本数切分更靠谱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Dynamic-batch-%E5%BF%85%E9%A1%BB%E9%85%8D%E5%A5%97%E4%B8%80%E4%B8%AA%E4%B8%9C%E8%A5%BF%EF%BC%9Aloss-scale%EF%BC%88%E5%90%A6%E5%88%99%E6%A2%AF%E5%BA%A6%E6%9C%89%E5%81%8F%EF%BC%89"><span class="toc-number">5.4.</span> <span class="toc-text">2.4 Dynamic batch 必须配套一个东西：loss scale（否则梯度有偏）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E4%BD%A0%E7%9C%9F%E6%AD%A3%E7%9A%84-compute-%E5%8D%95%E4%BD%8D%E6%98%AF-token%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF-sample"><span class="toc-number">5.5.</span> <span class="toc-text">2.5 你真正的 compute 单位是 token，而不是 sample</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PPO-vs-GRPO%EF%BC%9A%E5%90%8C%E6%A0%B7%E6%98%AF%E2%80%9Cpolicy-gradient%E2%80%9D%EF%BC%8C%E5%8F%82%E6%95%B0%E5%90%AB%E4%B9%89%E5%AE%8C%E5%85%A8%E4%B8%8D%E5%90%8C"><span class="toc-number">6.</span> <span class="toc-text">3. PPO vs GRPO：同样是“policy gradient”，参数含义完全不同</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-PPO%EF%BC%9A%E6%9B%B4%E7%B2%BE%E7%BB%86%EF%BC%8C%E4%BD%86%E4%BD%A0%E5%BF%85%E9%A1%BB%E5%85%BB%E5%BE%97%E8%B5%B7-critic"><span class="toc-number">6.1.</span> <span class="toc-text">3.1 PPO：更精细，但你必须养得起 critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-GRPO%EF%BC%9A%E6%B2%A1%E6%9C%89-critic%EF%BC%8C%E4%BD%A0%E6%8A%8A-baseline-%E6%94%BE%E8%BF%9B%E2%80%9C%E5%90%8C%E7%BB%84%E6%A0%B7%E6%9C%AC%E2%80%9D"><span class="toc-number">6.2.</span> <span class="toc-text">3.2 GRPO：没有 critic，你把 baseline 放进“同组样本”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E4%B8%80%E4%B8%AA%E5%BE%88%E5%AE%B9%E6%98%93%E8%A2%AB%E5%BF%BD%E7%95%A5%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%9Aloss-%E8%81%9A%E5%90%88%E6%96%B9%E5%BC%8F%EF%BC%88loss-agg-mode%EF%BC%89"><span class="toc-number">6.3.</span> <span class="toc-text">3.3 一个很容易被忽略的参数：loss 聚合方式（loss_agg_mode）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-KL-%E4%B8%8E-Entropy%EF%BC%9A%E4%B8%80%E4%B8%AA%E7%AE%A1%E2%80%9C%E5%88%AB%E9%A3%98%E2%80%9D%EF%BC%8C%E4%B8%80%E4%B8%AA%E7%AE%A1%E2%80%9C%E5%88%AB%E6%AD%BB%E2%80%9D"><span class="toc-number">7.</span> <span class="toc-text">4. KL 与 Entropy：一个管“别飘”，一个管“别死”</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-KL%EF%BC%9A%E4%BD%A0%E9%9C%80%E8%A6%81%E5%85%88%E5%88%86%E6%B8%85%E4%B8%A4%E7%A7%8D-KL"><span class="toc-number">7.1.</span> <span class="toc-text">4.1 KL：你需要先分清两种 KL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9CKL-entropy%E2%80%9D%E7%BB%8F%E5%B8%B8%E8%A6%81%E4%B8%80%E8%B5%B7%E8%B0%83"><span class="toc-number">7.2.</span> <span class="toc-text">4.2 为什么“KL + entropy”经常要一起调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-entropy-bonus-%E4%B8%8D%E6%98%AF%E2%80%9C%E8%A1%A5%E4%B8%81%E2%80%9D%EF%BC%8C%E6%9B%B4%E5%83%8F%E2%80%9C%E6%8E%A2%E7%B4%A2%E9%A2%84%E7%AE%97%E2%80%9D"><span class="toc-number">7.3.</span> <span class="toc-text">4.3.1 entropy bonus 不是“补丁”，更像“探索预算”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%AD%E7%BB%83%E6%97%B6%E4%BD%A0%E8%AF%A5%E7%9B%AF%E7%9A%84-4-%E6%9D%A1%E6%9B%B2%E7%BA%BF%EF%BC%88%E6%AF%94%E2%80%9Closs-%E5%8F%98%E6%B2%A1%E5%8F%98%E5%B0%8F%E2%80%9D%E9%87%8D%E8%A6%81%EF%BC%89"><span class="toc-number">7.4.</span> <span class="toc-text">4.3 训练时你该盯的 4 条曲线（比“loss 变没变小”重要）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%88%AB%E6%8A%8A-KL-%E7%B3%BB%E6%95%B0%E5%BD%93%E6%88%90%E5%B8%B8%E6%95%B0%E2%80%9C%E8%83%8C%E9%85%8D%E6%96%B9%E2%80%9D%EF%BC%8C%E6%9B%B4%E5%BB%BA%E8%AE%AE%E7%94%A8%E9%97%AD%E7%8E%AF%E6%8E%A7%E5%88%B6%E6%80%9D%E8%B7%AF"><span class="toc-number">7.5.</span> <span class="toc-text">4.4 别把 KL 系数当成常数“背配方”，更建议用闭环控制思路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E9%87%87%E6%A0%B7%E6%B8%A9%E5%BA%A6-vs-entropy-bonus%EF%BC%9A%E5%88%AB%E6%8A%8A%E5%90%8C%E4%B8%80%E4%BB%B6%E4%BA%8B%E8%B0%83%E4%B8%A4%E6%AC%A1"><span class="toc-number">7.6.</span> <span class="toc-text">4.5 采样温度 vs entropy bonus：别把同一件事调两次</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E4%B8%A4%E4%B8%AA%E5%B8%B8%E8%A7%81%E8%AF%AF%E8%AF%BB%EF%BC%88%E6%88%91%E5%BB%BA%E8%AE%AE%E4%BD%A0%E5%88%AB%E7%85%A7%E6%90%AC%E4%BB%BB%E4%BD%95%E4%BA%BA%E7%9A%84%E7%BB%93%E8%AE%BA%EF%BC%89"><span class="toc-number">7.7.</span> <span class="toc-text">4.6 两个常见误读（我建议你别照搬任何人的结论）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BD%A0%E8%A6%81%E6%8A%8A-verl-%E7%94%A8%E5%9C%A8-agentic-RL-deep-research%EF%BC%8C%E4%B8%8A%E6%9D%A5%E5%B0%B1%E8%AF%A5%E6%80%8E%E4%B9%88%E6%94%B9%E5%8F%82%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">5. 你要把 verl 用在 agentic RL &#x2F; deep research，上来就该怎么改参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-verl-veRL-%E6%A1%86%E6%9E%B6%E5%B1%82%E9%9D%A2%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%EF%BC%88%E6%AF%94%E5%8D%95%E4%B8%AA%E8%B6%85%E5%8F%82%E6%9B%B4%E9%87%8D%E8%A6%81%EF%BC%89"><span class="toc-number">9.</span> <span class="toc-text">6. verl&#x2F;veRL 框架层面的关键点（比单个超参更重要）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%B8%BA%E4%BB%80%E4%B9%88-agent-rollout-%E5%80%BE%E5%90%91-async%EF%BC%9A%E9%95%BF%E5%B0%BE%EF%BC%88straggler%EF%BC%89%E4%BC%9A%E5%90%83%E6%8E%89%E4%BD%A0%E7%9A%84-GPU"><span class="toc-number">9.1.</span> <span class="toc-text">6.1 为什么 agent rollout 倾向 async：长尾（straggler）会吃掉你的 GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-token-in-token-out%EF%BC%9A%E8%AE%AD%E7%BB%83-%E6%8E%A8%E7%90%86%E4%B8%80%E8%87%B4%E6%80%A7%E4%B8%8D%E6%98%AF%E7%BB%86%E8%8A%82%EF%BC%8C%E6%98%AF%E6%94%B6%E6%95%9B%E6%80%A7%E9%97%AE%E9%A2%98"><span class="toc-number">9.2.</span> <span class="toc-text">6.2 token-in&#x2F;token-out：训练-推理一致性不是细节，是收敛性问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-tool-%E8%BF%94%E5%9B%9E%E7%9A%84-token-%E4%B8%8D%E5%8F%82%E4%B8%8E-policy-gradient%EF%BC%88response-mask%EF%BC%89"><span class="toc-number">9.3.</span> <span class="toc-text">6.3 tool 返回的 token 不参与 policy gradient（response_mask）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-Hybrid-%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%90%8C%E4%B8%80%E6%89%B9-GPU-%E5%9C%A8%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E4%B9%8B%E9%97%B4%E5%88%86%E6%97%B6%E5%A4%8D%E7%94%A8"><span class="toc-number">9.4.</span> <span class="toc-text">6.4 Hybrid 模式：同一批 GPU 在训练和推理之间分时复用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%B8%80%E4%B8%AA%E6%9B%B4%E9%9D%A0%E8%B0%B1%E7%9A%84%E2%80%9C%E8%B0%83%E5%8F%82%E8%AF%8A%E6%96%AD%E8%A1%A8%E2%80%9D%EF%BC%9A%E7%94%A8%E7%8E%B0%E8%B1%A1%E5%AE%9A%E4%BD%8D%E5%88%B0%E6%97%8B%E9%92%AE"><span class="toc-number">10.</span> <span class="toc-text">7. 一个更靠谱的“调参诊断表”：用现象定位到旋钮</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 - 2026 By WP</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><div id="runtime"></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js" defer></script><script src="/js/main.js" defer></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(()=>{var t=()=>{var t;window.MathJax?(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise()):(window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],tags:"none"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(var e of document.querySelectorAll('script[type^="math/tex"]')){var a=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],a),n=document.createTextNode("");e.parentNode.replaceChild(n,e),a.start={node:n,delim:"",n:0},a.end={node:n,delim:"",n:0},t.math.push(a)}},""]}}},(t=document.createElement("script")).src="https://cdn.staticfile.org/mathjax/3.2.2/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t))};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{var e=()=>{var e;0!==(e=document.querySelectorAll("pre > code.mermaid")).length&&e.forEach(e=>{var t=document.createElement("pre"),a=(t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent,document.createElement("div"));a.className="mermaid-wrap",a.appendChild(t),e.parentNode.replaceWith(a)});let t=document.querySelectorAll("#article-container .mermaid-wrap");0!==t.length&&(e=()=>(e=>{window.loadMermaid=!0;let n="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,t)=>{let a=e.firstElementChild;e=`%%{init:{ 'theme':'${n}'}}%%
`+a.textContent,t=mermaid.render("mermaid-"+t,e);let d=e=>{a.insertAdjacentHTML("afterend",e)};"string"==typeof t?d(t):t.then(({svg:e})=>d(e))})})(t),btf.addGlobalFn("themeChange",e,"mermaid"),window.loadMermaid?e():btf.getScript("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.1/mermaid.min.js").then(e))};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{let n="shuoshuo"===GLOBAL_CONFIG_SITE.pageType,o=null,a=(t,e)=>{n&&(window.shuoshuoComment.destroyValine=()=>{t.children.length&&(t.innerHTML="",t.classList.add("no-comment"))});e={el:"#vcomment",appId:"FXG14lTbR0Yj3W2kb3tkAt4L-gzGzoHsz",appKey:"hohJIUW6lOhfboJzq5FvG8z7",avatar:"monsterid",serverURLs:"https://fxg14ltb.lc-cn-n1-shared.com",emojiMaps:"",visitor:!1,...o,path:n?e:o&&o.path||window.location.pathname};new Valine(e)};var t=async(t,e)=>{"function"==typeof Valine||await btf.getScript("https://unpkg.com/valine@1.5.1/dist/Valine.min.js"),a(t,e)};n?window.shuoshuoComment={loadComment:t}:setTimeout(t,0)})()</script></div><script type="text/javascript" src="/js/reward.js" defer></script><script src="/js/fix-avatar.js" defer></script><script src="/js/blog-cool-features.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/butterfly-extsrc/1.1.4/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"]):not([href="/gallery/"]):not([href="/about/"])',selectors:["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!1,scrollRestoration:!1});let t=e=>{e&&Object.values(e).forEach(e=>e())};document.addEventListener("pjax:send",()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");var e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode"),t(window.globalFn.pjaxSend)}),document.addEventListener("pjax:complete",()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach(e=>{let t=document.createElement("script");var a=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(a)),e.parentNode.replaceChild(t,e)}),t(window.globalFn.pjaxComplete)}),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js" defer></script></div></div><script data-pjax src="/js/githubcalendar-loader.js?v=20251124"></script><script data-pjax>function GithubCalendarConfig(){var t=document.getElementById("recent-posts");t&&"/"==location.pathname&&(console.log("已挂载hexo-github-calendar https://github.com/Barry-Flynn/hexo-github-calendar"),t.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>')),GithubCalendar("https://github-calendar-api.meta-code.top/api?user=wp-a",["#ebedf0","#a2f7af","#6ce480","#54ad63","#469252","#31753c","#1f5f2a","#13531f","#084111","#032b09","#000000"],"wp-a")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:0}}</style><style></style></body></html>